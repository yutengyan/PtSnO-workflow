#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
=============================================================================
Step2: GMX MSDé›†æˆå¹³å‡åˆ†æ
=============================================================================
åˆ›å»ºæ—¶é—´: 2025-10-16
æœ€åæ›´æ–°: 2025-11-25
ä½œè€…: GitHub Copilot

åŠŸèƒ½:
    1. è¯»å– GMX MSD åŸå§‹æ•°æ® (.xvgæ–‡ä»¶)
    2. å¯¹æ¯ä¸ª(ç»„æˆ-æ¸©åº¦-å…ƒç´ )ç»„è®¡ç®—é›†æˆå¹³å‡MSD
    3. æ‹Ÿåˆé›†æˆå¹³å‡æ›²çº¿å¾—åˆ°æ‰©æ•£ç³»æ•°D
    4. åº”ç”¨ç­›é€‰ç­–ç•¥è¿‡æ»¤ä½è´¨é‡æ•°æ®
    5. ä¿å­˜é›†æˆåˆ†æç»“æœä¾›step3ä½¿ç”¨

å·¥ä½œæµç¨‹:
    1. [å¯é€‰] å…ˆè¿è¡Œ step1 ç”Ÿæˆå¼‚å¸¸runæ¸…å•
    2. è¿è¡Œæœ¬è„šæœ¬è¿›è¡Œé›†æˆå¹³å‡å’Œç­›é€‰
    3. è¿è¡Œ step3 ç»˜åˆ¶MSDæ›²çº¿

ç­›é€‰ç­–ç•¥:
    ç¬¬0æ­¥: å¼‚å¸¸runé¢„ç­›é€‰ (å¯é€‰)
        - å¦‚æœå­˜åœ¨ large_D_outliers.csv,æ’é™¤å¼‚å¸¸run
        - å¦‚æœä¸å­˜åœ¨,ä½¿ç”¨æ‰€æœ‰run
    
    ç¬¬1æ­¥: Interceptç­›é€‰
        - é˜ˆå€¼: INTERCEPT_MAX (é»˜è®¤20.0 AÂ²)
        - ç‰©ç†æ„ä¹‰: è¿‡å¤§è¡¨ç¤ºç»“æ„æ¾å¼›å¼‚å¸¸
    
    ç¬¬2æ­¥: Då€¼ç‰©ç†åˆç†æ€§ç­›é€‰
        - é˜ˆå€¼: D < D_MAX_THRESHOLD (é»˜è®¤0.1 cmÂ²/s)
        - å»é™¤å›¢ç°‡æ•´ä½“æ¼‚ç§»ç­‰éç‰©ç†ç°è±¡

å‚æ•°è¯´æ˜:
    - INTERCEPT_MAX: Interceptä¸Šé™,é»˜è®¤20.0 AÂ²
      * 15.0: ä¸¥æ ¼æ¨¡å¼,é«˜è´¨é‡æ•°æ®
      * 20.0: å®½æ¾æ¨¡å¼,æ›´å¤šæ•°æ®è¦†ç›–
    
    - D_MAX_THRESHOLD: Då€¼ä¸Šé™,é»˜è®¤0.1 cmÂ²/s
      * ç­›é™¤å¼‚å¸¸å¤§çš„æ‰©æ•£ç³»æ•°

è¾“å…¥:
    - GMX MSDåŸå§‹æ•°æ® (.xvgæ–‡ä»¶,ä»GMX_DATA_DIRSè¯»å–)
    - [å¯é€‰] step1çš„å¼‚å¸¸æ¸…å• (large_D_outliers.csv)

è¾“å‡º: results/
    â”œâ”€â”€ ensemble_analysis_results.csv   (é›†æˆåˆ†æç»“æœ,ä¾›step3ä½¿ç”¨)
    â””â”€â”€ ensemble_analysis_filtered.csv  (è¢«ç­›é€‰çš„æ•°æ®)

ä½¿ç”¨æ–¹æ³•:
    python step2_ensemble_analysis.py   # è®¡ç®—é›†æˆå¹³å‡

è¾“å‡º:
    results/
    â”œâ”€â”€ ensemble_analysis_results.csv  (ç­›é€‰åæ•°æ®)
    â”œâ”€â”€ statistics_by_element.csv      (å…ƒç´ ç»Ÿè®¡)
    â””â”€â”€ filtering_report.txt           (ç­›é€‰æŠ¥å‘Š)
=============================================================================
"""

import pandas as pd
import numpy as np
from scipy.stats import linregress
from pathlib import Path
import os
import re
from collections import defaultdict

# ==================== å¯è°ƒå‚æ•° ====================

# åŸºç¡€ç›®å½• (è‡ªåŠ¨æ£€æµ‹è„šæœ¬æ‰€åœ¨çš„v3_simplified_workflow/)
BASE_DIR = Path(__file__).parent

# æ•°æ®æºç›®å½•
GMX_DATA_DIRS = [
    # BASE_DIR / 'data' / 'gmx_msd' / 'collected_gmx_msd',
    # BASE_DIR / 'data' / 'gmx_msd' / 'gmx_msd_results_20251015_184626_collected',
    # æ–°ç‰ˆunwrap per-atom MSDæ•°æ® (2025-11-18)
    # BASE_DIR / 'data' / 'gmx_msd' / 'unwrap' / 'gmx_msd_results_20251118_152614',
    BASE_DIR / 'data' / 'gmx_msd' / 'unwrap' / 'air' / 'gmx_msd_results_20251124_170114'  # ğŸŒ¬ï¸ æ°”è±¡æ•°æ®
]

# è¾“å‡ºç›®å½•
OUTPUT_DIR = BASE_DIR / 'results'

# å¼‚å¸¸runæ¸…å• (å¯é€‰,å¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡é¢„ç­›é€‰)
OUTLIERS_FILE = BASE_DIR / 'results' / 'large_D_outliers.csv'

# ==================== ç­›é€‰é˜ˆå€¼ (å¯è°ƒæ•´) ====================

# Intercepté˜ˆå€¼ (AÂ²)
# å»ºè®®å€¼:
#   15.0 - ä¸¥æ ¼æ¨¡å¼ (é«˜è´¨é‡æ•°æ®)
#   20.0 - å®½æ¾æ¨¡å¼ (é€‚åˆMSDç»˜å›¾,é»˜è®¤)
#   25.0 - æå®½æ¾ (å‡ ä¹ä¸ç­›é™¤)
INTERCEPT_MAX = 20.0

# Då€¼ä¸Šé™ (cmÂ²/s)
# è¶…è¿‡æ­¤å€¼è®¤ä¸ºæ˜¯å›¢ç°‡æ•´ä½“æ¼‚ç§»
D_MAX_THRESHOLD = 0.1

# æ‹Ÿåˆå‚æ•°
FIT_START_RATIO = 0.1  # æ‹Ÿåˆèµ·å§‹æ¯”ä¾‹
FIT_END_RATIO = 0.9    # æ‹Ÿåˆç»“æŸæ¯”ä¾‹

# ==================== æ ¸å¿ƒå‡½æ•° ====================

def load_large_D_outliers():
    """
    åŠ è½½å¤§Då€¼å¼‚å¸¸runæ¸…å• (å¯é€‰)
    
    è¿”å›:
        set: å¼‚å¸¸æ–‡ä»¶è·¯å¾„é›†åˆ,å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›ç©ºé›†
    """
    outlier_file = Path(OUTLIERS_FILE)
    try:
        df_outliers = pd.read_csv(outlier_file)
        outlier_files = set(df_outliers['filepath'].values)
        print(f"   [OK] åŠ è½½å¼‚å¸¸runæ¸…å•: {len(outlier_files)} ä¸ª")
        return outlier_files
    except FileNotFoundError:
        print(f"   [!] æœªæ‰¾åˆ°å¼‚å¸¸æ¸…å•: {outlier_file}")
        print(f"   [!] å°†ä¸è¿›è¡Œé¢„ç­›é€‰,ä½¿ç”¨æ‰€æœ‰run")
        return set()


def parse_gmx_filename(filepath):
    """
    è§£æGMX MSDæ–‡ä»¶å
    
    è·¯å¾„ç¤ºä¾‹:
        .../g-1-O1Sn4Pt3/1000K/T1000.r24.gpu0_msd_Pt.xvg
        .../pt8sn2-1-best/300K/T300.r0.gpu0_msd_Sn.xvg
    
    è¿”å›:
        dict: {'composition', 'temperature', 'element', 'filepath'}
    """
    filename = filepath.stem
    
    # æå–å…ƒç´ 
    if filename.endswith('_Pt'):
        element = 'Pt'
    elif filename.endswith('_Sn'):
        element = 'Sn'
    elif filename.endswith('_PtSn'):
        element = 'PtSn'
    else:
        return None
    
    # æå–æ¸©åº¦ (ä»æ–‡ä»¶å)
    temp_match = re.match(r'T(\d+)', filename)
    if not temp_match:
        return None
    temperature = f"{temp_match.group(1)}K"
    
    # æå–ç»„æˆ (ä»è·¯å¾„,æ¸©åº¦ç›®å½•çš„çˆ¶ç›®å½•)
    parts = filepath.parts
    if len(parts) < 3:
        return None
    
    # æ‰¾æ¸©åº¦ç›®å½•
    temp_dir_idx = None
    for i, part in enumerate(parts):
        if part == temperature:
            temp_dir_idx = i
            break
    
    if temp_dir_idx is None or temp_dir_idx == 0:
        return None
    
    composition = parts[temp_dir_idx - 1]
    
    return {
        'composition': composition,
        'temperature': temperature,
        'element': element,
        'filepath': filepath,
        'full_key': f"{composition}_{temperature}"
    }


def read_xvg(filepath):
    """
    è¯»å–GMX .xvgæ–‡ä»¶
    
    æ³¨æ„å•ä½è½¬æ¢:
        - GMXè¾“å‡º: MSD (nmÂ²), Time (ps)
        - æœ¬å‡½æ•°è¾“å‡º: MSD (AÂ²), Time (ps)
    """
    try:
        data = []
        with open(filepath, 'r') as f:
            for line in f:
                if line.startswith('#') or line.startswith('@'):
                    continue
                parts = line.strip().split()
                if len(parts) >= 2:
                    try:
                        t = float(parts[0])           # ps
                        msd = float(parts[1]) * 100   # nmÂ² â†’ AÂ² (Ã—100)
                        data.append([t, msd])
                    except ValueError:
                        continue
        
        if len(data) == 0:
            return None, None
        
        data = np.array(data)
        return data[:, 0], data[:, 1]
    
    except Exception as e:
        return None, None


def fit_msd_to_diffusion(time, msd, fit_start_ratio=0.1, fit_end_ratio=0.9):
    """
    çº¿æ€§æ‹ŸåˆMSD â†’ æ‰©æ•£ç³»æ•°
    
    è¾“å…¥:
        time: ps (çš®ç§’)
        msd: AÂ² (åŸƒÂ²)
    
    è¾“å‡º:
        D: cmÂ²/s
        r2: æ‹Ÿåˆä¼˜åº¦
        intercept: AÂ²
    
    å…¬å¼æ¨å¯¼:
        MSD (AÂ²) = 6D (cmÂ²/s) Ã— t (ps) Ã— (10^-16 cmÂ²/AÂ²) / (10^-12 s/ps)
        MSD = 6D Ã— t Ã— 10^-4
        slope (AÂ²/ps) = 6D Ã— 10^-4
        D (cmÂ²/s) = slope / (6 Ã— 10^-4)
    """
    if len(time) < 10:
        return None, None, None
    
    start_idx = int(len(time) * fit_start_ratio)
    end_idx = int(len(time) * fit_end_ratio)
    
    if end_idx <= start_idx:
        return None, None, None
    
    t_fit = time[start_idx:end_idx]
    msd_fit = msd[start_idx:end_idx]
    
    try:
        slope, intercept, r_value, p_value, std_err = linregress(t_fit, msd_fit)
        # âœ… ä¿®å¤å•ä½è½¬æ¢: slope (AÂ²/ps) â†’ D (cmÂ²/s)
        D = slope / 6.0 * 1e-4  # åŸæ¥ç¼ºå°‘ Ã—10^-4
        r2 = r_value ** 2
        return D, r2, intercept
    except Exception as e:
        return None, None, None


def ensemble_average_msd(file_list):
    """é›†åˆå¹³å‡å¤šä¸ªrunçš„MSD"""
    all_times = []
    all_msds = []
    
    for filepath in file_list:
        time, msd = read_xvg(filepath)
        if time is not None and msd is not None:
            all_times.append(time)
            all_msds.append(msd)
    
    if len(all_times) == 0:
        return None
    
    # å¯¹é½åˆ°æœ€çŸ­é•¿åº¦
    min_length = min(len(t) for t in all_times)
    time_avg = all_times[0][:min_length]
    
    msd_arrays = [msd[:min_length] for msd in all_msds]
    msd_avg = np.mean(msd_arrays, axis=0)
    msd_std = np.std(msd_arrays, axis=0, ddof=1) if len(msd_arrays) > 1 else np.zeros_like(msd_avg)
    
    return {
        'time': time_avg,
        'msd_mean': msd_avg,
        'msd_std': msd_std,
        'n_runs': len(all_times)
    }


def main():
    """ä¸»åˆ†ææµç¨‹"""
    
    print("\n" + "="*80)
    print("GMX MSDé›†åˆå¹³å‡åˆ†æå·¥å…· (v3.0 ç®€åŒ–ç‰ˆ)")
    print("="*80)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # æ­¥éª¤0: åŠ è½½å¼‚å¸¸æ¸…å•(å¯é€‰)
    print("\n[0/5] åŠ è½½å¤§Då€¼å¼‚å¸¸æ¸…å•...")
    large_D_outliers = load_large_D_outliers()
    
    # æ­¥éª¤1: æ”¶é›†æ–‡ä»¶ (å¸¦å»é‡)
    print("\n[1/5] æ”¶é›†GMX MSDæ–‡ä»¶...")
    all_files = []
    seen_files = set()  # ç”¨äºå»é‡çš„é›†åˆ
    for data_dir in GMX_DATA_DIRS:
        gmx_dir = Path(data_dir)
        if not gmx_dir.exists():
            print(f"  âš ï¸ ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            continue
        
        files_in_dir = list(gmx_dir.rglob('*_msd_*.xvg'))
        # å»é‡: ä½¿ç”¨ç›¸å¯¹äºBASE_DIRçš„è§„èŒƒåŒ–è·¯å¾„
        unique_count = 0
        duplicate_count = 0
        for f in files_in_dir:
            try:
                # è·å–è§„èŒƒåŒ–çš„ç»å¯¹è·¯å¾„
                normalized_path = f.resolve()
                if normalized_path not in seen_files:
                    seen_files.add(normalized_path)
                    all_files.append(f)
                    unique_count += 1
                else:
                    duplicate_count += 1
            except:
                # å¦‚æœresolve()å¤±è´¥,ä»ç„¶æ·»åŠ 
                all_files.append(f)
                unique_count += 1
        
        print(f"  {gmx_dir.name}: {len(files_in_dir)} æ–‡ä»¶ ({unique_count} å”¯ä¸€, {duplicate_count} é‡å¤)")
    
    print(f"\n   æ€»è®¡: {len(all_files)} æ–‡ä»¶ (å·²å»é‡)")
    
    # æ­¥éª¤2: è§£æå’Œåˆ†ç»„
    print("\n[2/5] è§£ææ–‡ä»¶ä¿¡æ¯...")
    file_info = []
    for filepath in all_files:
        info = parse_gmx_filename(filepath)
        if info:
            file_info.append(info)
    
    print(f"   æˆåŠŸè§£æ: {len(file_info)} æ–‡ä»¶")
    
    # æŒ‰ä½“ç³»-æ¸©åº¦-å…ƒç´ åˆ†ç»„
    print("\n[3/5] åˆ†ç»„æ•°æ®...")
    grouped = defaultdict(list)
    for info in file_info:
        key = f"{info['full_key']}_{info['element']}"
        grouped[key].append(info['filepath'])
    
    print(f"   ç‹¬ç«‹ç»„åˆ: {len(grouped)} ç»„ (æ¯ç»„åŒ…å«å¤šæ¬¡æ¨¡æ‹Ÿrun)")
    
    # ç»Ÿè®¡runæ•°åˆ†å¸ƒ
    run_counts = [len(files) for files in grouped.values()]
    avg_runs = sum(run_counts) / len(run_counts)
    min_runs = min(run_counts)
    max_runs = max(run_counts)
    print(f"   æ¯ç»„runæ•°: æœ€å°‘{min_runs}, æœ€å¤š{max_runs}, å¹³å‡{avg_runs:.1f}")
    
    # é¢„ç­›é€‰ç»Ÿè®¡
    total_files_before = sum(len(files) for files in grouped.values())
    files_to_remove = 0
    for files in grouped.values():
        for filepath in files:
            if str(filepath) in large_D_outliers:
                files_to_remove += 1
    
    print(f"\n   é¢„ç­›é€‰(step1å¼‚å¸¸æ£€æµ‹):")
    print(f"     æ€»runæ•°: {total_files_before}")
    print(f"     å·²èˆå¼ƒå¼‚å¸¸run: {files_to_remove} ({files_to_remove/total_files_before*100:.1f}%)")
    print(f"     ä¿ç•™æœ‰æ•ˆrun: {total_files_before - files_to_remove}")
    
    # æ­¥éª¤4: é›†åˆå¹³å‡åˆ†æ (ä½¿ç”¨step1ç­›é€‰åçš„run)
    print(f"\n[4/5] å¯¹ {len(grouped)} ç»„è¿›è¡Œé›†åˆå¹³å‡åˆ†æ...")
    print(f"   æ–¹æ³•: MSDæ›²çº¿å¹³å‡ â†’ æ‹Ÿåˆ â†’ æ±‚Då€¼")
    print(f"   (éç›´æ¥å¹³å‡Då€¼,è€Œæ˜¯å…ˆå¹³å‡MSDæ—¶é—´åºåˆ—å†æ‹Ÿåˆ)")
    results = []
    results_filtered = []  # æ–°å¢: ä¿å­˜è¢«ç­›é€‰æ‰çš„runsçš„é›†åˆå¹³å‡
    processed = 0
    processed_filtered = 0
    step0_filtered = 0  # step1èˆå¼ƒçš„runæ•°
    total_runs_before_filter = 0
    
    for key, files in grouped.items():
        # è§£ækey
        parts = key.rsplit('_', 1)
        comp_temp = parts[0]
        element = parts[1]
        
        comp_temp_parts = comp_temp.rsplit('_', 1)
        composition = comp_temp_parts[0]
        temperature = comp_temp_parts[1]
        temp_value = int(temperature.replace('K', ''))
        
        # ç»Ÿè®¡åŸå§‹runæ•°
        original_n_files = len(files)
        total_runs_before_filter += original_n_files
        
        # ä½¿ç”¨step1ç­›é€‰åçš„run (èˆå¼ƒlarge_D_outliersä¸­çš„run)
        valid_files = [f for f in files if str(f) not in large_D_outliers]
        filtered_files = [f for f in files if str(f) in large_D_outliers]  # æ–°å¢: è¢«ç­›é€‰æ‰çš„runs
        n_filtered = original_n_files - len(valid_files)
        step0_filtered += n_filtered
        
        # === å¤„ç†æœ‰æ•ˆruns (åŸæœ‰é€»è¾‘) ===
        if len(valid_files) > 0:
            # é›†åˆå¹³å‡(åªç”¨é€šè¿‡ç­›é€‰çš„run)
            avg_result = ensemble_average_msd(valid_files)
            if avg_result is not None:
                # æ‹Ÿåˆé›†åˆå¹³å‡çš„MSD
                D, r2, intercept = fit_msd_to_diffusion(
                    avg_result['time'],
                    avg_result['msd_mean'],
                    FIT_START_RATIO,
                    FIT_END_RATIO
                )
                
                if D is not None and r2 is not None:
                    processed += 1
                    results.append({
                        'composition': composition,
                        'temperature': temperature,
                        'temp_value': temp_value,
                        'element': element,
                        'n_runs': len(valid_files),
                        'n_runs_original': original_n_files,
                        'n_runs_filtered': n_filtered,
                        'D_ensemble': D,
                        'R2_ensemble': r2,
                        'intercept': intercept
                    })
        
        # === æ–°å¢: å¤„ç†è¢«ç­›é€‰æ‰çš„runs ===
        if len(filtered_files) > 0:
            avg_result_filt = ensemble_average_msd(filtered_files)
            if avg_result_filt is not None:
                D_filt, r2_filt, intercept_filt = fit_msd_to_diffusion(
                    avg_result_filt['time'],
                    avg_result_filt['msd_mean'],
                    FIT_START_RATIO,
                    FIT_END_RATIO
                )
                
                if D_filt is not None and r2_filt is not None:
                    processed_filtered += 1
                    results_filtered.append({
                        'composition': composition,
                        'temperature': temperature,
                        'temp_value': temp_value,
                        'element': element,
                        'n_runs': len(filtered_files),
                        'D_ensemble': D_filt,
                        'R2_ensemble': r2_filt,
                        'intercept': intercept_filt
                    })
        
        if (processed + processed_filtered) % 100 == 0:
            print(f"   [+] å·²å¤„ç† {processed} ä¸ªæœ‰æ•ˆç»„ + {processed_filtered} ä¸ªå¼‚å¸¸ç»„...")
    
    # æ­¥éª¤5: ä¿å­˜ç»“æœ
    print(f"\n[5/5] ä¿å­˜ç»“æœ...")
    
    # ä¿å­˜æœ‰æ•ˆrunsçš„é›†åˆå¹³å‡
    df_results = pd.DataFrame(results)
    output_file = Path(OUTPUT_DIR) / 'ensemble_analysis_results.csv'
    df_results.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"   [OK] æœ‰æ•ˆrunsç»“æœ: {output_file}")
    
    # ä¿å­˜è¢«ç­›é€‰æ‰runsçš„é›†åˆå¹³å‡
    if len(results_filtered) > 0:
        df_filtered = pd.DataFrame(results_filtered)
        output_file_filt = Path(OUTPUT_DIR) / 'ensemble_analysis_filtered.csv'
        df_filtered.to_csv(output_file_filt, index=False, encoding='utf-8-sig')
        print(f"   [OK] è¢«ç­›é€‰runsç»“æœ: {output_file_filt}")
    else:
        print(f"   âš ï¸ æ— è¢«ç­›é€‰æ‰çš„runså¯åšé›†åˆå¹³å‡")
    
    # ç»Ÿè®¡æŠ¥å‘Š
    print("\n" + "="*80)
    print("é›†åˆå¹³å‡ç»Ÿè®¡")
    print("="*80)
    print(f"  åŸå§‹æ•°æ®:")
    print(f"    â€¢ æ€»runæ•°: {total_runs_before_filter} ä¸ªrun")
    print(f"    â€¢ ç‹¬ç«‹ç»„åˆ: {len(grouped)} ç»„ (ä½“ç³»Ã—æ¸©åº¦Ã—å…ƒç´ )")
    print(f"    â€¢ å¹³å‡æ¯ç»„: {total_runs_before_filter/len(grouped):.1f} run")
    print(f"\n  Step1è´¨é‡ç­›é€‰:")
    print(f"    â€¢ èˆå¼ƒrun: {step0_filtered} run ({step0_filtered/total_runs_before_filter*100:.1f}%)")
    print(f"    â€¢ ä¿ç•™run: {total_runs_before_filter - step0_filtered} run")
    print(f"\n  é›†åˆå¹³å‡ç»“æœ:")
    print(f"    â€¢ æ–¹æ³•: MSDæ›²çº¿å¹³å‡ â†’ çº¿æ€§æ‹Ÿåˆ â†’ Då€¼")
    print(f"    â€¢ æœ‰æ•ˆç»„åˆ: {len(results)} ç»„")
    print(f"    â€¢ å¼‚å¸¸ç»„åˆ: {len(results_filtered)} ç»„ (è¢«step1ç­›é€‰æ‰çš„runs)")
    print(f"    â€¢ Step2ç­›é€‰: æ—  (ä¿ç•™æ‰€æœ‰å¯å¤„ç†ç»„åˆ)")
    print(f"    â€¢ ç»„åˆå¯ç”¨ç‡: {len(results)/len(grouped)*100:.1f}% ({len(results)}/{len(grouped)})")

    
    # æ•°æ®ç»Ÿè®¡
    if len(results) > 0:
        print(f"\næ•°æ®ç»Ÿè®¡:")
        print(f"  RÂ² èŒƒå›´: {df_results['R2_ensemble'].min():.4f} ~ {df_results['R2_ensemble'].max():.4f}")
        print(f"  RÂ² å¹³å‡: {df_results['R2_ensemble'].mean():.4f}")
        print(f"  RÂ² < 0.5: {len(df_results[df_results['R2_ensemble'] < 0.5])} ç»„ ({len(df_results[df_results['R2_ensemble'] < 0.5])/len(df_results)*100:.1f}%) - æ‹Ÿåˆè´¨é‡å·®")
        print(f"  Intercept èŒƒå›´: {df_results['intercept'].min():.2f} ~ {df_results['intercept'].max():.2f} AÂ²")
        print(f"  Intercept >10: {len(df_results[df_results['intercept'] > 10])}")
        print(f"  Intercept >15: {len(df_results[df_results['intercept'] > 15])}")
        print(f"  n_runs = 1: {len(df_results[df_results['n_runs'] == 1])} ç»„ ({len(df_results[df_results['n_runs'] == 1])/len(df_results)*100:.1f}%) - æ— æ³•é›†åˆå¹³å‡")
        print(f"  n_runs â‰¥ 3: {len(df_results[df_results['n_runs'] >= 3])} ç»„ ({len(df_results[df_results['n_runs'] >= 3])/len(df_results)*100:.1f}%)")

    
    print("\n" + "="*80)
    print("åˆ†æå®Œæˆ!")
    print("="*80)


if __name__ == '__main__':
    main()
