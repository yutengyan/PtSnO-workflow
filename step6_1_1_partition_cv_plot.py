#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Step 6.1.1: åˆ†åŒºçƒ­å®¹æ‹Ÿåˆå›¾ - è®ºæ–‡å‡ºå›¾ä¸“ç”¨

ä» step6_1 æå–çš„æ ¸å¿ƒç»˜å›¾åŠŸèƒ½ï¼Œç”Ÿæˆé€‚åˆè®ºæ–‡å‘è¡¨çš„åˆ†åŒºçƒ­å®¹æ‹Ÿåˆå›¾ï¼š
- æ•´ä½“æ‹Ÿåˆ vs åˆ†åŒºæ‹Ÿåˆå¯¹æ¯”
- æŒ‰æ¸©åº¦å¹³å‡çš„æ•°æ®ç‚¹ï¼ˆå¸¦è¯¯å·®æ£’ï¼‰
- å¤šæ•°æŠ•ç¥¨è§„åˆ™é¿å…æ¸©åº¦äº¤å‰

è¾“å…¥: step6_1 ç”Ÿæˆçš„èšç±»ç»“æœ CSV æ–‡ä»¶
è¾“å‡º: é«˜è´¨é‡è®ºæ–‡å›¾ (PNG/PDF)

ç”¨æ³•:
  python step6_1_1_partition_cv_plot.py --structure Pt8sn6
  python step6_1_1_partition_cv_plot.py --structure Air86 --format pdf
  python step6_1_1_partition_cv_plot.py --structure all --dpi 600
  python step6_1_1_partition_cv_plot.py --list

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-11-30
"""

import os
import sys
import glob
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from scipy.stats import linregress
from pathlib import Path
from datetime import datetime

# è®¾ç½®é«˜è´¨é‡è®ºæ–‡å›¾æ ·å¼
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.size'] = 11
plt.rcParams['axes.linewidth'] = 1.2
plt.rcParams['xtick.major.width'] = 1.2
plt.rcParams['ytick.major.width'] = 1.2


# ç›¸æ€é¢œè‰²é…ç½®
PHASE_COLORS = {
    'Solid': '#3498db',      # è“è‰² - å›ºæ€
    'Pre-melting': '#f39c12', # æ©™è‰² - é¢„ç†”åŒ–
    'Liquid': '#e74c3c',     # çº¢è‰² - æ¶²æ€
    'Phase_1': '#3498db',
    'Phase_2': '#e74c3c',
    'Phase_3': '#f39c12',
}

# è½½ä½“çƒ­å®¹ (meV/K)
CV_SUPPORT = 38.2151


def find_clustering_results(base_dir='results/step6_1_clustering'):
    """æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨çš„èšç±»ç»“æœ"""
    results = {}
    # ä½¿ç”¨å®é™…çš„æ–‡ä»¶å‘½åæ¨¡å¼
    pattern = os.path.join(base_dir, '*_kmeans_n2_clustered_data.csv')
    files = glob.glob(pattern)
    
    for f in files:
        basename = os.path.basename(f)
        structure = basename.replace('_kmeans_n2_clustered_data.csv', '')
        results[structure] = f
    
    return results


def load_support_energy_data():
    """åŠ è½½è½½ä½“èƒ½é‡æ•°æ®"""
    support_csv = 'data/lammps_energy/sup/energy_master_20251021_151520.csv'
    
    if not os.path.exists(support_csv):
        return None
    
    try:
        df_support = pd.read_csv(support_csv)
        if 'temp' in df_support.columns and 'avg_energy' in df_support.columns:
            T = df_support['temp'].values
            E = df_support['avg_energy'].values
            slope, intercept, r_value, _, _ = linregress(T, E)
            return slope, intercept, r_value**2
    except Exception as e:
        print(f"  è­¦å‘Š: è¯»å–è½½ä½“èƒ½é‡æ•°æ®å¤±è´¥: {e}")
    
    return None


def load_cluster_data(csv_path):
    """åŠ è½½èšç±»ç»“æœæ•°æ®"""
    try:
        df = pd.read_csv(csv_path)
        return df
    except Exception as e:
        print(f"  é”™è¯¯: æ— æ³•è¯»å– {csv_path}: {e}")
        return None


def plot_partition_cv(df, structure_name, output_dir, output_format='png', dpi=300):
    """
    ç»˜åˆ¶åˆ†åŒºçƒ­å®¹æ‹Ÿåˆå›¾ï¼ˆè®ºæ–‡å‡ºå›¾ä¸“ç”¨ï¼‰
    
    æ ¸å¿ƒé€»è¾‘ï¼š
    1. æŒ‰æ¸©åº¦åˆ†ç»„è®¡ç®—å›¢ç°‡èƒ½é‡å¹³å‡å€¼å’Œæ ‡å‡†å·®
    2. ä½¿ç”¨å¤šæ•°æŠ•ç¥¨è§„åˆ™å°†æ¯ä¸ªæ¸©åº¦åˆ†é…ç»™å”¯ä¸€çš„ç›¸æ€
    3. å¯¹æ¯ä¸ªç›¸æ€çš„ä¸“å±æ¸©åº¦ç‚¹è¿›è¡Œçº¿æ€§æ‹Ÿåˆ
    4. ç»˜åˆ¶æ•´ä½“æ‹Ÿåˆçº¿ vs åˆ†åŒºæ‹Ÿåˆçº¿å¯¹æ¯”
    """
    
    print(f"\n>>> ç»˜åˆ¶ {structure_name} åˆ†åŒºçƒ­å®¹å›¾...")
    
    # æ£€æŸ¥å¿…è¦åˆ—
    required_cols = ['temp', 'avg_energy', 'phase_clustered']
    if not all(col in df.columns for col in required_cols):
        print(f"  é”™è¯¯: ç¼ºå°‘å¿…è¦åˆ— {required_cols}")
        return None
    
    # åˆ¤æ–­æ˜¯å¦æ˜¯ Air ç³»åˆ—ï¼ˆæ°”ç›¸å›¢ç°‡ï¼‰
    is_air_system = structure_name.startswith('Air') or structure_name in ['68', '86']
    
    # åŠ è½½è½½ä½“èƒ½é‡æ•°æ®
    if is_air_system:
        slope_support = 0.0
        intercept_support = 0.0
        print(f"  [Airç³»åˆ—] æ°”ç›¸çº³ç±³å›¢ç°‡ï¼Œä¸æ‰£é™¤è½½ä½“èƒ½é‡")
    else:
        support_fit = load_support_energy_data()
        if support_fit is not None:
            slope_support, intercept_support, R2_support = support_fit
            print(f"  [è½½ä½“æ•°æ®] Cv_support={slope_support*1000:.4f} meV/K, RÂ²={R2_support:.6f}")
        else:
            slope_support = CV_SUPPORT / 1000  # meV/K -> eV/K
            T_min = df['temp'].min()
            E_total_min = df[df['temp'] == T_min]['avg_energy'].mean()
            intercept_support = E_total_min * 0.9 - slope_support * T_min
            print(f"  [è­¦å‘Š] ä½¿ç”¨é»˜è®¤Cv_supportä¼°ç®—è½½ä½“èƒ½é‡")
    
    # ========== 1. æŒ‰æ¸©åº¦åˆ†ç»„è®¡ç®—å›¢ç°‡èƒ½é‡ ==========
    temp_groups = df.groupby('temp')
    temps_unique = []
    E_cluster_mean = []
    E_cluster_std = []
    
    for temp, group in temp_groups:
        if is_air_system:
            E_cluster = group['avg_energy'].values
        else:
            E_support = slope_support * temp + intercept_support
            E_cluster = group['avg_energy'].values - E_support
        
        temps_unique.append(temp)
        E_cluster_mean.append(np.mean(E_cluster))
        E_cluster_std.append(np.std(E_cluster))
    
    temps_unique = np.array(temps_unique)
    E_cluster_mean = np.array(E_cluster_mean)
    E_cluster_std = np.array(E_cluster_std)
    
    # è®¡ç®—ç›¸å¯¹èƒ½é‡ï¼ˆç›¸å¯¹äºæœ€ä½æ¸©åº¦ï¼‰
    E_cluster_ref = E_cluster_mean.min()
    E_cluster_mean_rel = E_cluster_mean - E_cluster_ref
    
    # ========== 2. å¤šæ•°æŠ•ç¥¨ç¡®å®šæ¯ä¸ªæ¸©åº¦çš„ä¸“å±ç›¸æ€ ==========
    temp_to_partition = {}
    print(f"\n  å¤šæ•°æŠ•ç¥¨æ¸©åº¦åˆ†é…:")
    
    for temp in temps_unique:
        df_temp = df[df['temp'] == temp]
        partition_counts = df_temp['phase_clustered'].value_counts()
        dominant_partition = partition_counts.idxmax()
        temp_to_partition[temp] = dominant_partition
        print(f"    T={temp:4.0f}K: {dict(partition_counts)} â†’ {dominant_partition}")
    
    # ========== 3. æ•´ä½“æ‹Ÿåˆ ==========
    if len(temps_unique) < 3:
        print(f"  é”™è¯¯: æ¸©åº¦ç‚¹ä¸è¶³ ({len(temps_unique)} < 3)")
        return None
    
    slope_overall, intercept_overall, r_value_overall, _, std_err_overall = linregress(
        temps_unique, E_cluster_mean_rel)
    R2_overall = r_value_overall ** 2
    Cv_overall = slope_overall * 1000  # meV/K
    Cv_overall_err = std_err_overall * 1000
    
    print(f"\n  æ•´ä½“æ‹Ÿåˆ: Cv={Cv_overall:.4f}Â±{Cv_overall_err:.4f} meV/K, RÂ²={R2_overall:.4f}")
    
    # ========== 4. åˆ†åŒºæ‹Ÿåˆ ==========
    phases = df['phase_clustered'].unique()
    phase_fits = {}
    
    for phase in phases:
        phase_temps = [temp for temp, part in temp_to_partition.items() if part == phase]
        phase_temps = sorted(phase_temps)
        
        if len(phase_temps) >= 2:
            mask = np.isin(temps_unique, phase_temps)
            T_phase = temps_unique[mask]
            E_phase_rel = E_cluster_mean_rel[mask]
            E_phase_std = E_cluster_std[mask]
            
            slope_ph, intercept_ph, r_value_ph, _, std_err_ph = linregress(T_phase, E_phase_rel)
            R2_ph = r_value_ph ** 2
            Cv_ph = slope_ph * 1000
            Cv_ph_err = std_err_ph * 1000
            
            phase_fits[phase] = {
                'slope': slope_ph,
                'intercept': intercept_ph,
                'R2': R2_ph,
                'Cv': Cv_ph,
                'Cv_err': Cv_ph_err,
                'n_temps': len(T_phase),
                'T_range': (T_phase.min(), T_phase.max()),
                'T_data': T_phase,
                'E_data': E_phase_rel,
                'E_std': E_phase_std
            }
            
            print(f"  {phase}: Cv={Cv_ph:.4f}Â±{Cv_ph_err:.4f} meV/K, RÂ²={R2_ph:.4f}, "
                  f"n={len(T_phase)}, T={T_phase.min():.0f}-{T_phase.max():.0f}K")
    
    # ========== 5. ç»˜åˆ¶ç®€æ´çš„åŒYè½´å›¾ ==========
    fig, ax1 = plt.subplots(figsize=(8, 6))
    
    # ----- å·¦Yè½´: èƒ½é‡-æ¸©åº¦æ•°æ®ç‚¹ï¼ˆå¸¦è¯¯å·®æ£’ï¼‰å’Œæ‹Ÿåˆçº¿ -----
    # ç»˜åˆ¶æ•°æ®ç‚¹ï¼ˆå¸¦è¯¯å·®æ£’ï¼‰
    ax1.errorbar(temps_unique, E_cluster_mean_rel, yerr=E_cluster_std,
                 fmt='o', markersize=7, color='black', 
                 ecolor='gray', elinewidth=1.5, capsize=3, capthick=1.5,
                 zorder=5, label='Data')
    
    # ç»˜åˆ¶æ‹Ÿåˆçº¿ï¼ˆé»‘è‰²ï¼‰
    phases_sorted = sorted(phase_fits.keys())
    for phase in phases_sorted:
        fit = phase_fits[phase]
        T_phase_fit = np.linspace(fit['T_range'][0], fit['T_range'][1], 50)
        E_phase_fit = fit['slope'] * T_phase_fit + fit['intercept']
        ax1.plot(T_phase_fit, E_phase_fit, '-', color='black', linewidth=2, zorder=4)
    
    # è¿æ¥ä¸¤ä¸ªåˆ†åŒºä¹‹é—´çš„æ•°æ®ç‚¹ï¼ˆå®çº¿è¿æ¥å®é™…æ•°æ®ç‚¹ï¼Œè€Œéæ‹Ÿåˆçº¿ï¼‰
    if len(phases_sorted) >= 2:
        fit1 = phase_fits[phases_sorted[0]]
        fit2 = phase_fits[phases_sorted[1]]
        # åˆ†åŒº1çš„æœ€åä¸€ä¸ªæ•°æ®ç‚¹
        T1_end = fit1['T_range'][1]
        idx1 = np.where(temps_unique == T1_end)[0]
        if len(idx1) > 0:
            E1_end = E_cluster_mean_rel[idx1[0]]
        else:
            E1_end = fit1['slope'] * T1_end + fit1['intercept']
        # åˆ†åŒº2çš„ç¬¬ä¸€ä¸ªæ•°æ®ç‚¹
        T2_start = fit2['T_range'][0]
        idx2 = np.where(temps_unique == T2_start)[0]
        if len(idx2) > 0:
            E2_start = E_cluster_mean_rel[idx2[0]]
        else:
            E2_start = fit2['slope'] * T2_start + fit2['intercept']
        # ç”¨å®çº¿è¿æ¥ä¸¤ä¸ªæ•°æ®ç‚¹
        ax1.plot([T1_end, T2_start], [E1_end, E2_start], '-', color='black', linewidth=2, zorder=4)
    
    ax1.set_xlabel('Temperature (K)', fontsize=13, fontweight='bold')
    ax1.set_ylabel('Total Energy (eV)', fontsize=13, fontweight='bold')
    ax1.tick_params(axis='both', labelsize=11)
    
    # ----- å³Yè½´: çƒ­å®¹æ›²çº¿ -----
    ax2 = ax1.twinx()
    
    # åˆ†ç•Œæ¸©åº¦å’Œçƒ­å®¹å€¼ï¼ˆç”¨äºå¯¼å‡ºï¼‰
    T_boundary = None
    Cv1 = None
    Cv2 = None
    Cv_peak = None
    
    if len(phases_sorted) >= 2:
        # æ‰¾åˆ°åˆ†åŒºè¾¹ç•Œæ¸©åº¦
        phase1_temps = [t for t, p in temp_to_partition.items() if p == phases_sorted[0]]
        phase2_temps = [t for t, p in temp_to_partition.items() if p == phases_sorted[1]]
        
        if phase1_temps and phase2_temps:
            T1_last = max(phase1_temps)   # åˆ†åŒº1æœ€åä¸€ä¸ªæ¸©åº¦
            T2_first = min(phase2_temps)  # åˆ†åŒº2ç¬¬ä¸€ä¸ªæ¸©åº¦
            T_boundary = (T1_last + T2_first) / 2
            print(f"\n  åˆ†ç•Œæ¸©åº¦: {T_boundary:.0f} K (è¿‡æ¸¡åŒº: {T1_last:.0f}-{T2_first:.0f}K)")
            
            Cv1 = phase_fits[phases_sorted[0]]['Cv']
            Cv2 = phase_fits[phases_sorted[1]]['Cv']
            
            # è®¡ç®—è¿‡æ¸¡åŒºçƒ­å®¹ï¼ˆæ•°å€¼å¾®åˆ†ï¼‰
            idx1 = np.where(temps_unique == T1_last)[0]
            idx2 = np.where(temps_unique == T2_first)[0]
            if len(idx1) > 0 and len(idx2) > 0:
                E1 = E_cluster_mean_rel[idx1[0]]
                E2 = E_cluster_mean_rel[idx2[0]]
                Cv_transition = (E2 - E1) / (T2_first - T1_last) * 1000  # meV/K
            else:
                Cv_transition = (Cv1 + Cv2) / 2
            
            # åˆ¤æ–­æ˜¯å¦å­˜åœ¨çƒ­å®¹å³°
            has_peak = Cv_transition > max(Cv1, Cv2)
            
            if has_peak:
                Cv_peak = Cv_transition
                print(f"  â˜… å­˜åœ¨çƒ­å®¹å³°: Cv_peak={Cv_peak:.2f} meV/K (è¿‡æ¸¡åŒº)")
                print(f"  çƒ­å®¹: Cv1={Cv1:.2f}, Cv_peak={Cv_peak:.2f}, Cv2={Cv2:.2f} meV/K")
                
                # ç»˜åˆ¶å¸¦å¹³æ»‘å³°çš„çƒ­å®¹æ›²çº¿ï¼ˆä½¿ç”¨é«˜æ–¯å³° + sigmoidè¿‡æ¸¡ï¼‰
                T_plot = np.linspace(temps_unique.min(), temps_unique.max(), 500)
                Cv_plot = np.zeros_like(T_plot)
                
                # å³°çš„å®½åº¦å‚æ•°
                sigma = (T2_first - T1_last) / 2  # é«˜æ–¯å®½åº¦
                
                for i, T in enumerate(T_plot):
                    # åŸºçº¿ï¼šsigmoid ä» Cv1 è¿‡æ¸¡åˆ° Cv2
                    transition = 1 / (1 + np.exp(-(T - T_boundary) / (sigma * 0.5)))
                    baseline = Cv1 + (Cv2 - Cv1) * transition
                    
                    # é«˜æ–¯å³°å åŠ 
                    gaussian = (Cv_peak - baseline) * np.exp(-0.5 * ((T - T_boundary) / sigma)**2)
                    Cv_plot[i] = baseline + gaussian
                
                ax2.plot(T_plot, Cv_plot, 'r-', linewidth=2, zorder=3)
                
                # æ„å»ºå¯¼å‡ºæ•°æ®ï¼ˆå…³é”®ç‚¹ï¼‰
                T_cv = np.array([temps_unique.min(), T1_last, T_boundary, T2_first, temps_unique.max()])
                Cv_curve = np.array([Cv1, Cv1, Cv_peak, Cv2, Cv2])
            else:
                print(f"  çƒ­å®¹: Cv1={Cv1:.2f} meV/K, Cv2={Cv2:.2f} meV/K (æ— å³°)")
                
                # ç»˜åˆ¶é˜¶æ¢¯å½¢çƒ­å®¹æ›²çº¿ï¼ˆæ— å³°ï¼‰
                ax2.plot([temps_unique.min(), T_boundary], [Cv1, Cv1], 'r-', linewidth=2, zorder=3)
                ax2.plot([T_boundary, T_boundary], [Cv1, Cv2], 'r--', linewidth=1.5, zorder=3)
                ax2.plot([T_boundary, temps_unique.max()], [Cv2, Cv2], 'r-', linewidth=2, zorder=3)
                
                T_cv = np.array([temps_unique.min(), T_boundary - 0.1, T_boundary, T_boundary + 0.1, temps_unique.max()])
                Cv_curve = np.array([Cv1, Cv1, (Cv1 + Cv2) / 2, Cv2, Cv2])
    else:
        Cv_single = list(phase_fits.values())[0]['Cv']
        T_cv = np.array([temps_unique.min(), temps_unique.max()])
        Cv_curve = np.array([Cv_single, Cv_single])
        ax2.plot(T_cv, Cv_curve, 'r-', linewidth=2, zorder=3)
        Cv1 = Cv_single
        Cv2 = Cv_single
    
    ax2.set_ylabel('Cv (meV/K)', fontsize=13, fontweight='bold', color='red')
    ax2.tick_params(axis='y', labelcolor='red', labelsize=11, color='red')
    ax2.spines['right'].set_color('red')
    
    # è®¾ç½®Yè½´èŒƒå›´ï¼ˆè€ƒè™‘å³°å€¼ï¼‰
    cv_values = [Cv1, Cv2] if Cv1 and Cv2 else list(Cv_curve)
    if Cv_peak:
        cv_values.append(Cv_peak)
    cv_min = min(cv_values) * 0.85
    cv_max = max(cv_values) * 1.1
    ax2.set_ylim(cv_min, cv_max)
    
    ax1.set_title(f'{structure_name}', fontsize=14, fontweight='bold', pad=10)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    output_file = Path(output_dir) / f'{structure_name}_partition_cv.{output_format}'
    plt.savefig(output_file, dpi=dpi, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"\n  å›¾å·²ä¿å­˜: {output_file}")
    
    # ========== 6. å¯¼å‡ºæ•°æ®ä¾› Origin ä½¿ç”¨ ==========
    # å¯¼å‡ºèƒ½é‡æ•°æ®
    df_energy = pd.DataFrame({
        'Temperature_K': temps_unique,
        'Energy_eV': E_cluster_mean_rel,
        'Energy_std_eV': E_cluster_std,
        'Partition': [temp_to_partition.get(t, 'unknown') for t in temps_unique]
    })
    energy_csv = Path(output_dir) / f'{structure_name}_energy_data.csv'
    df_energy.to_csv(energy_csv, index=False)
    print(f"  èƒ½é‡æ•°æ®å·²å¯¼å‡º: {energy_csv}")
    
    # å¯¼å‡ºçƒ­å®¹æ•°æ®ï¼ˆé˜¶æ¢¯å‡½æ•°å…³é”®ç‚¹ï¼‰
    df_cv = pd.DataFrame({
        'Temperature_K': T_cv,
        'Cv_meV_K': Cv_curve
    })
    cv_csv = Path(output_dir) / f'{structure_name}_cv_curve.csv'
    df_cv.to_csv(cv_csv, index=False)
    print(f"  çƒ­å®¹æ›²çº¿å·²å¯¼å‡º: {cv_csv}")
    
    # å¯¼å‡ºæ‹Ÿåˆå‚æ•°æ±‡æ€»
    fit_summary = {
        'structure': structure_name,
        'T_boundary_K': T_boundary,
        'Cv_overall_meV_K': Cv_overall,
        'Cv_overall_err': Cv_overall_err,
        'R2_overall': R2_overall,
    }
    for i, (phase, fit) in enumerate(phase_fits.items()):
        fit_summary[f'phase_{i+1}_name'] = phase
        fit_summary[f'phase_{i+1}_Cv_meV_K'] = fit['Cv']
        fit_summary[f'phase_{i+1}_Cv_err'] = fit['Cv_err']
        fit_summary[f'phase_{i+1}_R2'] = fit['R2']
        fit_summary[f'phase_{i+1}_T_min_K'] = fit['T_range'][0]
        fit_summary[f'phase_{i+1}_T_max_K'] = fit['T_range'][1]
        fit_summary[f'phase_{i+1}_slope_eV_K'] = fit['slope']
        fit_summary[f'phase_{i+1}_intercept_eV'] = fit['intercept']
    
    fit_csv = Path(output_dir) / f'{structure_name}_fit_params.csv'
    pd.DataFrame([fit_summary]).to_csv(fit_csv, index=False)
    print(f"  æ‹Ÿåˆå‚æ•°å·²å¯¼å‡º: {fit_csv}")
    
    # è¿”å›æ‹Ÿåˆç»“æœ
    return {
        'structure': structure_name,
        'overall': {'Cv': Cv_overall, 'Cv_err': Cv_overall_err, 'R2': R2_overall},
        'partitions': phase_fits
    }


def list_available_structures(base_dir='results/step6_1_clustering'):
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç»“æ„"""
    results = find_clustering_results(base_dir)
    
    print("\n" + "=" * 60)
    print("å¯ç”¨ç»“æ„åˆ—è¡¨")
    print("=" * 60)
    
    # åˆ†ç±»
    air_series = []
    pt6_series = []
    pt8_series = []
    oxide_series = []
    other = []
    
    for name in sorted(results.keys()):
        name_lower = name.lower()
        if 'air' in name_lower:
            air_series.append(name)
        elif name == 'Cv' or 'o' in name_lower:
            oxide_series.append(name)
        elif name_lower.startswith('pt6'):
            pt6_series.append(name)
        elif name_lower.startswith('pt8'):
            pt8_series.append(name)
        else:
            other.append(name)
    
    print(f"\nğŸ”µ æ°”ç›¸å›¢ç°‡ ({len(air_series)}): {', '.join(air_series) if air_series else 'æ— '}")
    print(f"ğŸŸ¢ Pt6ç³»åˆ— ({len(pt6_series)}): {', '.join(sorted(pt6_series)) if pt6_series else 'æ— '}")
    print(f"ğŸŸ¢ Pt8ç³»åˆ— ({len(pt8_series)}): {', '.join(sorted(pt8_series)) if pt8_series else 'æ— '}")
    print(f"ğŸŸ  å«æ°§å›¢ç°‡ ({len(oxide_series)}): {', '.join(sorted(oxide_series)) if oxide_series else 'æ— '}")
    if other:
        print(f"âšª å…¶ä»– ({len(other)}): {', '.join(sorted(other))}")
    
    print(f"\næ€»è®¡: {len(results)} ä¸ªç»“æ„")
    print("=" * 60)
    
    return results


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='åˆ†åŒºçƒ­å®¹æ‹Ÿåˆå›¾ - è®ºæ–‡å‡ºå›¾ä¸“ç”¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  %(prog)s --structure Pt8sn6              # å•ä¸ªç»“æ„
  %(prog)s --structure Air86 --format pdf  # è¾“å‡ºPDF
  %(prog)s --structure all --dpi 600       # æ‰€æœ‰ç»“æ„ï¼Œé«˜åˆ†è¾¨ç‡
  %(prog)s --list                          # åˆ—å‡ºå¯ç”¨ç»“æ„
        '''
    )
    
    parser.add_argument('--structure', '-s', type=str, default=None,
                        help='ç»“æ„åç§° (å¦‚ Pt8sn6, Air86) æˆ– "all" å¤„ç†æ‰€æœ‰')
    parser.add_argument('--list', '-l', action='store_true',
                        help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨ç»“æ„')
    parser.add_argument('--format', '-f', type=str, default='png',
                        choices=['png', 'pdf', 'svg', 'eps'],
                        help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: png)')
    parser.add_argument('--dpi', type=int, default=300,
                        help='è¾“å‡ºåˆ†è¾¨ç‡ (é»˜è®¤: 300)')
    parser.add_argument('--output-dir', '-o', type=str, 
                        default='results/step6_1_1_partition_cv',
                        help='è¾“å‡ºç›®å½•')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print("=" * 70)
    print("Step 6.1.1: åˆ†åŒºçƒ­å®¹æ‹Ÿåˆå›¾ - è®ºæ–‡å‡ºå›¾ä¸“ç”¨")
    print("=" * 70)
    
    # åˆ—å‡ºå¯ç”¨ç»“æ„
    if args.list:
        list_available_structures()
        return
    
    if args.structure is None:
        print("é”™è¯¯: è¯·æŒ‡å®š --structure æˆ–ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨ç»“æ„")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–å¯ç”¨ç»“æ„
    available = find_clustering_results()
    
    if args.structure.lower() == 'all':
        structures = list(available.keys())
        print(f"\nå¤„ç†æ‰€æœ‰ {len(structures)} ä¸ªç»“æ„...")
    else:
        structures = [args.structure]
    
    # å¤„ç†æ¯ä¸ªç»“æ„
    results = []
    success = 0
    failed = 0
    
    for structure in structures:
        # æŸ¥æ‰¾ç»“æ„ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
        found_name = None
        for name in available.keys():
            if name.lower() == structure.lower():
                found_name = name
                break
        
        if found_name is None:
            print(f"\nè­¦å‘Š: æœªæ‰¾åˆ°ç»“æ„ '{structure}'")
            failed += 1
            continue
        
        csv_path = available[found_name]
        df = load_cluster_data(csv_path)
        
        if df is None:
            failed += 1
            continue
        
        result = plot_partition_cv(df, found_name, output_dir, 
                                   args.format, args.dpi)
        
        if result:
            results.append(result)
            success += 1
        else:
            failed += 1
    
    # æ±‡æ€»
    print("\n" + "=" * 70)
    print(f"å¤„ç†å®Œæˆ: æˆåŠŸ {success}, å¤±è´¥ {failed}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 70)
    
    # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
    if results:
        summary_file = output_dir / 'partition_cv_summary.csv'
        rows = []
        for r in results:
            row = {
                'structure': r['structure'],
                'Cv_overall': r['overall']['Cv'],
                'Cv_overall_err': r['overall']['Cv_err'],
                'R2_overall': r['overall']['R2'],
            }
            for i, (phase, fit) in enumerate(r['partitions'].items()):
                row[f'phase_{i+1}'] = phase
                row[f'Cv_{i+1}'] = fit['Cv']
                row[f'Cv_{i+1}_err'] = fit['Cv_err']
                row[f'R2_{i+1}'] = fit['R2']
            rows.append(row)
        
        df_summary = pd.DataFrame(rows)
        df_summary.to_csv(summary_file, index=False)
        print(f"æ±‡æ€»è¡¨å·²ä¿å­˜: {summary_file}")


if __name__ == '__main__':
    main()
