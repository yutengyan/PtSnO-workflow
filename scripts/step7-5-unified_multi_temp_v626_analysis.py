#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€å¤šæ¸©åº¦åˆ†æè„šæœ¬ - v625æ•°æ® (2025-10-26)

æ”¯æŒä¸‰å¤§ç³»åˆ—åˆ†æ,é€šè¿‡å‚æ•°æŒ‡å®š:
1. Pt8Snx: Pt8+0-10Snç³»åˆ— (åŸå­æ•°8-18)
2. PtxSn8-x: æ€»åŸå­æ•°=8çš„ç³»åˆ— (Pt8â†’Pt3Sn5)
3. Pt6Snx: Pt6+0-9Snç³»åˆ— (åŸå­æ•°6-15,ç¼ºpt6sn10)

ç”¨æ³•:
    python step7-5-unified_multi_temp_v625_analysis.py --series Pt8Snx
    python step7-5-unified_multi_temp_v625_analysis.py --series PtxSn8-x
    python step7-5-unified_multi_temp_v625_analysis.py --series Pt6Snx
    python step7-5-unified_multi_temp_v625_analysis.py --all  # è¿è¡Œæ‰€æœ‰ç³»åˆ—

åŠŸèƒ½:
- è‡ªåŠ¨æ£€æµ‹è¿è¡Œæ–‡ä»¶å¤¹(ä½¿ç”¨V625DataLocator)
- å¤šè¿è¡Œå¹³å‡(4-8æ¬¡)
- å®Œæ•´å¯è§†åŒ–(ç»¼åˆå›¾3Ã—10 + çƒ­å›¾2Ã—3 + Q6å¯¹æ¯”2Ã—3)
- é”®ç±»å‹ç»Ÿè®¡(Pt-Pt, Pt-Sn, Sn-Sn)
- æ¸©åº¦æ•ˆåº”å’Œç»„åˆ†æ•ˆåº”åˆ†æ

Author: AI Assistant
Date: 2025-10-26
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import font_manager
from scipy.stats import pearsonr
import os
import sys
import warnings
import argparse
import re
from pathlib import Path
from v625_data_locator import V625DataLocator
import seaborn as sns

# è®¾ç½®æ§åˆ¶å°è¾“å‡ºç¼–ç  - å¿…é¡»åœ¨æœ€æ—©å¯¼å…¥åç«‹å³è®¾ç½®
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    # è®¾ç½®é»˜è®¤ç¼–ç 
    import locale
    locale.setlocale(locale.LC_ALL, 'zh_CN.UTF-8')

warnings.filterwarnings('ignore')

# é…ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial']
plt.rcParams['axes.unicode_minus'] = False

def extract_path_signature(filepath, is_msd_path=True):
    """
    ä»æ–‡ä»¶è·¯å¾„æå–4çº§è·¯å¾„ç­¾å (ä¸Step 7.4å®Œå…¨ä¸€è‡´)
    
    Args:
        filepath: å®Œæ•´æ–‡ä»¶è·¯å¾„
        is_msd_path: True=MSDè·¯å¾„(æœ‰æ¸©åº¦ç›®å½•), False=èƒ½é‡è·¯å¾„(æ— æ¸©åº¦ç›®å½•)
    
    Returns:
        path_signature: 4çº§è·¯å¾„ç­¾å,å¦‚ "run3/o2/o2pt4sn6/t1000.r24.gpu0"
                       æˆ– "parent/composition/t1000.r24.gpu0" (æ— æ‰¹æ¬¡æ—¶)
    
    Examples:
        MSDè·¯å¾„:
        >>> extract_path_signature(
        ...     "D:/data/more/run3/o2/O2Pt4Sn6/1000K/T1000.r24.gpu0_msd_Pt.xvg",
        ...     is_msd_path=True
        ... )
        'run3/o2/o2pt4sn6/t1000.r24.gpu0'
        
        Lindemannè·¯å¾„:
        >>> extract_path_signature(
        ...     "/home/data/run3/o2/O2Pt7Sn7/T200.r0.gpu0",
        ...     is_msd_path=False
        ... )
        'run3/o2/o2pt7sn7/t200.r0.gpu0'
    """
    if not filepath or pd.isna(filepath):
        return None
    
    filepath = str(filepath).replace('\\', '/')
    
    # 1. æå–runä¿¡æ¯ (T1000.r24.gpu0)
    run_match = re.search(r'(T\d+\.r\d+\.gpu\d+)', filepath, re.IGNORECASE)
    if not run_match:
        return None
    run_info = run_match.group(1).lower()
    
    # 2. åˆ†å‰²è·¯å¾„
    parts = re.split(r'[\\/]', filepath)
    
    # 3. æ‰¾åˆ°å…³é”®ç›®å½•çš„ç´¢å¼•
    if is_msd_path:
        # MSDè·¯å¾„: æ‰¾æ¸©åº¦ç›®å½• (1000K)
        key_idx = None
        for i, part in enumerate(parts):
            if re.match(r'\d+K$', part, re.IGNORECASE):
                key_idx = i
                break
    else:
        # èƒ½é‡/Lindemannè·¯å¾„: æ‰¾runæ‰€åœ¨ä½ç½®
        key_idx = None
        for i, part in enumerate(parts):
            if re.search(r'T\d+\.r\d+\.gpu\d+', part, re.IGNORECASE):
                key_idx = i
                break
    
    if key_idx is None or key_idx < 2:
        # æ— æ³•æå–è¶³å¤Ÿçš„å±‚çº§,è¿”å›ç®€åŒ–ç­¾å
        return run_info
    
    # 4. æå–ç›®å½•å±‚çº§
    composition_dir = parts[key_idx - 1].lower()  # O2Pt4Sn6 æˆ– pt8sn5-1-best
    parent_dir = parts[key_idx - 2].lower()       # o2 æˆ– Pt8
    
    # 5. æ£€æŸ¥æ‰¹æ¬¡æ ‡è¯†ç¬¦ (run3, run2, run4, run5)
    batch_keywords = ['run3', 'run2', 'run4', 'run5']
    path_signature = f"{parent_dir}/{composition_dir}/{run_info}"
    
    # å‘ä¸Šæœç´¢æ‰¹æ¬¡æ ‡è¯†ç¬¦ (æœ€å¤šå‘ä¸Š3çº§)
    if key_idx >= 3:
        for check_idx in range(key_idx - 3, max(-1, key_idx - 6), -1):
            if check_idx < 0 or check_idx >= len(parts):
                break
            check_dir = parts[check_idx].lower()
            if check_dir in batch_keywords:
                # æ‰¾åˆ°æ‰¹æ¬¡æ ‡è¯†,æ„å»º4çº§ç­¾å
                path_signature = f"{check_dir}/{parent_dir}/{composition_dir}/{run_info}"
                break
    
    return path_signature

def load_msd_outliers():
    """
    åŠ è½½Step 1çš„MSDå¼‚å¸¸å€¼åˆ—è¡¨
    è¿”å›è·¯å¾„ç­¾åé›†åˆ
    """
    outlier_file = Path(__file__).parent.parent / 'results' / 'large_D_outliers.csv'
    
    if not outlier_file.exists():
        print(f"\n[è­¦å‘Š] MSDå¼‚å¸¸å€¼æ–‡ä»¶ä¸å­˜åœ¨: {outlier_file}")
        print("  å°†ä¸åº”ç”¨MSDè¿‡æ»¤")
        return set()
    
    print(f"\n[MSDè¿‡æ»¤] åŠ è½½å¼‚å¸¸å€¼æ–‡ä»¶: {outlier_file}")
    
    try:
        df_outliers = pd.read_csv(outlier_file)
        
        if 'filepath' not in df_outliers.columns:
            print(f"  [é”™è¯¯] æ–‡ä»¶ç¼ºå°‘filepathåˆ—")
            return set()
        
        # æå–è·¯å¾„ç­¾å
        outlier_signatures = set()
        for filepath in df_outliers['filepath'].dropna():
            sig = extract_path_signature(filepath, is_msd_path=True)
            if sig:
                outlier_signatures.add(sig)
        
        print(f"  åŠ è½½å®Œæˆ: {len(df_outliers)}æ¡å¼‚å¸¸è®°å½•")
        print(f"  å”¯ä¸€è·¯å¾„ç­¾å: {len(outlier_signatures)}ä¸ª")
        
        # æ˜¾ç¤ºå¼‚å¸¸åŸå› åˆ†å¸ƒ
        if 'reason' in df_outliers.columns:
            print(f"\n  å¼‚å¸¸åŸå› åˆ†å¸ƒ:")
            reason_counts = df_outliers['reason'].value_counts()
            for reason, count in reason_counts.items():
                print(f"    {reason}: {count}æ¡ ({count/len(df_outliers)*100:.1f}%)")
        
        return outlier_signatures
        
    except Exception as e:
        print(f"  [é”™è¯¯] åŠ è½½å¤±è´¥: {e}")
        return set()

class UnifiedMultiTempAnalyzer:
    """ç»Ÿä¸€çš„å¤šæ¸©åº¦åˆ†æå™¨"""
    
    # ç³»åˆ—é…ç½®
    SERIES_CONFIGS = {
        'Pt8Snx': {
            'name': 'Pt8Snx',
            'display_name': 'Pt8+Snç³»åˆ—',
            'run_folder': 'Pt8',
            'systems': [
                ('pt8sn0-2-best', 0, 8, 0, 'Pt8'),
                ('pt8sn1-2-best', 1, 8, 1, 'Pt8Sn1'),
                ('pt8sn2-1-best', 2, 8, 2, 'Pt8Sn2'),
                ('pt8sn3-1-best', 3, 8, 3, 'Pt8Sn3'),
                ('pt8sn4-1-best', 4, 8, 4, 'Pt8Sn4'),
                ('pt8sn5-1-best', 5, 8, 5, 'Pt8Sn5'),
                ('pt8sn6-1-best', 6, 8, 6, 'Pt8Sn6'),
                ('pt8sn7-1-best', 7, 8, 7, 'Pt8Sn7'),
                ('pt8sn8-1-best', 8, 8, 8, 'Pt8Sn8'),
                ('pt8sn9-1-best', 9, 8, 9, 'Pt8Sn9'),
                ('pt8sn10-2-best', 10, 8, 10, 'Pt8Sn10')
            ],
            'output_subdir': 'step7.5.unified',  # ç»Ÿä¸€è¾“å‡ºç›®å½•
            'exclude_from_heatmap': []  # é»˜è®¤ä¸å±è”½,ä½¿ç”¨--filterå‚æ•°è‡ªå®šä¹‰
        },
        'PtxSn8-x': {
            'name': 'PtxSn8-x',
            'display_name': 'PtxSn8-xç³»åˆ—(æ€»åŸå­æ•°=8)',
            'run_folders': {
                'Pt8': ['Pt8'],  # Pt8ç³»åˆ—
                'PtxSn8-x': ['PtxSn8-x']  # PtxSn8-xç³»åˆ—
            },
            'systems': [
                ('pt8sn0-2-best', 0, 8, 0, 'Pt8', 'Pt8'),
                ('pt7sn1-1', 1, 7, 1, 'Pt7Sn1', 'PtxSn8-x'),
                ('pt6sn2', 2, 6, 2, 'Pt6Sn2', 'PtxSn8-x'),  # ä¿®æ­£æ–‡ä»¶å¤¹å
                ('pt5sn3-1-best', 3, 5, 3, 'Pt5Sn3', 'PtxSn8-x'),  # ä¿®æ­£æ–‡ä»¶å¤¹å
                ('pt4sn4-2', 4, 4, 4, 'Pt4Sn4', 'PtxSn8-x'),  # ä¿®æ­£æ–‡ä»¶å¤¹å
                ('pt3sn5', 5, 3, 5, 'Pt3Sn5', 'PtxSn8-x')  # ä¿®æ­£æ–‡ä»¶å¤¹å
            ],
            'output_subdir': 'step7.5.unified'  # ç»Ÿä¸€è¾“å‡ºç›®å½•
        },
        'Pt6Snx': {
            'name': 'Pt6Snx',
            'display_name': 'Pt6+Snç³»åˆ—',
            'run_folder': 'Pt6',
            'systems': [
                ('pt6', 0, 6, 0, 'Pt6'),
                ('pt6sn1', 1, 6, 1, 'Pt6Sn1'),
                ('pt6sn2', 2, 6, 2, 'Pt6Sn2'),
                ('pt6sn3', 3, 6, 3, 'Pt6Sn3'),
                ('pt6sn4', 4, 6, 4, 'Pt6Sn4'),
                ('pt6sn5-2', 5, 6, 5, 'Pt6Sn5'),
                ('pt6sn6-2', 6, 6, 6, 'Pt6Sn6'),
                ('pt6sn7', 7, 6, 7, 'Pt6Sn7'),
                ('pt6sn8', 8, 6, 8, 'Pt6Sn8'),
                ('pt6sn9-2', 9, 6, 9, 'Pt6Sn9')
            ],
            'output_subdir': 'step7.5.unified',  # ç»Ÿä¸€è¾“å‡ºç›®å½•
            'exclude_from_heatmap': []  # ä¸å±è”½
        }
    }
    
    def __init__(self, base_path, output_base_dir, series_name, enable_msd_filter=False):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            base_path: v626æ•°æ®æ ¹ç›®å½•
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
            series_name: ç³»åˆ—åç§° ('Pt8Snx', 'PtxSn8-x', 'Pt6Snx')
            enable_msd_filter: æ˜¯å¦å¯ç”¨MSDå¼‚å¸¸å€¼ç­›é€‰ï¼ˆé»˜è®¤Falseï¼‰
        """
        self.base_path = Path(base_path)
        self.output_base_dir = Path(output_base_dir)
        self.series_name = series_name
        self.enable_msd_filter = enable_msd_filter
        
        # è·å–ç³»åˆ—é…ç½®
        if series_name not in self.SERIES_CONFIGS:
            raise ValueError(f"æœªçŸ¥ç³»åˆ—: {series_name}, å¯é€‰: {list(self.SERIES_CONFIGS.keys())}")
        
        self.config = self.SERIES_CONFIGS[series_name]
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = self.output_base_dir / self.config['output_subdir']
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®å®šä½å™¨
        self.locator = V625DataLocator(base_path)
        
        # åŠ è½½ç³»åˆ—é…ç½®
        self.systems = self.config['systems']
        self.temperatures = ['200K', '300K', '400K', '500K', '600K', 
                            '700K', '800K', '900K', '1000K', '1100K']
        self.results = {}
        
        # MSDè¿‡æ»¤ç»Ÿè®¡
        self.msd_filter_stats = {
            'total_runs': 0,
            'filtered_runs': 0,
            'filtered_data_points': 0
        }
        
        # åŠ è½½MSDå¼‚å¸¸è·¯å¾„ç­¾åï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if enable_msd_filter:
            self.msd_outliers = self.load_msd_outliers()
        else:
            self.msd_outliers = set()
        
        print(f"\n{'='*100}")
        print(f"åˆå§‹åŒ–åˆ†æå™¨: {self.config['display_name']}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        if self.enable_msd_filter:
            print(f"âœ… MSDè¿‡æ»¤: å·²å¯ç”¨ ({len(self.msd_outliers)}ä¸ªå¼‚å¸¸è·¯å¾„ç­¾å)")
        else:
            print(f"âš ï¸ MSDè¿‡æ»¤: æœªå¯ç”¨ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®")
        print(f"{'='*100}")
    
    def load_msd_outliers(self):
        """
        åŠ è½½Step 1çš„MSDå¼‚å¸¸è·¯å¾„ç­¾å
        
        Returns:
            set: å¼‚å¸¸è·¯å¾„ç­¾åé›†åˆ
        """
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºscriptsç›®å½•çš„ä¸Šçº§ï¼‰
        script_dir = Path(__file__).parent
        outliers_file = script_dir.parent / 'results' / 'large_D_outliers.csv'
        
        if not outliers_file.exists():
            print(f"\nâš ï¸ æœªæ‰¾åˆ°MSDå¼‚å¸¸æ–‡ä»¶: {outliers_file}")
            print(f"   å°†ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä¸è¿›è¡Œç­›é€‰")
            return set()
        
        try:
            df_outliers = pd.read_csv(outliers_file, encoding='utf-8')
            print(f"\n>>> åŠ è½½MSDå¼‚å¸¸æ•°æ®:")
            print(f"    å¼‚å¸¸è®°å½•æ•°: {len(df_outliers)}")
            
            # ç»Ÿè®¡å¼‚å¸¸åŸå› 
            if 'reason' in df_outliers.columns:
                for reason, count in df_outliers['reason'].value_counts().items():
                    pct = count / len(df_outliers) * 100
                    print(f"      - {reason}: {count} ({pct:.1f}%)")
            
            # æå–è·¯å¾„ç­¾å
            filter_signatures = set()
            for _, row in df_outliers.iterrows():
                filepath = row.get('filepath', '')
                if filepath:
                    sig = extract_path_signature(filepath, is_msd_path=True)
                    if sig:
                        filter_signatures.add(sig)
            
            print(f"    å”¯ä¸€è·¯å¾„ç­¾å: {len(filter_signatures)}")
            return filter_signatures
            
        except Exception as e:
            print(f"\nâš ï¸ åŠ è½½MSDå¼‚å¸¸æ–‡ä»¶å‡ºé”™: {e}")
            print(f"   å°†ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä¸è¿›è¡Œç­›é€‰")
            return set()
    
    def load_coordination_time_series(self, run_path, sys_name, temp):
        """
        åŠ è½½coordinationæ—¶é—´åºåˆ—æ•°æ®
        æ”¯æŒv625æ ¼å¼(300K)å’Œv626æ ¼å¼(T300.r3.gpu0)
        v626ä¼šè‡ªåŠ¨åŠ è½½è¯¥æ¸©åº¦çš„æ‰€æœ‰é‡å¤è¿è¡Œå¹¶è¿”å›åˆ—è¡¨
        
        Returns:
            list[DataFrame] æˆ– None: è¿”å›æ‰€æœ‰è¿è¡Œçš„DataFrameåˆ—è¡¨
        """
        sys_path = run_path / sys_name
        if not sys_path.exists():
            return None
        
        # å°è¯•v625æ ¼å¼: 300K (å•æ¬¡è¿è¡Œ)
        csv_path_v625 = sys_path / temp / 'coordination_time_series.csv'
        if csv_path_v625.exists():
            try:
                df = pd.read_csv(csv_path_v625)
                return [df]  # è¿”å›åˆ—è¡¨æ ¼å¼ä»¥ç»Ÿä¸€å¤„ç†
            except Exception as e:
                return None
        
        # å°è¯•v626æ ¼å¼: T300.r*.gpu* (å¤šæ¬¡è¿è¡Œ)
        temp_value = temp.replace('K', '')
        temp_pattern = f"T{temp_value}.*"
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ‰€æœ‰ç›®å½•
        matching_dirs = sorted(sys_path.glob(temp_pattern))
        
        if matching_dirs:
            # åŠ è½½æ‰€æœ‰è¿è¡Œçš„æ•°æ®
            all_dfs = []
            for temp_dir in matching_dirs:
                csv_path = temp_dir / 'coordination_time_series.csv'
                if csv_path.exists():
                    try:
                        df = pd.read_csv(csv_path)
                        all_dfs.append(df)
                    except Exception as e:
                        pass
            
            if all_dfs:
                return all_dfs  # è¿”å›æ‰€æœ‰è¿è¡Œçš„åˆ—è¡¨
        
        return None
    
    def load_q6_time_series(self, run_path, sys_name, temp):
        """
        åŠ è½½Q6æ—¶é—´åºåˆ—æ•°æ®
        æ”¯æŒv625æ ¼å¼(300K)å’Œv626æ ¼å¼(T300.r3.gpu0)
        v626ä¼šè‡ªåŠ¨åŠ è½½è¯¥æ¸©åº¦çš„æ‰€æœ‰é‡å¤è¿è¡Œå¹¶è¿”å›åˆ—è¡¨
        
        Returns:
            list[DataFrame] æˆ– None: è¿”å›æ‰€æœ‰è¿è¡Œçš„DataFrameåˆ—è¡¨
        """
        sys_path = run_path / sys_name
        if not sys_path.exists():
            return None
        
        # å°è¯•v625æ ¼å¼: 300K (å•æ¬¡è¿è¡Œ)
        csv_path_v625 = sys_path / temp / 'cluster_global_q6_time_series.csv'
        if csv_path_v625.exists():
            try:
                df = pd.read_csv(csv_path_v625)
                if 'cluster_metal_q6_global' not in df.columns:
                    return None
                return [df]  # è¿”å›åˆ—è¡¨æ ¼å¼ä»¥ç»Ÿä¸€å¤„ç†
            except Exception as e:
                return None
        
        # å°è¯•v626æ ¼å¼: T300.r*.gpu* (å¤šæ¬¡è¿è¡Œ)
        temp_value = temp.replace('K', '')
        temp_pattern = f"T{temp_value}.*"
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ‰€æœ‰ç›®å½•
        matching_dirs = sorted(sys_path.glob(temp_pattern))
        
        if matching_dirs:
            # åŠ è½½æ‰€æœ‰è¿è¡Œçš„æ•°æ®
            all_dfs = []
            for temp_dir in matching_dirs:
                csv_path = temp_dir / 'cluster_global_q6_time_series.csv'
                if csv_path.exists():
                    try:
                        df = pd.read_csv(csv_path)
                        if 'cluster_metal_q6_global' not in df.columns:
                            continue
                        all_dfs.append(df)
                    except Exception as e:
                        pass
            
            if all_dfs:
                return all_dfs  # è¿”å›æ‰€æœ‰è¿è¡Œçš„åˆ—è¡¨
        
        return None
    
    def load_geometry_time_series(self, run_path, sys_name, temp):
        """
        åŠ è½½å‡ ä½•æ•°æ®(å›è½¬åŠå¾„ã€è´¨å¿ƒè·ç¦»ç­‰)
        æ”¯æŒv625æ ¼å¼(300K)å’Œv626æ ¼å¼(T300.r3.gpu0)
        v626ä¼šè‡ªåŠ¨åŠ è½½è¯¥æ¸©åº¦çš„æ‰€æœ‰é‡å¤è¿è¡Œå¹¶è¿”å›åˆ—è¡¨
        
        Returns:
            list[DataFrame] æˆ– None: è¿”å›æ‰€æœ‰è¿è¡Œçš„DataFrameåˆ—è¡¨
        """
        sys_path = run_path / sys_name
        if not sys_path.exists():
            return None
        
        # å°è¯•v625æ ¼å¼: 300K (å•æ¬¡è¿è¡Œ)
        csv_path_v625 = sys_path / temp / 'cluster_geometry_time_series.csv'
        if csv_path_v625.exists():
            try:
                df = pd.read_csv(csv_path_v625)
                return [df]  # è¿”å›åˆ—è¡¨æ ¼å¼ä»¥ç»Ÿä¸€å¤„ç†
            except Exception as e:
                return None
        
        # å°è¯•v626æ ¼å¼: T300.r*.gpu* (å¤šæ¬¡è¿è¡Œ)
        temp_value = temp.replace('K', '')
        temp_pattern = f"T{temp_value}.*"
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ‰€æœ‰ç›®å½•
        matching_dirs = sorted(sys_path.glob(temp_pattern))
        
        if matching_dirs:
            # åŠ è½½æ‰€æœ‰è¿è¡Œçš„æ•°æ®
            all_dfs = []
            for temp_dir in matching_dirs:
                csv_path = temp_dir / 'cluster_geometry_time_series.csv'
                if csv_path.exists():
                    try:
                        df = pd.read_csv(csv_path)
                        all_dfs.append(df)
                    except Exception as e:
                        pass
            
            if all_dfs:
                return all_dfs  # è¿”å›æ‰€æœ‰è¿è¡Œçš„åˆ—è¡¨
        
        return None
    
    def load_element_comparison(self, run_path, sys_name, temp):
        """
        åŠ è½½å…ƒç´ å¯¹æ¯”æ•°æ®(Q4ç­‰)
        æ”¯æŒv625æ ¼å¼(300K)å’Œv626æ ¼å¼(T300.r3.gpu0)
        v626ä¼šè‡ªåŠ¨åŠ è½½è¯¥æ¸©åº¦çš„æ‰€æœ‰é‡å¤è¿è¡Œå¹¶è¿”å›åˆ—è¡¨
        
        Returns:
            list[DataFrame] æˆ– None: è¿”å›æ‰€æœ‰è¿è¡Œçš„DataFrameåˆ—è¡¨
        """
        sys_path = run_path / sys_name
        if not sys_path.exists():
            return None
        
        # å°è¯•v625æ ¼å¼: 300K (å•æ¬¡è¿è¡Œ)
        csv_path_v625 = sys_path / temp / 'element_comparison.csv'
        if csv_path_v625.exists():
            try:
                df = pd.read_csv(csv_path_v625)
                return [df]  # è¿”å›åˆ—è¡¨æ ¼å¼ä»¥ç»Ÿä¸€å¤„ç†
            except Exception as e:
                return None
        
        # å°è¯•v626æ ¼å¼: T300.r*.gpu* (å¤šæ¬¡è¿è¡Œ)
        temp_value = temp.replace('K', '')
        temp_pattern = f"T{temp_value}.*"
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ‰€æœ‰ç›®å½•
        matching_dirs = sorted(sys_path.glob(temp_pattern))
        
        if matching_dirs:
            # åŠ è½½æ‰€æœ‰è¿è¡Œçš„æ•°æ®
            all_dfs = []
            for temp_dir in matching_dirs:
                csv_path = temp_dir / 'element_comparison.csv'
                if csv_path.exists():
                    try:
                        df = pd.read_csv(csv_path)
                        all_dfs.append(df)
                    except Exception as e:
                        pass
            
            if all_dfs:
                return all_dfs  # è¿”å›æ‰€æœ‰è¿è¡Œçš„åˆ—è¡¨
        
        return None
    
    def _get_run_paths_for_system(self, system_info):
        """è·å–ç‰¹å®šä½“ç³»çš„è¿è¡Œè·¯å¾„"""
        if self.series_name == 'PtxSn8-x':
            # PtxSn8-xç³»åˆ—éœ€è¦åŒºåˆ†Pt8å’ŒPtxSn8-x
            folder_key = system_info[5]  # 'Pt8' æˆ– 'PtxSn8-x'
            return self.locator.find_all_runs(folder_key)
        else:
            # Pt8Snxå’ŒPt6Snxç³»åˆ—
            return self.locator.find_all_runs(self.config['run_folder'])
    
    def load_system_data(self, system_info):
        """
        åŠ è½½å•ä¸ªä½“ç³»åœ¨ç‰¹å®šæ¸©åº¦ä¸‹çš„æ•°æ®
        
        Args:
            system_info: (sys_name, sn_num, n_pt, n_sn, display_name, [folder_key])
        
        Returns:
            dict: {temp: data_dict}
        """
        sys_name = system_info[0]
        sn_num = system_info[1]
        n_pt = system_info[2]
        n_sn = system_info[3]
        display_name = system_info[4]
        
        # è·å–è¯¥ä½“ç³»çš„è¿è¡Œè·¯å¾„
        run_paths = self._get_run_paths_for_system(system_info)
        
        print(f"\nå¤„ç† {display_name} ({sys_name}, {n_pt}Pt+{n_sn}Sn={n_pt+n_sn}åŸå­)...")
        print(f"  ä½¿ç”¨{len(run_paths)}ä¸ªè¿è¡Œæ–‡ä»¶å¤¹")
        
        temp_results = {}
        
        for temp in self.temperatures:
            run_data_list = []
            
            # è¯»å–æ‰€æœ‰è¿è¡Œçš„æ•°æ®
            for run_path in run_paths:
                try:
                    # åŠ è½½coordinationæ•°æ® - è¿”å›åˆ—è¡¨
                    cn_dfs_list = self.load_coordination_time_series(run_path, sys_name, temp)
                    if cn_dfs_list is None:
                        continue
                    
                    # åŠ è½½Q6æ•°æ® - è¿”å›åˆ—è¡¨
                    q6_dfs_list = self.load_q6_time_series(run_path, sys_name, temp)
                    if q6_dfs_list is None:
                        continue
                    
                    # åŠ è½½å‡ ä½•æ•°æ® - è¿”å›åˆ—è¡¨
                    geo_dfs_list = self.load_geometry_time_series(run_path, sys_name, temp)
                    
                    # åŠ è½½å…ƒç´ å¯¹æ¯”æ•°æ®(Q4ç­‰) - è¿”å›åˆ—è¡¨
                    elem_dfs_list = self.load_element_comparison(run_path, sys_name, temp)
                    
                    # è·å–æ¸©åº¦ç›®å½•åˆ—è¡¨ç”¨äºè·¯å¾„ç­¾åæ£€æŸ¥
                    sys_path = run_path / sys_name
                    temp_value = temp.replace('K', '')
                    temp_dirs = sorted(sys_path.glob(f"T{temp_value}.*"))
                    
                    # å¤„ç†æ¯ä¸ªé‡å¤è¿è¡Œ
                    num_runs = len(cn_dfs_list)
                    for idx in range(num_runs):
                        # MSDç­›é€‰: æ£€æŸ¥å½“å‰è¿è¡Œçš„è·¯å¾„ç­¾å
                        if self.enable_msd_filter and idx < len(temp_dirs):
                            current_dir = temp_dirs[idx]
                            path_signature = extract_path_signature(str(current_dir), is_msd_path=False)
                            
                            if path_signature and path_signature in self.msd_outliers:
                                self.msd_filter_stats['filtered_runs'] += 1
                                continue  # è·³è¿‡è¯¥æ¬¡è¿è¡Œ
                        
                        df_cn = cn_dfs_list[idx]
                        df_q6 = q6_dfs_list[idx]
                        
                        # è®¡ç®—ç»Ÿè®¡é‡
                        pt_cn_total = df_cn['Pt_cn_total'].mean()
                        pt_pt_bonds = df_cn['Pt_cn_Pt_Pt'].mean() if 'Pt_cn_Pt_Pt' in df_cn.columns else 0
                        pt_sn_bonds = df_cn['Pt_cn_Pt_Sn'].mean() if 'Pt_cn_Pt_Sn' in df_cn.columns and n_sn > 0 else 0
                        
                        sn_cn_total = df_cn['Sn_cn_total'].mean() if 'Sn_cn_total' in df_cn.columns and n_sn > 0 else 0
                        sn_sn_bonds = df_cn['Sn_cn_Sn_Sn'].mean() if 'Sn_cn_Sn_Sn' in df_cn.columns and n_sn > 0 else 0
                        sn_pt_bonds = df_cn['Sn_cn_Sn_Pt'].mean() if 'Sn_cn_Sn_Pt' in df_cn.columns and n_sn > 0 else 0
                        
                        # å½’ä¸€åŒ–çš„é”®å¯†åº¦
                        pt_pt_bonds_per_pt = pt_pt_bonds / n_pt if n_pt > 0 else 0
                        pt_sn_bonds_per_pt = pt_sn_bonds / n_pt if n_pt > 0 else 0
                        sn_sn_bonds_per_sn = sn_sn_bonds / n_sn if n_sn > 0 else 0
                        sn_pt_bonds_per_sn = sn_pt_bonds / n_sn if n_sn > 0 else 0
                        
                        run_data = {
                            'q6': df_q6['cluster_metal_q6_global'].mean(),
                            'q6_std': df_q6['cluster_metal_q6_global'].std(),
                            'pt_q6': df_q6['Pt_q6'].mean() if 'Pt_q6' in df_q6.columns else 0,
                            'sn_q6': df_q6['Sn_q6'].mean() if 'Sn_q6' in df_q6.columns and n_sn > 0 else 0,
                            'pt_cn_total': pt_cn_total,
                            'pt_pt_bonds': pt_pt_bonds,
                            'pt_sn_bonds': pt_sn_bonds,
                            'pt_pt_bonds_per_pt': pt_pt_bonds_per_pt,
                            'pt_sn_bonds_per_pt': pt_sn_bonds_per_pt,
                            'sn_cn_total': sn_cn_total,
                            'sn_sn_bonds': sn_sn_bonds,
                            'sn_pt_bonds': sn_pt_bonds,
                            'sn_sn_bonds_per_sn': sn_sn_bonds_per_sn,
                            'sn_pt_bonds_per_sn': sn_pt_bonds_per_sn,
                            'rg': 0, 'rg_std': 0,
                            'pt_dist': 0, 'sn_dist': 0, 'd_sn_pt': 0,
                            'pt_q4': 0, 'sn_q4': 0, 'cluster_q4': 0,
                            'n_pt': n_pt, 'n_sn': n_sn,
                            'total_atoms': n_pt + n_sn
                        }
                        
                        # è¯»å–å‡ ä½•æ•°æ®
                        if geo_dfs_list and idx < len(geo_dfs_list):
                            df_geo = geo_dfs_list[idx]
                            try:
                                run_data['rg'] = df_geo['gyration_radius'].mean()
                                run_data['rg_std'] = df_geo['gyration_radius'].std()
                                run_data['pt_dist'] = df_geo['pt_avg_dist_to_center'].mean()
                                if 'sn_avg_dist_to_center' in df_geo.columns and n_sn > 0:
                                    run_data['sn_dist'] = df_geo['sn_avg_dist_to_center'].mean()
                                    run_data['d_sn_pt'] = run_data['sn_dist'] - run_data['pt_dist']
                            except:
                                pass
                        
                        # è¯»å–Q4æ•°æ®
                        if elem_dfs_list and idx < len(elem_dfs_list):
                            df_elem = elem_dfs_list[idx]
                            try:
                                # element_comparison.csvæ ¼å¼: Element,CN_total,wGCN,Sn_wGCN,Q6,Q4
                                pt_row = df_elem[df_elem['Element'] == 'Pt']
                                if not pt_row.empty and 'Q4' in df_elem.columns:
                                    run_data['pt_q4'] = pt_row['Q4'].values[0]
                                
                                sn_row = df_elem[df_elem['Element'] == 'Sn']
                                if not sn_row.empty and 'Q4' in df_elem.columns and n_sn > 0:
                                    run_data['sn_q4'] = sn_row['Q4'].values[0]
                                
                                # å›¢ç°‡æ•´ä½“Q4 (åŠ æƒå¹³å‡)
                                if n_sn > 0:
                                    run_data['cluster_q4'] = (run_data['pt_q4'] * n_pt + run_data['sn_q4'] * n_sn) / (n_pt + n_sn)
                                else:
                                    run_data['cluster_q4'] = run_data['pt_q4']
                            except:
                                pass
                        
                        run_data_list.append(run_data)
                    
                except Exception as e:
                    continue
            
            # å¦‚æœæœ‰æˆåŠŸçš„è¿è¡Œ,è®¡ç®—å¹³å‡å€¼
            if run_data_list:
                avg_data = {}
                for key in run_data_list[0].keys():
                    values = [rd[key] for rd in run_data_list]
                    if isinstance(values[0], (int, float, np.number)):
                        avg_data[key] = np.mean(values)
                    else:
                        avg_data[key] = values[0]
                
                avg_data['n_runs'] = len(run_data_list)
                temp_results[temp] = avg_data
                
                # è¾“å‡ºä¿¡æ¯
                info_parts = [
                    f"Q6={avg_data['q6']:.3f}",
                    f"Pt-Pt={avg_data['pt_pt_bonds_per_pt']:.3f}",
                    f"Pt-Sn={avg_data['pt_sn_bonds_per_pt']:.3f}"
                ]
                
                # å¦‚æœæœ‰å‡ ä½•æ•°æ®ï¼Œæ·»åŠ Rgå’ŒÎ”d
                if avg_data['rg'] > 0:
                    info_parts.append(f"Rg={avg_data['rg']:.3f}")
                    if n_sn > 0 and avg_data['d_sn_pt'] != 0:
                        info_parts.append(f"Î”d={avg_data['d_sn_pt']:.3f}")
                
                # å¦‚æœæœ‰Q4æ•°æ®ï¼Œæ·»åŠ Q4
                if avg_data['cluster_q4'] > 0:
                    info_parts.append(f"Q4={avg_data['cluster_q4']:.3f}")
                
                info_parts.append(f"(å¹³å‡{len(run_data_list)}æ¬¡è¿è¡Œ)")
                print(f"  [OK] {temp}: {', '.join(info_parts)}")
        
        return temp_results
    
    def collect_all_data(self):
        """æ”¶é›†æ‰€æœ‰ä½“ç³»æ‰€æœ‰æ¸©åº¦çš„æ•°æ®"""
        print("\n" + "="*100)
        print(f"æ”¶é›†{self.config['display_name']}æ•°æ® - å¤šè¿è¡Œå¹³å‡")
        print("="*100)
        
        total_expected = len(self.systems) * len(self.temperatures)
        total_success = 0
        total_runs_used = 0
        run_distribution = {}
        
        for system_info in self.systems:
            sn_num = system_info[1]
            temp_results = self.load_system_data(system_info)
            
            if temp_results:
                self.results[sn_num] = temp_results
                for temp, data in temp_results.items():
                    total_success += 1
                    n_runs = data.get('n_runs', 1)
                    total_runs_used += n_runs
                    
                    if n_runs not in run_distribution:
                        run_distribution[n_runs] = 0
                    run_distribution[n_runs] += 1
        
        print("\n" + "="*100)
        print("æ•°æ®æ”¶é›†å®Œæˆ!")
        print("="*100)
        print(f"\næ•°æ®ç»Ÿè®¡:")
        print(f"  é¢„æœŸæ•°æ®ç‚¹: {total_expected} ({len(self.systems)}ä½“ç³» Ã— {len(self.temperatures)}æ¸©åº¦)")
        print(f"  æˆåŠŸè¯»å–: {total_success} ({total_success/total_expected*100:.1f}%)")
        print(f"  å¤±è´¥: {total_expected - total_success}")
        print(f"  æ€»è¿è¡Œæ¬¡æ•°: {total_runs_used}")
        if total_success > 0:
            print(f"  å¹³å‡æ¯ç‚¹è¿è¡Œæ•°: {total_runs_used/total_success:.2f}")
        
        if run_distribution:
            print(f"\nè¿è¡Œæ•°åˆ†å¸ƒ:")
            for n_runs in sorted(run_distribution.keys()):
                count = run_distribution[n_runs]
                pct = count / total_success * 100
                print(f"    {n_runs}æ¬¡è¿è¡Œ: {count}ä¸ªæ•°æ®ç‚¹ ({pct:.1f}%)")
        
        # æ˜¾ç¤ºMSDç­›é€‰ç»Ÿè®¡
        if self.enable_msd_filter and self.msd_filter_stats['filtered_runs'] > 0:
            print(f"\n[MSDç­›é€‰ç»Ÿè®¡]:")
            total_attempted = total_runs_used + self.msd_filter_stats['filtered_runs']
            filter_rate = self.msd_filter_stats['filtered_runs'] / total_attempted * 100 if total_attempted > 0 else 0
            print(f"  å°è¯•è¯»å–è¿è¡Œ: {total_attempted}")
            print(f"  ä¿ç•™è¿è¡Œ: {total_runs_used} ({100-filter_rate:.1f}%)")
            print(f"  è¿‡æ»¤è¿è¡Œ: {self.msd_filter_stats['filtered_runs']} ({filter_rate:.1f}%)")
        
        print("="*100)
    
    def save_data_table(self):
        """ä¿å­˜æ•°æ®è¡¨"""
        rows = []
        for sn_num in sorted(self.results.keys()):
            for temp in self.temperatures:
                if temp in self.results[sn_num]:
                    data = self.results[sn_num][temp]
                    row = {
                        'series': self.series_name,
                        'sn_num': sn_num,
                        'temperature': temp,
                        'temp_value': int(temp.replace('K', '')),
                        'n_pt': data['n_pt'],
                        'n_sn': data['n_sn'],
                        'total_atoms': data['total_atoms'],
                        'q6': data['q6'],
                        'q6_std': data['q6_std'],
                        'pt_q6': data.get('pt_q6', 0),
                        'sn_q6': data.get('sn_q6', 0),
                        'pt_cn_total': data['pt_cn_total'],
                        'pt_pt_bonds_per_pt': data['pt_pt_bonds_per_pt'],
                        'pt_sn_bonds_per_pt': data['pt_sn_bonds_per_pt'],
                        'sn_cn_total': data.get('sn_cn_total', 0),
                        'sn_sn_bonds_per_sn': data.get('sn_sn_bonds_per_sn', 0),
                        'sn_pt_bonds_per_sn': data.get('sn_pt_bonds_per_sn', 0),
                        'rg': data.get('rg', 0),
                        'd_sn_pt': data.get('d_sn_pt', 0),
                        'n_runs': data.get('n_runs', 1)
                    }
                    rows.append(row)
        
        df = pd.DataFrame(rows)
        csv_file = self.output_dir / f"{self.series_name.lower()}_multi_temp_data.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ æ•°æ®è¡¨å·²ä¿å­˜: {csv_file}")
    
    def plot_comprehensive_analysis(self):
        """ç»˜åˆ¶ç»¼åˆåˆ†æå›¾(6è¡ŒÃ—10åˆ—),æ”¯æŒè¿‡æ»¤æŒ‡å®šSnå«é‡"""
        fig = plt.figure(figsize=(24, 18))
        gs = fig.add_gridspec(6, 10, hspace=0.35, wspace=0.3)
        
        # è·å–è¿‡æ»¤é…ç½®
        exclude_sn = self.config.get('exclude_from_heatmap', [])
        
        # ä¸ºæ¯ä¸ªæ¸©åº¦ç»˜åˆ¶6ä¸ªå­å›¾
        for temp_idx, temp in enumerate(self.temperatures):
            sn_nums = []
            q6_vals = []
            pt_pt_vals = []
            pt_sn_vals = []
            rg_vals = []
            d_sn_pt_vals = []
            q4_vals = []
            
            for sn_num in sorted(self.results.keys()):
                # åº”ç”¨è¿‡æ»¤
                if sn_num in exclude_sn:
                    continue
                    
                if temp in self.results[sn_num]:
                    data = self.results[sn_num][temp]
                    sn_nums.append(sn_num)
                    q6_vals.append(data['q6'])
                    pt_pt_vals.append(data['pt_pt_bonds_per_pt'])
                    pt_sn_vals.append(data['pt_sn_bonds_per_pt'])
                    rg_vals.append(data.get('rg', 0))
                    d_sn_pt_vals.append(data.get('d_sn_pt', 0))
                    q4_vals.append(data.get('cluster_q4', 0))
            
            if not sn_nums:
                continue
            
            # Row 1: Q6 vs Snå«é‡
            ax1 = fig.add_subplot(gs[0, temp_idx])
            ax1.plot(sn_nums, q6_vals, 'o-', linewidth=2, markersize=6, color='blue')
            ax1.set_xlabel('Snå«é‡', fontsize=8)
            ax1.set_ylabel('Q6', fontsize=8)
            ax1.set_title(f'{temp}', fontsize=9, fontweight='bold')
            ax1.grid(True, alpha=0.3)
            ax1.tick_params(labelsize=7)
            
            # Row 2: Pt-Pté”® vs Snå«é‡
            ax2 = fig.add_subplot(gs[1, temp_idx])
            ax2.plot(sn_nums, pt_pt_vals, 's-', linewidth=2, markersize=6, color='green')
            ax2.set_xlabel('Snå«é‡', fontsize=8)
            ax2.set_ylabel('Pt-Pté”®/Pt', fontsize=8)
            ax2.grid(True, alpha=0.3)
            ax2.tick_params(labelsize=7)
            
            # Row 3: Pt-Sné”® vs Snå«é‡
            ax3 = fig.add_subplot(gs[2, temp_idx])
            ax3.plot(sn_nums, pt_sn_vals, '^-', linewidth=2, markersize=6, color='red')
            ax3.set_xlabel('Snå«é‡', fontsize=8)
            ax3.set_ylabel('Pt-Sné”®/Pt', fontsize=8)
            ax3.grid(True, alpha=0.3)
            ax3.tick_params(labelsize=7)
            
            # Row 4: å›è½¬åŠå¾„Rg vs Snå«é‡
            ax4 = fig.add_subplot(gs[3, temp_idx])
            if any(v > 0 for v in rg_vals):
                ax4.plot(sn_nums, rg_vals, 'd-', linewidth=2, markersize=6, color='purple')
                ax4.set_xlabel('Snå«é‡', fontsize=8)
                ax4.set_ylabel('Rg (Ã…)', fontsize=8)
                ax4.grid(True, alpha=0.3)
                ax4.tick_params(labelsize=7)
            else:
                ax4.text(0.5, 0.5, 'No Rg data', ha='center', va='center', transform=ax4.transAxes)
                ax4.axis('off')
            
            # Row 5: è·ç¦»å·®Î”d vs Snå«é‡ (æ ¸å£³æŒ‡æ ‡)
            ax5 = fig.add_subplot(gs[4, temp_idx])
            if any(v != 0 for v in d_sn_pt_vals):
                ax5.plot(sn_nums, d_sn_pt_vals, 'v-', linewidth=2, markersize=6, color='orange')
                ax5.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
                ax5.set_xlabel('Snå«é‡', fontsize=8)
                ax5.set_ylabel('Î”d (Sn-Pt) (Ã…)', fontsize=8)
                ax5.grid(True, alpha=0.3)
                ax5.tick_params(labelsize=7)
            else:
                ax5.text(0.5, 0.5, 'No Î”d data', ha='center', va='center', transform=ax5.transAxes)
                ax5.axis('off')
            
            # Row 6: Q4å››æ¬¡å¯¹ç§°æ€§ vs Snå«é‡
            ax6 = fig.add_subplot(gs[5, temp_idx])
            if any(v > 0 for v in q4_vals):
                ax6.plot(sn_nums, q4_vals, 'h-', linewidth=2, markersize=6, color='brown')
                ax6.set_xlabel('Snå«é‡', fontsize=8)
                ax6.set_ylabel('Q4', fontsize=8)
                ax6.grid(True, alpha=0.3)
                ax6.tick_params(labelsize=7)
            else:
                ax6.text(0.5, 0.5, 'No Q4 data', ha='center', va='center', transform=ax6.transAxes)
                ax6.axis('off')
        
        plt.suptitle(f'{self.config["display_name"]} - å¤šæ¸©åº¦ç»¼åˆåˆ†æ (v626å®Œæ•´ç‰ˆ)', 
                     fontsize=16, fontweight='bold', y=0.995)
        
        output_file = self.output_dir / f'{self.series_name.lower()}_comprehensive_analysis.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š ç»¼åˆåˆ†æå›¾å·²ä¿å­˜: {output_file}")
    
    def plot_heatmaps(self):
        """ç»˜åˆ¶çƒ­å›¾,æ”¯æŒå±è”½æŒ‡å®šSnå«é‡"""
        fig, axes = plt.subplots(3, 3, figsize=(22, 16))
        
        # å‡†å¤‡æ•°æ® - è¿‡æ»¤æ‰éœ€è¦å±è”½çš„Snå«é‡
        exclude_sn = self.config.get('exclude_from_heatmap', [])
        sn_nums = [sn for sn in sorted(self.results.keys()) if sn not in exclude_sn]
        temps = self.temperatures
        
        if exclude_sn:
            print(f"\n[çƒ­å›¾] å±è”½Snå«é‡: {exclude_sn}")
        
        def create_matrix(field):
            matrix = []
            for temp in temps:
                row = []
                for sn_num in sn_nums:
                    if temp in self.results[sn_num]:
                        value = self.results[sn_num][temp].get(field, 0)
                        # å¯¹äºæ–°å¢å­—æ®µï¼Œå¦‚æœä¸º0å¯èƒ½è¡¨ç¤ºæ²¡æœ‰æ•°æ®
                        if field in ['rg', 'd_sn_pt', 'cluster_q4'] and value == 0:
                            row.append(np.nan)
                        else:
                            row.append(value)
                    else:
                        row.append(np.nan)
                matrix.append(row)
            return np.array(matrix)
        
        # 9ä¸ªçƒ­å›¾
        fields = [
            ('q6', 'Q6 å…­æ¬¡å¯¹ç§°æ€§'),
            ('pt_pt_bonds_per_pt', 'Pt-Pté”®/Pt'),
            ('pt_sn_bonds_per_pt', 'Pt-Sné”®/Pt'),
            ('rg', 'å›è½¬åŠå¾„ Rg (Ã…)'),
            ('d_sn_pt', 'è·ç¦»å·® Î”d(Sn-Pt) (Ã…)'),
            ('cluster_q4', 'Q4 å››æ¬¡å¯¹ç§°æ€§'),
            ('sn_sn_bonds_per_sn', 'Sn-Sné”®/Sn'),
            ('pt_dist', 'Ptåˆ°è´¨å¿ƒè·ç¦» (Ã…)'),
            ('sn_dist', 'Snåˆ°è´¨å¿ƒè·ç¦» (Ã…)')
        ]
        
        for idx, (field, title) in enumerate(fields):
            ax = axes[idx // 3, idx % 3]
            matrix = create_matrix(field)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            if np.all(np.isnan(matrix)):
                ax.text(0.5, 0.5, f'No {field} data', 
                       ha='center', va='center', fontsize=12, transform=ax.transAxes)
                ax.set_title(title, fontsize=11, fontweight='bold')
                ax.axis('off')
                continue
            
            # ç‰¹æ®Šå¤„ç†Î”dçš„colormap (ä¸­å¿ƒä¸º0)
            if field == 'd_sn_pt':
                vmax = np.nanmax(np.abs(matrix))
                im = ax.imshow(matrix, aspect='auto', cmap='RdBu_r', 
                              interpolation='nearest', vmin=-vmax, vmax=vmax)
            else:
                im = ax.imshow(matrix, aspect='auto', cmap='RdYlBu_r', interpolation='nearest')
            
            ax.set_xticks(range(len(sn_nums)))
            ax.set_xticklabels([f'Sn{s}' for s in sn_nums], fontsize=8)
            ax.set_yticks(range(len(temps)))
            ax.set_yticklabels(temps, fontsize=8)
            ax.set_xlabel('Snå«é‡', fontsize=9)
            ax.set_ylabel('æ¸©åº¦', fontsize=9)
            ax.set_title(title, fontsize=10, fontweight='bold')
            
            # æ·»åŠ æ•°å€¼æ ‡æ³¨
            for i in range(len(temps)):
                for j in range(len(sn_nums)):
                    if not np.isnan(matrix[i, j]):
                        text = ax.text(j, i, f'{matrix[i, j]:.2f}',
                                     ha="center", va="center", color="black", fontsize=6)
            
            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        
        plt.suptitle(f'{self.config["display_name"]} - çƒ­å›¾åˆ†æ (v626å®Œæ•´ç‰ˆ)', 
                    fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        output_file = self.output_dir / f'{self.series_name.lower()}_heatmap.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š çƒ­å›¾å·²ä¿å­˜: {output_file}")
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        print(f"\n{'='*100}")
        print(f"å¼€å§‹{self.config['display_name']}å®Œæ•´åˆ†æ")
        print(f"{'='*100}")
        
        # 1. æ”¶é›†æ•°æ®
        self.collect_all_data()
        
        # 2. ä¿å­˜æ•°æ®è¡¨
        self.save_data_table()
        
        # 3. ç»˜åˆ¶å›¾è¡¨
        self.plot_comprehensive_analysis()
        self.plot_heatmaps()
        
        print(f"\n{'='*100}")
        print(f"âœ… {self.config['display_name']}åˆ†æå®Œæˆ!")
        print(f"{'='*100}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€å¤šæ¸©åº¦åˆ†æè„šæœ¬ - v625æ•°æ®')
    parser.add_argument('--series', type=str, choices=['Pt8Snx', 'PtxSn8-x', 'Pt6Snx'],
                       help='æŒ‡å®šè¦åˆ†æçš„ç³»åˆ—')
    parser.add_argument('--all', action='store_true',
                       help='è¿è¡Œæ‰€æœ‰ç³»åˆ—çš„åˆ†æ')
    parser.add_argument('--filter', type=str, default='',
                       help='è‡ªå®šä¹‰è¿‡æ»¤Snå«é‡(é€—å·åˆ†éš”),ä¾‹å¦‚: --filter 0,1,2 å±è”½Sn0/Sn1/Sn2')
    parser.add_argument('--enable-msd-filter', action='store_true',
                       help='å¯ç”¨Step 1çš„MSDå¼‚å¸¸å€¼ç­›é€‰(ç§»é™¤æ•°æ®è´¨é‡å·®çš„è¿è¡Œ)')
    
    args = parser.parse_args()
    
    # è·¯å¾„é…ç½® (ä½¿ç”¨v626æ•°æ®)
    base_path = Path(__file__).parent / 'data' / 'coordination' / 'coordination_time_series_results_sample_20251106_214943'
    output_base_dir = Path(__file__).parent / 'results'
    
    # ç¡®å®šè¦è¿è¡Œçš„ç³»åˆ—
    if args.all:
        series_list = ['Pt8Snx', 'PtxSn8-x', 'Pt6Snx']
    elif args.series:
        series_list = [args.series]
    else:
        print("é”™è¯¯: è¯·æŒ‡å®š --series æˆ– --all")
        print("ç”¨æ³•ç¤ºä¾‹:")
        print("  python step7-5-unified_multi_temp_v626_analysis.py --series Pt8Snx")
        print("  python step7-5-unified_multi_temp_v626_analysis.py --all")
        print("  python step7-5-unified_multi_temp_v626_analysis.py --series Pt8Snx --enable-msd-filter")
        return
    
    # è¿è¡Œåˆ†æ
    for series in series_list:
        try:
            analyzer = UnifiedMultiTempAnalyzer(
                base_path=base_path,
                output_base_dir=output_base_dir,
                series_name=series,
                enable_msd_filter=args.enable_msd_filter
            )
            
            # åº”ç”¨è‡ªå®šä¹‰è¿‡æ»¤
            if args.filter:
                filter_sn = [int(x.strip()) for x in args.filter.split(',')]
                analyzer.config['exclude_from_heatmap'] = filter_sn
                print(f"\n[è‡ªå®šä¹‰è¿‡æ»¤] å±è”½Snå«é‡: {filter_sn}")
            
            analyzer.run_analysis()
                
        except Exception as e:
            print(f"\nâŒ {series}åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue


if __name__ == '__main__':
    main()
