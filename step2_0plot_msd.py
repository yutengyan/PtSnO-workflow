#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step3: ç»˜åˆ¶MSDæ›²çº¿ - é«˜é€Ÿç‰ˆ (æ–‡ä»¶ç´¢å¼•ä¼˜åŒ–)
========================================
åˆ›å»ºæ—¶é—´: 2025-10-16
æœ€åæ›´æ–°: 2025-11-26
ä½œè€…: GitHub Copilot

åŠŸèƒ½:
    1. ç›´æ¥æ‰«æ GMX MSD æ•°æ®ç›®å½•æ„å»ºæ–‡ä»¶ç´¢å¼•
    2. è‡ªåŠ¨å‘ç°æ‰€æœ‰ç³»ç»Ÿå’Œæ¸©åº¦
    3. æŒ‰ç³»ç»Ÿå’Œæ¸©åº¦ç»˜åˆ¶MSDæ›²çº¿
    4. æ”¯æŒå¼‚å¸¸å€¼ç­›é€‰ (å¯é€‰, ä½¿ç”¨ step1_2 çš„ large_D_outliers.csv)
    5. åŒæ—¶æ˜¾ç¤ºæœ‰æ•ˆrunså’Œè¢«ç­›é€‰runs (ä¸åŒæ ·å¼)

å·¥ä½œæµç¨‹:
    - å¯ç‹¬ç«‹è¿è¡Œ,æ— éœ€å…ˆè¿è¡Œå…¶ä»–è„šæœ¬
    - å¦‚éœ€ç­›é€‰å¼‚å¸¸runs,éœ€è¦ step1_2 ç”Ÿæˆçš„ large_D_outliers.csv

ä¾èµ–å…³ç³»:
    - å¯é€‰: step1_2 çš„ large_D_outliers.csv ç”¨äºç­›é€‰å¼‚å¸¸runs
    - æ— å¿…éœ€ä¾èµ– (ç³»ç»Ÿåˆ—è¡¨ä»æ–‡ä»¶ç›´æ¥å‘ç°)

æ€§èƒ½ä¼˜åŒ–:
    1. ä¸€æ¬¡æ€§æ„å»ºæ–‡ä»¶ç´¢å¼• (é¿å…é‡å¤rglob)
    2. ç¼“å­˜æ–‡ä»¶è·¯å¾„æ˜ å°„
    3. é¢„è®¡é€Ÿåº¦æå‡: 10-20å€

è¾“å…¥:
    - GMX MSDåŸå§‹æ•°æ® (.xvgæ–‡ä»¶,ä»GMX_DATA_DIRSè¯»å–)
    - [å¯é€‰] step1_2çš„å¼‚å¸¸æ¸…å• (large_D_outliers.csv)

è¾“å‡º: results/msd_curves/
    â”œâ”€â”€ {system}_all_temps_GMX.png      (å„ç³»ç»Ÿçš„MSDæ›²çº¿å›¾)
    â””â”€â”€ filtering_statistics.txt        (ç­›é€‰ç»Ÿè®¡æŠ¥å‘Š)

ä½¿ç”¨æ–¹æ³•:
--------
python step3_plot_msd.py              # é»˜è®¤å¯ç”¨å¼‚å¸¸å€¼ç­›é€‰
python step3_plot_msd.py --nofilter   # å…³é—­ç­›é€‰ï¼Œç»˜åˆ¶æ‰€æœ‰æ›²çº¿
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime
import re
import warnings
from collections import defaultdict
from tqdm import tqdm
import argparse
warnings.filterwarnings('ignore')

# ===== å…¨å±€é…ç½® =====
BASE_DIR = Path(__file__).parent  # workflowç›®å½•

# æ•°æ®æº: step1_2 çš„è¾“å‡ºç»“æœ
# - ensemble_analysis_results.csv: æœ‰æ•ˆrunsçš„é›†æˆå¹³å‡ (ç”¨äºè·å–ç³»ç»Ÿåˆ—è¡¨)
# - large_D_outliers.csv: å¼‚å¸¸runsæ¸…å• (ç”¨äºç­›é€‰ç»˜å›¾)
RESULTS_CSV = BASE_DIR / 'results' / 'ensemble_analysis_results.csv'
OUTLIERS_CSV = BASE_DIR / 'results' / 'large_D_outliers.csv'

GMX_DATA_DIRS = [
    # BASE_DIR / 'data' / 'gmx_msd' / 'collected_gmx_msd',
    # BASE_DIR / 'data' / 'gmx_msd' / 'gmx_msd_results_20251015_184626_collected',
    # æ–°ç‰ˆunwrap per-atom MSDæ•°æ® (2025-11-18)
    BASE_DIR / 'data' / 'gmx_msd' / 'unwrap' / 'gmx_msd_results_20251118_152614',
    # BASE_DIR / 'data' / 'gmx_msd' / 'unwrap' / 'air' / 'gmx_msd_results_20251124_170114'  # ğŸŒ¬ï¸ æ°”è±¡æ•°æ®
]

OUTPUT_DIR = BASE_DIR / 'results' / 'msd_curves'
OUTPUT_DIR.mkdir(exist_ok=True, parents=True)

# ===== å¼‚å¸¸å€¼ç­›é€‰å¼€å…³ï¼ˆå¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ï¼‰ =====
# é»˜è®¤å€¼ï¼šTrue = å¯ç”¨ç­›é€‰ï¼ŒFalse = ç»˜åˆ¶æ‰€æœ‰æ›²çº¿
# å‘½ä»¤è¡Œï¼š--nofilter å°†è¦†ç›–æ­¤è®¾ç½®
ENABLE_OUTLIER_FILTERING = True  # é»˜è®¤å¯ç”¨ç­›é€‰

# ===== ç³»ç»Ÿè¿‡æ»¤é…ç½® (å¯é€‰) =====
# å¦‚æœä¸ºç©ºåˆ—è¡¨,åˆ™ç»˜åˆ¶æ‰€æœ‰ç³»ç»Ÿ
# å¦‚æœæŒ‡å®šæ¨¡å¼,åˆ™åªç»˜åˆ¶åŒ¹é…çš„ç³»ç»Ÿ
SYSTEM_FILTER = {
    'include_patterns': [
        # ç¤ºä¾‹: åªç»˜åˆ¶Pt8ç›¸å…³ç³»ç»Ÿ
        r'^pt8',           # pt8å¼€å¤´çš„æ‰€æœ‰ç³»ç»Ÿ
        # r'pt8sn\d+',       # pt8sn0, pt8sn1, ..., pt8sn10
        # r'pt\d+sn\d+',     # æ‰€æœ‰ptXsnYæ ¼å¼
        # r'^\d+$',          # æ°”è±¡æ•°æ® (çº¯æ•°å­—å‘½åï¼Œå¦‚ 68)
    ],
    'exclude_patterns': [
        # ç¤ºä¾‹: æ’é™¤å«æ°§ç³»ç»Ÿ
        # r'^[Oo]\d+',       # O1, O2, o3, o4ç­‰å¼€å¤´
        # r'[Oo]\d+Pt',      # O2Pt4Sn6ç­‰
        # r'Pt\d+Sn\d+O',    # Pt2Sn2O1ç­‰
    ]
}

COLORS = {
    'Pt': '#E74C3C',
    'Sn': '#3498DB',
    'PtSn': '#2ECC71'
}

plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def load_large_D_outliers(enable_filtering=None):
    """
    åŠ è½½å¤§Då€¼å¼‚å¸¸runæ¸…å•
    
    Parameters:
    -----------
    enable_filtering : bool, optional
        æ˜¯å¦å¯ç”¨ç­›é€‰ã€‚å¦‚æœä¸º Noneï¼Œä½¿ç”¨å…¨å±€é…ç½®
    
    Returns:
    --------
    outliers : set
        å¼‚å¸¸æ–‡ä»¶è·¯å¾„é›†åˆ
    """
    # ä½¿ç”¨å‚æ•°æˆ–å…¨å±€é…ç½®
    if enable_filtering is None:
        enable_filtering = ENABLE_OUTLIER_FILTERING
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç­›é€‰
    if not enable_filtering:
        print(f"   [!] Outlier filtering is DISABLED (--nofilter)")
        print(f"   [!] Will plot ALL runs (including outliers)")
        return set()
    
    try:
        df_outliers = pd.read_csv(OUTLIERS_CSV)
        outlier_files = set(df_outliers['filepath'].values)
        print(f"   [OK] Loaded outlier list: {len(outlier_files)} files")
        print(f"        (These runs will be excluded from plotting)")
        return outlier_files
    except FileNotFoundError:
        print(f"   [!] Outlier file not found: {OUTLIERS_CSV}")
        print(f"   [!] Will plot all runs without filtering")
        return set()


def build_file_index(outlier_files=None):
    """
    æ„å»ºæ–‡ä»¶ç´¢å¼• - ä¸€æ¬¡æ€§æ‰«ææ‰€æœ‰æ–‡ä»¶,åˆ†åˆ«ç´¢å¼•æœ‰æ•ˆrunså’Œè¢«ç­›é€‰runs
    
    Parameters:
    -----------
    outlier_files : set, optional
        å¼‚å¸¸æ–‡ä»¶è·¯å¾„é›†åˆ
    
    Returns:
    --------
    file_index : dict
        {(composition, temperature, element): [file_path1, file_path2, ...]} - æœ‰æ•ˆruns
    file_index_filtered : dict
        {(composition, temperature, element): [file_path1, file_path2, ...]} - è¢«ç­›é€‰runs
    filter_stats : dict
        {(composition, temperature, element): {'total': int, 'kept': int, 'filtered': int}}
    """
    if outlier_files is None:
        outlier_files = set()
    
    print("\n[*] Building file index...")
    file_index = defaultdict(list)
    file_index_filtered = defaultdict(list)  # æ–°å¢: è¢«ç­›é€‰runsçš„ç´¢å¼•
    filter_stats = defaultdict(lambda: {'total': 0, 'kept': 0, 'filtered': 0})
    
    total_files = 0
    filtered_files = 0
    seen_files = set()  # ç”¨äºå»é‡
    
    for gmx_dir in GMX_DATA_DIRS:
        print(f"  Scanning: {gmx_dir.name}...")
        
        # æŸ¥æ‰¾æ‰€æœ‰MSD .xvgæ–‡ä»¶
        for xvg_file in gmx_dir.rglob("*_msd_*.xvg"):
            # å»é‡æ£€æŸ¥
            try:
                normalized_path = xvg_file.resolve()
                if normalized_path in seen_files:
                    continue  # è·³è¿‡é‡å¤æ–‡ä»¶
                seen_files.add(normalized_path)
            except:
                pass  # å¦‚æœresolve()å¤±è´¥,ç»§ç»­å¤„ç†
            try:
                # è§£æè·¯å¾„: ä»çˆ¶ç›®å½•å±‚çº§æå–ä¿¡æ¯
                parts = xvg_file.parts
                
                # æå–element (ä»æ–‡ä»¶å)
                filename = xvg_file.stem  # T1000.r7.gpu0_msd_Pt
                if '_msd_' in filename:
                    element = filename.split('_msd_')[-1]  # Pt, Sn, PtSn
                else:
                    continue
                
                # æå–temperatureå’Œcomposition (ä»è·¯å¾„å‘ä¸ŠæŸ¥æ‰¾)
                temperature = None
                composition = None
                for i in range(len(parts)-1, 0, -1):
                    if parts[i].endswith('K'):
                        temperature = parts[i]
                        composition = parts[i-1]
                        break
                
                if not temperature or not composition:
                    continue
                
                key = (composition, temperature, element)
                filter_stats[key]['total'] += 1
                total_files += 1
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¼‚å¸¸run
                if str(xvg_file) in outlier_files:
                    filter_stats[key]['filtered'] += 1
                    file_index_filtered[key].append(xvg_file)  # æ·»åŠ åˆ°ç­›é€‰ç´¢å¼•
                    filtered_files += 1
                else:
                    filter_stats[key]['kept'] += 1
                    file_index[key].append(xvg_file)
                
            except Exception as e:
                continue
    
    print(f"   [OK] Indexed {total_files - filtered_files} valid files")
    print(f"   [OK] Indexed {filtered_files} filtered files")
    print(f"   [OK] Filtered out {filtered_files} outlier runs")
    
    return file_index, file_index_filtered, filter_stats


def extract_base_system(composition_name):
    """æå–åŸºç¡€ä½“ç³»åç§°"""
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆé˜²æ­¢æ•´æ•°ç­‰ç±»å‹ï¼‰
    composition_name = str(composition_name)
    match = re.match(r'^(Cv)-\d+$', composition_name)
    if match:
        return match.group(1)
    return composition_name


def group_compositions_by_system(df):
    """æŒ‰ä½“ç³»åˆ†ç»„"""
    df['base_system'] = df['composition'].apply(extract_base_system)
    
    system_groups = {}
    for base_sys in df['base_system'].unique():
        comps = df[df['base_system'] == base_sys]['composition'].unique()
        system_groups[base_sys] = sorted(comps)
    
    return system_groups


def filter_systems(system_groups, include_patterns=None, exclude_patterns=None):
    """
    æ ¹æ®æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼è¿‡æ»¤ç³»ç»Ÿ
    
    Parameters:
    -----------
    system_groups : dict
        {base_system: [compositions]}
    include_patterns : list of str
        åŒ…å«æ¨¡å¼åˆ—è¡¨ (æ­£åˆ™è¡¨è¾¾å¼)
        å¦‚æœä¸ºç©º,åˆ™åŒ…å«æ‰€æœ‰ç³»ç»Ÿ
    exclude_patterns : list of str
        æ’é™¤æ¨¡å¼åˆ—è¡¨ (æ­£åˆ™è¡¨è¾¾å¼)
    
    Returns:
    --------
    filtered_groups : dict
        è¿‡æ»¤åçš„ç³»ç»Ÿå­—å…¸
    """
    if include_patterns is None:
        include_patterns = []
    if exclude_patterns is None:
        exclude_patterns = []
    
    filtered_groups = {}
    
    for base_sys, comps in system_groups.items():
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…includeæ¨¡å¼
        if include_patterns:
            included = any(re.search(pattern, base_sys, re.IGNORECASE) 
                          for pattern in include_patterns)
            if not included:
                continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…excludeæ¨¡å¼
        if exclude_patterns:
            excluded = any(re.search(pattern, base_sys, re.IGNORECASE) 
                          for pattern in exclude_patterns)
            if excluded:
                continue
        
        # é€šè¿‡è¿‡æ»¤
        filtered_groups[base_sys] = comps
    
    return filtered_groups


def read_gmx_msd_xvg(filepath):
    """è¯»å–GMX MSD .xvgæ–‡ä»¶"""
    time_data = []
    msd_data = []
    
    try:
        with open(filepath, 'r') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#') or line.startswith('@'):
                    continue
                parts = line.split()
                if len(parts) >= 2:
                    try:
                        t = float(parts[0])
                        msd_nm2 = float(parts[1])
                        msd_a2 = msd_nm2 * 100
                        time_data.append(t)
                        msd_data.append(msd_a2)
                    except ValueError:
                        continue
    except:
        return None, None
    
    if len(time_data) == 0:
        return None, None
    
    return np.array(time_data), np.array(msd_data)


def plot_system_all_temps_fast(base_system, compositions, df_all, file_index, file_index_filtered, filter_stats, max_temps=None, use_file_temps=False):
    """
    å¿«é€Ÿç»˜åˆ¶ - ä½¿ç”¨æ–‡ä»¶ç´¢å¼•,åŒæ—¶ç»˜åˆ¶æœ‰æ•ˆrunså’Œè¢«ç­›é€‰runs
    
    Parameters:
    -----------
    base_system : str
        åŸºç¡€ä½“ç³»åç§°
    compositions : list
        ç»„æˆåˆ—è¡¨
    df_all : DataFrame
        æ‰€æœ‰æ•°æ®
    file_index : dict
        æœ‰æ•ˆrunsçš„æ–‡ä»¶ç´¢å¼•
    file_index_filtered : dict
        è¢«ç­›é€‰runsçš„æ–‡ä»¶ç´¢å¼•
    filter_stats : dict
        ç­›é€‰ç»Ÿè®¡
    max_temps : int, optional
        æœ€å¤§æ¸©åº¦æ•°
    use_file_temps : bool, optional
        æ˜¯å¦ä»æ–‡ä»¶ç´¢å¼•è·å–æ¸©åº¦åˆ—è¡¨ï¼ˆç”¨äº --nofilter æ¨¡å¼ï¼‰
    """
    
    print(f"\n{'='*80}")
    print(f"System: {base_system}")
    if len(compositions) > 1:
        print(f"Compositions: {', '.join(compositions)}")
    print(f"{'='*80}")
    
    df_sys = df_all[df_all['composition'].isin(compositions)]
    
    # è·å–æ¸©åº¦åˆ—è¡¨
    if use_file_temps:
        # ä»æ–‡ä»¶ç´¢å¼•è·å–æ‰€æœ‰å¯ç”¨æ¸©åº¦ï¼ˆç”¨äº --nofilterï¼‰
        temp_set = set()
        for key in list(file_index.keys()) + list(file_index_filtered.keys()):
            comp_str, temp, elem = key
            # æ£€æŸ¥ç»„æˆæ˜¯å¦åŒ¹é…
            if any(str(c) == comp_str for c in compositions):
                temp_set.add(temp)
        
        if not temp_set:
            print(f"[X] No files found in file index")
            return
        
        temperatures = sorted(temp_set, key=lambda x: int(x.replace('K', '')))
        print(f"Temperatures (from files): {len(temperatures)} - {temperatures}")
    else:
        # ä» DataFrame è·å–æ¸©åº¦ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
        if len(df_sys) == 0:
            print(f"[X] No data in DataFrame")
            return
        
        temperatures = sorted(df_sys['temperature'].unique(),
                             key=lambda x: int(x.replace('K', '')))
    
    print(f"Data points: {len(df_sys)}")
    print(f"Temperatures: {len(temperatures)} - {temperatures}")
    
    if max_temps and len(temperatures) > max_temps:
        indices = np.linspace(0, len(temperatures)-1, max_temps, dtype=int)
        temperatures = [temperatures[i] for i in indices]
    
    n_temps = len(temperatures)
    n_cols = min(3, n_temps)
    n_rows = (n_temps + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(7*n_cols, 5*n_rows))
    if n_temps == 1:
        axes = [axes]
    else:
        axes = axes.flatten()
    
    # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°æ® + æ‰¾æœ€å¤§å€¼
    print("\n[*] Loading MSD data...")
    
    msd_cache = {}  # {(temp, element): [(time, msd), ...]} - æœ‰æ•ˆruns
    msd_cache_filtered = {}  # {(temp, element): [(time, msd), ...]} - è¢«ç­›é€‰runs
    global_max_msd = 0
    
    # ç»Ÿè®¡æ¯ä¸ªæ¸©åº¦çš„ç­›é€‰æƒ…å†µ
    temp_filter_stats = {}  # {temp: {'total': int, 'kept': int, 'filtered': int}}
    
    for temp in temperatures:
        temp_stats = {'total': 0, 'kept': 0, 'filtered': 0}
        
        for element in ['Pt', 'Sn', 'PtSn']:
            msd_list = []
            msd_list_filtered = []
            
            # ä»ç´¢å¼•æŸ¥æ‰¾æ–‡ä»¶
            for comp in compositions:
                # ç¡®ä¿ composition æ˜¯å­—ç¬¦ä¸²ï¼ˆæ–‡ä»¶ç´¢å¼•çš„keyæ˜¯å­—ç¬¦ä¸²ï¼‰
                key = (str(comp), temp, element)
                
                # ç´¯åŠ ç»Ÿè®¡
                if key in filter_stats:
                    temp_stats['total'] += filter_stats[key]['total']
                    temp_stats['kept'] += filter_stats[key]['kept']
                    temp_stats['filtered'] += filter_stats[key]['filtered']
                
                # è¯»å–æœ‰æ•ˆruns
                files = file_index.get(key, [])
                for filepath in files:
                    time, msd = read_gmx_msd_xvg(filepath)
                    if time is not None:
                        msd_list.append((time, msd))
                        
                        # æ›´æ–°å…¨å±€æœ€å¤§å€¼
                        max_val = np.max(msd)
                        if max_val > global_max_msd:
                            global_max_msd = max_val
                
                # è¯»å–è¢«ç­›é€‰runs
                files_filtered = file_index_filtered.get(key, [])
                for filepath in files_filtered:
                    time, msd = read_gmx_msd_xvg(filepath)
                    if time is not None:
                        msd_list_filtered.append((time, msd))
                        
                        # æ›´æ–°å…¨å±€æœ€å¤§å€¼
                        max_val = np.max(msd)
                        if max_val > global_max_msd:
                            global_max_msd = max_val
            
            if msd_list:
                msd_cache[(temp, element)] = msd_list
            if msd_list_filtered:
                msd_cache_filtered[(temp, element)] = msd_list_filtered
        
        temp_filter_stats[temp] = temp_stats
    
    total_curves = sum(len(v) for v in msd_cache.values())
    total_curves_filt = sum(len(v) for v in msd_cache_filtered.values())
    print(f"  [OK] Loaded {total_curves} valid curves + {total_curves_filt} filtered curves")
    print(f"  [OK] Global max MSD: {global_max_msd:.2f} A^2")
    
    # ç»˜åˆ¶
    print("\n[*] Plotting...")
    
    for idx, temp in enumerate(temperatures):
        ax = axes[idx]
        has_data = False
        
        # è·å–è¯¥æ¸©åº¦çš„ç­›é€‰ç»Ÿè®¡
        stats = temp_filter_stats.get(temp, {'total': 0, 'kept': 0, 'filtered': 0})
        
        # === ç»˜åˆ¶æœ‰æ•ˆruns (å®çº¿,é¥±å’Œé¢œè‰²) ===
        for element in ['Pt', 'Sn', 'PtSn']:
            msd_list = msd_cache.get((temp, element), [])
            
            if not msd_list:
                continue
            
            # å¯¹é½æ—¶é—´è½´
            all_times = [t for t, m in msd_list]
            min_time = max(t.min() for t in all_times)
            max_time = min(t.max() for t in all_times)
            common_time = np.linspace(min_time, max_time, 500)
            
            # æ’å€¼
            msd_interp_list = []
            for time, msd in msd_list:
                try:
                    msd_interp = np.interp(common_time, time, msd)
                    msd_interp_list.append(msd_interp)
                    
                    # é€æ˜å•æ¬¡ (æœ‰æ•ˆruns)
                    ax.plot(common_time, msd_interp, alpha=0.2, linewidth=1,
                           color=COLORS[element], zorder=1)
                except:
                    continue
            
            if not msd_interp_list:
                continue
            
            # é›†åˆå¹³å‡ (æœ‰æ•ˆruns)
            if len(msd_interp_list) >= 2:
                msd_array = np.array(msd_interp_list)
                msd_mean = np.mean(msd_array, axis=0)
                msd_std = np.std(msd_array, axis=0, ddof=1)
                
                ax.plot(common_time, msd_mean, '-', linewidth=3,
                       color=COLORS[element],
                       label=f'{element} Valid (n={len(msd_interp_list)})', zorder=10)
                
                ax.fill_between(common_time, msd_mean - msd_std, msd_mean + msd_std,
                               alpha=0.2, color=COLORS[element], zorder=5)
            else:
                ax.plot(common_time, msd_interp_list[0], '-', linewidth=3,
                       color=COLORS[element],
                       label=f'{element} Valid (n=1)', zorder=10)
            
            has_data = True
        
        # === ç»˜åˆ¶è¢«ç­›é€‰runs (è™šçº¿,ç°è‰²) ===
        for element in ['Pt', 'Sn', 'PtSn']:
            msd_list_filt = msd_cache_filtered.get((temp, element), [])
            
            if not msd_list_filt:
                continue
            
            # å¯¹é½æ—¶é—´è½´
            all_times_filt = [t for t, m in msd_list_filt]
            min_time_filt = max(t.min() for t in all_times_filt)
            max_time_filt = min(t.max() for t in all_times_filt)
            common_time_filt = np.linspace(min_time_filt, max_time_filt, 500)
            
            # æ’å€¼
            msd_interp_list_filt = []
            for time, msd in msd_list_filt:
                try:
                    msd_interp = np.interp(common_time_filt, time, msd)
                    msd_interp_list_filt.append(msd_interp)
                    
                    # é€æ˜å•æ¬¡ (è¢«ç­›é€‰runs - ç°è‰²è™šçº¿)
                    ax.plot(common_time_filt, msd_interp, '--', alpha=0.15, linewidth=1,
                           color='gray', zorder=0.5)
                except:
                    continue
            
            if not msd_interp_list_filt:
                continue
            
            # é›†åˆå¹³å‡ (è¢«ç­›é€‰runs - æ·±ç°è‰²è™šçº¿)
            if len(msd_interp_list_filt) >= 1:
                msd_array_filt = np.array(msd_interp_list_filt)
                msd_mean_filt = np.mean(msd_array_filt, axis=0)
                
                ax.plot(common_time_filt, msd_mean_filt, '--', linewidth=2.5,
                       color='dimgray', alpha=0.6,
                       label=f'{element} Filtered (n={len(msd_interp_list_filt)})', zorder=8)
            
            has_data = True
        
        if not has_data:
            ax.text(0.5, 0.5, 'No Data', ha='center', va='center',
                   transform=ax.transAxes, fontsize=14)
        
        # æ·»åŠ ç­›é€‰ä¿¡æ¯åˆ°æ ‡é¢˜
        if stats['total'] > 0:
            filter_text = f" [{stats['kept']}/{stats['total']} runs]"
            if stats['filtered'] > 0:
                title_text = f"{temp}{filter_text}"
                title_color = 'darkred' if stats['filtered'] > stats['total'] * 0.3 else 'black'
            else:
                title_text = f"{temp}{filter_text}"
                title_color = 'black'
        else:
            title_text = f"{temp}"
            title_color = 'black'
        
        ax.set_xlabel('Time (ps)', fontsize=11, fontweight='bold')
        ax.set_ylabel('MSD (A^2)', fontsize=11, fontweight='bold')
        ax.set_title(title_text, fontsize=12, fontweight='bold', color=title_color)
        ax.legend(loc='upper left', fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))
        
        if global_max_msd > 0:
            ax.set_ylim(0, global_max_msd * 1.1)
    
    for idx in range(n_temps, len(axes)):
        axes[idx].axis('off')
    
    title_text = f'{base_system} - MSD Curves\n'
    if len(compositions) > 1:
        title_text += f'Combined: {", ".join(compositions)}\n'
    title_text += 'Thick: Average | Shaded: Â±1Ïƒ | Thin: Individual | Y-axis unified'
    
    fig.suptitle(title_text, fontsize=14, fontweight='bold', y=0.995)
    
    # ä½¿ç”¨try-excepté¿å…tight_layouté”™è¯¯
    try:
        plt.tight_layout(rect=[0, 0, 1, 0.99])
    except Exception as e:
        print(f"  [!] Warning: tight_layout failed ({e}), continuing...")
    
    output_file = OUTPUT_DIR / f'{base_system}_all_temps_GMX.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.close(fig)
    
    print(f"  [OK] Saved: {output_file.name}")
    print(f"{'='*80}\n")


def main(enable_filtering=None):
    """
    ä¸»å‡½æ•°
    
    Parameters:
    -----------
    enable_filtering : bool, optional
        æ˜¯å¦å¯ç”¨å¼‚å¸¸å€¼ç­›é€‰ã€‚None = ä½¿ç”¨å…¨å±€é…ç½®
    """
    print("\n" + "="*80)
    print("MSD Curves - Fast Version (with file indexing + outlier filtering)")
    print("="*80)
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # 0. åŠ è½½å¼‚å¸¸æ¸…å•
    print("\n[0/5] Loading outlier list...")
    outlier_files = load_large_D_outliers(enable_filtering)
    
    # 1. æ„å»ºæ–‡ä»¶ç´¢å¼• (ä¸€æ¬¡æ€§,åˆ†åˆ«ç´¢å¼•æœ‰æ•ˆrunså’Œè¢«ç­›é€‰runs)
    file_index, file_index_filtered, filter_stats = build_file_index(outlier_files)
    
    # 2. ä»æ–‡ä»¶ç´¢å¼•ç›´æ¥å‘ç°ç³»ç»Ÿåˆ—è¡¨ (ä¸å†ä¾èµ– ensemble_analysis_results.csv)
    print("\n[2/5] Discovering systems from file index...")
    
    # ä»æ–‡ä»¶ç´¢å¼•æå–æ‰€æœ‰ (composition, temperature) ç»„åˆ
    all_compositions = set()
    for key in list(file_index.keys()) + list(file_index_filtered.keys()):
        comp, temp, elem = key
        all_compositions.add(comp)
    
    if not all_compositions:
        print(f"\n[X] Error: No files found in file index")
        return
    
    print(f"   [OK] Found {len(all_compositions)} compositions from files")
    
    # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿ DataFrame ç”¨äº group_compositions_by_system
    df = pd.DataFrame({'composition': list(all_compositions)})
    
    # 3. æŒ‰ä½“ç³»åˆ†ç»„
    system_groups = group_compositions_by_system(df)
    
    print(f"\n[*] Systems identified: {len(system_groups)}")
    
    # 4. åº”ç”¨ç³»ç»Ÿè¿‡æ»¤
    include_patterns = SYSTEM_FILTER.get('include_patterns', [])
    exclude_patterns = SYSTEM_FILTER.get('exclude_patterns', [])
    
    if include_patterns or exclude_patterns:
        print(f"\n[*] Applying system filters...")
        if include_patterns:
            print(f"    Include patterns: {include_patterns}")
        if exclude_patterns:
            print(f"    Exclude patterns: {exclude_patterns}")
        
        system_groups = filter_systems(system_groups, include_patterns, exclude_patterns)
        print(f"    Filtered: {len(system_groups)} systems selected")
    
    print(f"[*] Output dir: {OUTPUT_DIR}")
    
    print("\nSystem list:")
    for i, (base_sys, comps) in enumerate(sorted(system_groups.items()), 1):
        # ç»Ÿè®¡è¯¥ç³»ç»Ÿçš„æ–‡ä»¶æ•°
        n_files = 0
        for comp in comps:
            for key in list(file_index.keys()) + list(file_index_filtered.keys()):
                if key[0] == str(comp):
                    n_files += len(file_index.get(key, [])) + len(file_index_filtered.get(key, []))
        
        if len(comps) > 1:
            print(f"  {i:2d}. {base_sys:30s} ({n_files:3d} files) <- {len(comps)} comps")
        else:
            print(f"  {i:2d}. {base_sys:30s} ({n_files:3d} files)")
    
    print("\n" + "="*80)
    print("Start plotting...")
    print("="*80)
    
    success = 0
    failed = []
    
    # å§‹ç»ˆä»æ–‡ä»¶ç´¢å¼•è·å–æ¸©åº¦åˆ—è¡¨ (ä¸å†ä¾èµ– CSV)
    use_file_temps = True
    
    for idx, (base_sys, comps) in enumerate(sorted(system_groups.items()), 1):
        print(f"\n[{idx}/{len(system_groups)}] {base_sys}...")
        
        try:
            plot_system_all_temps_fast(base_sys, comps, df, file_index, file_index_filtered, filter_stats, 
                                      max_temps=None, use_file_temps=use_file_temps)
            success += 1
        except Exception as e:
            print(f"[X] Failed: {e}")
            import traceback
            traceback.print_exc()
            failed.append((base_sys, str(e)))
    
    print("\n" + "="*80)
    print("DONE!")
    print("="*80)
    print(f"Expected: {len(system_groups)}")
    print(f"Success:  {success}")
    print(f"Failed:   {len(failed)}")
    
    if failed:
        print("\nFailed systems:")
        for sys, err in failed:
            print(f"  - {sys}: {err}")
    
    print(f"\nOutput: {OUTPUT_DIR}")
    print("="*80)
    
    # ç”Ÿæˆç­›é€‰ç»Ÿè®¡æŠ¥å‘Š
    print("\n[*] Generating filtering statistics report...")
    generate_filter_report(filter_stats, OUTPUT_DIR)


def generate_filter_report(filter_stats, output_dir):
    """ç”Ÿæˆç­›é€‰ç»Ÿè®¡æŠ¥å‘Š"""
    report_file = output_dir / 'filtering_statistics.txt'
    
    # æŒ‰ä½“ç³»å’Œæ¸©åº¦åˆ†ç»„ç»Ÿè®¡
    system_stats = defaultdict(lambda: defaultdict(lambda: {'total': 0, 'kept': 0, 'filtered': 0}))
    
    for (comp, temp, elem), stats in filter_stats.items():
        # æå–åŸºç¡€ä½“ç³»åç§°
        base_sys = extract_base_system(comp)
        system_stats[base_sys][temp]['total'] += stats['total']
        system_stats[base_sys][temp]['kept'] += stats['kept']
        system_stats[base_sys][temp]['filtered'] += stats['filtered']
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("Step1 Filtering Statistics - By System and Temperature\n")
        f.write("="*80 + "\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*80 + "\n\n")
        
        # æ€»ä½“ç»Ÿè®¡
        total_runs = sum(s['total'] for stats in filter_stats.values() for s in [stats])
        kept_runs = sum(s['kept'] for stats in filter_stats.values() for s in [stats])
        filtered_runs = sum(s['filtered'] for stats in filter_stats.values() for s in [stats])
        
        f.write("Overall Statistics:\n")
        f.write(f"  Total runs:    {total_runs}\n")
        if total_runs > 0:
            f.write(f"  Kept runs:     {kept_runs} ({kept_runs/total_runs*100:.1f}%)\n")
            f.write(f"  Filtered runs: {filtered_runs} ({filtered_runs/total_runs*100:.1f}%)\n")
        else:
            f.write(f"  Kept runs:     {kept_runs}\n")
            f.write(f"  Filtered runs: {filtered_runs}\n")
        f.write("\n" + "="*80 + "\n\n")
        
        # æŒ‰ä½“ç³»è¯¦ç»†ç»Ÿè®¡
        for base_sys in sorted(system_stats.keys()):
            temps = system_stats[base_sys]
            
            f.write(f"System: {base_sys}\n")
            f.write("-"*80 + "\n")
            f.write(f"{'Temperature':<15} {'Total':<10} {'Kept':<10} {'Filtered':<12} {'Rate':<10}\n")
            f.write("-"*80 + "\n")
            
            for temp in sorted(temps.keys(), key=lambda x: int(x.replace('K', ''))):
                stats = temps[temp]
                if stats['total'] > 0:
                    rate = stats['kept'] / stats['total'] * 100
                    f.write(f"{temp:<15} {stats['total']:<10} {stats['kept']:<10} "
                           f"{stats['filtered']:<12} {rate:>6.1f}%\n")
            
            # ä½“ç³»æ€»è®¡
            sys_total = sum(s['total'] for s in temps.values())
            sys_kept = sum(s['kept'] for s in temps.values())
            sys_filtered = sum(s['filtered'] for s in temps.values())
            sys_rate = sys_kept / sys_total * 100 if sys_total > 0 else 0
            
            f.write("-"*80 + "\n")
            f.write(f"{'Total':<15} {sys_total:<10} {sys_kept:<10} "
                   f"{sys_filtered:<12} {sys_rate:>6.1f}%\n")
            f.write("\n")
        
        f.write("="*80 + "\n")
        f.write("Note: [Kept/Total runs] shown in each subplot title\n")
        f.write("      Red title = >30% filtered\n")
        f.write("="*80 + "\n")
    
    print(f"  [OK] Saved: {report_file.name}")


if __name__ == '__main__':
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='ç»˜åˆ¶MSDæ›²çº¿ (æ”¯æŒå¼‚å¸¸å€¼ç­›é€‰)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python step3_plot_msd.py              # é»˜è®¤å¯ç”¨ç­›é€‰
  python step3_plot_msd.py --nofilter   # å…³é—­ç­›é€‰ï¼Œç»˜åˆ¶æ‰€æœ‰æ›²çº¿
        """
    )
    parser.add_argument(
        '--nofilter',
        action='store_true',
        help='å…³é—­å¼‚å¸¸å€¼ç­›é€‰ï¼Œç»˜åˆ¶æ‰€æœ‰æ›²çº¿ï¼ˆåŒ…æ‹¬å¼‚å¸¸å€¼ï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å†³å®šæ˜¯å¦ç­›é€‰
    if args.nofilter:
        enable_filtering = False
    else:
        enable_filtering = None  # ä½¿ç”¨å…¨å±€é…ç½®
    
    main(enable_filtering)
