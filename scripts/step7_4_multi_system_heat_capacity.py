#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
Step 7.4: å¤šä½“ç³»å•æ¬¡è¿è¡Œçƒ­å®¹åˆ†æ (Multi-System Heat Capacity Analysis)
================================================================================

ä½œè€…: GitHub Copilot
æ—¥æœŸ: 2025-10-22
ç‰ˆæœ¬: v1.2

æœ€è¿‘æ›´æ–°:
---------
v1.3 (2025-10-22):
  - ğŸ”„ **é‡è¦ä¿®å¤**: Cvä½“ç³»æ•°æ®åˆå¹¶
    Â· é—®é¢˜: Cv-1, Cv-2, Cv-3, Cv-4, Cv-5è¢«é”™è¯¯åœ°è¯†åˆ«ä¸º5ä¸ªç‹¬ç«‹ç»“æ„
    Â· å®é™…: è¿™äº›æ˜¯åŒä¸€ä¸ªä½“ç³»(Sn8Pt6O4)çš„5æ¬¡é‡å¤æ¨¡æ‹Ÿ
    Â· ä¿®å¤: detect_system_type()å‡½æ•°ç»Ÿä¸€è¿”å›('Cv', 'Cv')
    Â· ç»“æœ: Cvæ•°æ®ä»5Ã—19=95ä¸ªç‚¹åˆå¹¶ä¸ºå•ä¸€ä½“ç³»(95ä¸ªç‚¹,19ä¸ªæ¸©åº¦Ã—5æ¬¡é‡å¤)
    Â· å½±å“: step7_4è¾“å‡ºä¸­Cvç°ä¸º1ä¸ªç»“æ„(ä¹‹å‰5ä¸ª), step7_4_2èšç±»åˆ†æä½¿ç”¨Cvè€ŒéCv-1~5

v1.2 (2025-10-22):
  - ğŸ”§ ä¿®å¤: ç»Ÿä¸€match_keyç”Ÿæˆç®—æ³•,ä½¿ç”¨4-levelè·¯å¾„ç­¾å (ä¸MSDç­›é€‰ä¸€è‡´)
  - ğŸ“Š ä¿®å¤: æ•°æ®é‡ä»é”™è¯¯çš„8030â†’5296æ›´æ­£ä¸ºæ­£ç¡®çš„3262â†’2370
  - âœ… éªŒè¯: ç­›é€‰ç‡ä»é”™è¯¯çš„34.05%æ›´æ­£ä¸ºæ­£ç¡®çš„27.35%
  - ğŸ“ å¢å¼º: æ·»åŠ è¯¦ç»†çš„æ•°æ®æ¥æºå’Œå»é‡è¯´æ˜

v1.1 (2025-10-21):
  - âœ¨ æ–°å¢: Lindemannæ•°æ®å»é‡åŠŸèƒ½ (4012â†’3262æ¡)
  - ğŸ“ˆ æ–°å¢: ç­›é€‰ç»Ÿè®¡æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ
  - ğŸ¯ æ–°å¢: K-meansèšç±»åˆ†æ (å¯é€‰)
  - ğŸ› ä¿®å¤: Unicodeç¼–ç é—®é¢˜ (Windowså…¼å®¹æ€§)

================================================================================
åŠŸèƒ½æ¦‚è¿°
================================================================================
æœ¬è„šæœ¬ç”¨äºåˆ†æå¤šä¸ªçº³ç±³å›¢ç°‡ä½“ç³»çš„çƒ­å®¹,åŸºäºLAMMPSèƒ½é‡è¾“å‡ºå’ŒLindemannæŒ‡æ•°,
æ”¯æŒå•æ¬¡è¿è¡Œçº§åˆ«çš„è¯¦ç»†åˆ†æ,å¹¶æä¾›å¤šç§æ•°æ®è´¨é‡æ§åˆ¶æ–¹æ³•ã€‚

**æ ¸å¿ƒç‰¹æ€§**:
1. âœ… **å¤šä½“ç³»æ”¯æŒ**: Cvã€Pt8SnXã€Pt6SnXã€Pt8-xSnxã€PtxSnyã€PtxSnyOz (6å¤§ç³»åˆ—, 51ä¸ªç»“æ„)
   - æ³¨: Cv-1~Cv-5è‡ªåŠ¨åˆå¹¶ä¸ºå•ä¸€Cvä½“ç³»(5æ¬¡é‡å¤æ¨¡æ‹Ÿ,95ä¸ªæ•°æ®ç‚¹)
2. âœ… **æ™ºèƒ½ä½“ç³»è¯†åˆ«**: è‡ªåŠ¨åˆ†ç±»å’Œå‘½åæ ‡å‡†åŒ– (system_type + system_id)
   - ç‰¹æ®Šå¤„ç†: Cv-X â†’ ('Cv', 'Cv') ç»Ÿä¸€å‘½å,é¿å…é‡å¤æ¨¡æ‹Ÿè¢«æ‹†åˆ†
3. âœ… **è·¯å¾„ç­¾åç­›é€‰**: ç»§æ‰¿Step 6çš„4çº§è·¯å¾„ç­¾åç®—æ³• (ä¸MSDç­›é€‰å®Œå…¨ä¸€è‡´)
4. âœ… **åŒé‡æ•°æ®è´¨é‡æ§åˆ¶**:
   - Method 1 (opt-in): Step 1 MSDå¼‚å¸¸ç­›é€‰ (892ä¸ªè·¯å¾„ç­¾å, 27.35%ç§»é™¤ç‡)
   - Method 2 (opt-in): IQRç»Ÿè®¡å¼‚å¸¸ç­›é€‰ (Lindemann + Energy, ~3.5%é¢å¤–ç§»é™¤)
5. âœ… **å•ç»“æ„ç»†ç²’åº¦åˆ†æ**: æ¯ä¸ªç»“æ„ç‹¬ç«‹åˆ†æ (system_idçº§åˆ«), ç”Ÿæˆ55ä¸ªç‹¬ç«‹å›¾è¡¨
6. âœ… **ä¸‰åŒºåŸŸçƒ­å®¹æ‹Ÿåˆ**: Solid / Premelting / LiquidåŒºåŸŸç‹¬ç«‹çº¿æ€§æ‹Ÿåˆ
7. âœ… **èšç±»åˆ†æ** (å¯é€‰): K-meansè‡ªåŠ¨æ£€æµ‹ç›¸è¾¹ç•Œ (é’ˆå¯¹ç‰¹å®šä½“ç³»å¦‚Pt6Sn8)
8. âœ… **ç­›é€‰é€æ˜åº¦**: è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„ç­›é€‰ç»Ÿè®¡æŠ¥å‘Š

================================================================================
è¾“å…¥æ–‡ä»¶
================================================================================
**å¿…éœ€è¾“å…¥** (è‡ªåŠ¨åŠ è½½):

1. **èƒ½é‡æ•°æ®** (LAMMPSæ€»èƒ½é‡)
   è·¯å¾„: data/lammps_energy/energy_master_20251016_121110.csv
   æ¥æº: LAMMPS MDæ¨¡æ‹Ÿçš„etotalè¾“å‡º (å›¢ç°‡ + è½½ä½“æ€»èƒ½é‡)
   åˆ—: path, structure, temp, run_num, total_steps, sample_steps,
       avg_energy, std, min, max, sample_interval, skip_steps, full_path
   ç»Ÿè®¡: 3262æ¡è®°å½• â†’ 2370æ¡ (åº”ç”¨--msd-filterå)
   è·¯å¾„ç¤ºä¾‹: /home/scms/jychen/.../pt8sn8-1-best/T900.r15.gpu0

2. **LindemannæŒ‡æ•°æ•°æ®** (ç›¸æ€åˆ†ç±»)
   è·¯å¾„: data/lindemann/lindemann_master_run_*.csv (3ä¸ªæ–‡ä»¶)
   æ¥æº: MSDè½¨è¿¹è®¡ç®—çš„LindemannæŒ‡æ•° (åŸºäºPt-Snè·ç¦»MSD)
   åˆ—: ç›®å½•, ç»“æ„, æ¸©åº¦(K), LindemannæŒ‡æ•°, æ–¹æ³•, è€—æ—¶(s), æ—¶é—´æˆ³
   ç»Ÿè®¡: 
     - åŸå§‹åˆå¹¶: 4012æ¡ (3ä¸ªCSVæ–‡ä»¶)
     - å»é‡å: 3262æ¡ (ç§»é™¤750æ¡é‡å¤, 18.7%)
     - ç­›é€‰å: 2370æ¡ (åº”ç”¨--msd-filterå)
   è·¯å¾„ç¤ºä¾‹: /home/scms/jychen/.../pt8sn8-1-best/T900.r15.gpu0
   æ³¨æ„: å»é‡åŸºäº(ç›®å½•, ç»“æ„, æ¸©åº¦)ä¸‰å…ƒç»„, ä¿ç•™æœ€æ–°è®°å½•

3. **Step 1 MSDå¼‚å¸¸è¿‡æ»¤æ•°æ®** (æ•°æ®è´¨é‡æ§åˆ¶)
   è·¯å¾„: results/large_D_outliers.csv
   æ¥æº: Step 1çš„Ptå’ŒSnå…ƒç´ MSDå¼‚å¸¸æ£€æµ‹
   åˆ—: group_key, composition, temperature, element, run_id, 
       gmx_D, filepath, reason
   ç»Ÿè®¡: 
     - 2227æ¡å¼‚å¸¸è®°å½•
     - 892ä¸ªå”¯ä¸€è·¯å¾„ç­¾å (4-level)
   å¼‚å¸¸åŸå› åˆ†å¸ƒ:
     - linked_bad_simulation: 804æ¡ (36.1%)
     - IQR_outlier: 717æ¡ (32.2%)
     - Intercept>10.0AÂ²: 706æ¡ (31.7%)
   å…³é”®: åŸºäºPt/Sn **å…ƒç´ MSD**, éPt-Snè·ç¦»MSD

4. **è½½ä½“çƒ­å®¹æ•°æ®** (å¯é€‰,ç”¨äºè®¡ç®—çº¯å›¢ç°‡çƒ­å®¹)
   è·¯å¾„: data/lammps_energy/sup/energy_master_20251021_151520.csv
   æ¥æº: çº¯Alâ‚‚Oâ‚ƒè½½ä½“çš„LAMMPSæ¨¡æ‹Ÿ
   ç”¨é€”: ä»æ€»çƒ­å®¹ä¸­æ‰£é™¤è½½ä½“è´¡çŒ®
   é»˜è®¤å€¼: 38.2151 meV/K (å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨)

================================================================================
è·¯å¾„ç­¾åç®—æ³• (v1.2 å…³é”®ä¿®å¤)
================================================================================
**å…³é”®ä¿®å¤ (v1.2)**: `normalize_path`å’Œ`extract_path_signature`ç°åœ¨ä½¿ç”¨**ç›¸åŒ**çš„ç®—æ³•!

**é—®é¢˜èƒŒæ™¯** (v1.1åŠä¹‹å‰):
- `extract_path_signature`: 4-levelç­¾å â†’ ç”¨äºMSDç­›é€‰
- `normalize_path`: ç®€å•çš„ç»“æ„+æ¸©åº¦+run â†’ ç”¨äºEnergy-Lindemannåˆå¹¶
- **åæœ**: å¯¼è‡´æ•°æ®é‡è™šé«˜ (8030æ¡) å’Œç­›é€‰ç‡é”™è¯¯ (34.05%)

**ä¿®å¤æ–¹æ¡ˆ (v1.2)**:
```python
def normalize_path(path, ...):
    # ç°åœ¨ç›´æ¥è°ƒç”¨extract_path_signature!
    is_msd_path = 'msd_' in path.lower() or re.search(r'\\d+K[/\\\\]', path)
    return extract_path_signature(path, is_msd_path=is_msd_path)
```

**4çº§è·¯å¾„ç­¾åç®—æ³•** (ç»Ÿä¸€ä½¿ç”¨):
æå–æ ¼å¼: batch/parent/composition/run
  MSDè·¯å¾„: run3/o2/o2pt4sn6/t1000.r24.gpu0
  èƒ½é‡è·¯å¾„: run3/o2/o2pt7sn7/t200.r0.gpu0
  
ç­¾åæ„å»ºæ­¥éª¤:
1. æå–runä¿¡æ¯: T\\d+\\.r\\d+\\.gpu\\d+ â†’ å°å†™åŒ– â†’ t1000.r24.gpu0
2. å‘ä¸Šæå–compositionç›®å½•: O2Pt4Sn6 â†’ å°å†™åŒ– â†’ o2pt4sn6
3. å†å‘ä¸Šæå–parentç›®å½•: o2 â†’ å°å†™åŒ–
4. æ£€æµ‹batchæ ‡è¯†ç¬¦: run3, run2, run4, run5 (å‘ä¸Šæœ€å¤š3çº§)
5. æ„å»º4çº§æˆ–3çº§ç­¾å (å–å†³äºæ˜¯å¦æ£€æµ‹åˆ°æ‰¹æ¬¡)

**è·¯å¾„å…¼å®¹æ€§**:
- Windowsè·¯å¾„: æ”¯æŒåæ–œæ  (\\)
- Linuxè·¯å¾„: æ”¯æŒæ­£æ–œæ  (/)
- MSDè·¯å¾„: è¯†åˆ«æ¸©åº¦ç›®å½• (1000K/)
- Energy/Lindemannè·¯å¾„: ç›´æ¥æå–runä¿¡æ¯

**ç­›é€‰æ•ˆæœ** (v1.2ä¿®å¤å):
- èƒ½é‡æ•°æ®: 3262 â†’ 2370 (ç­›é™¤892æ¡, 27.3%) âœ… æ­£ç¡®
- Lindemannæ•°æ®: 3262 â†’ 2370 (ç­›é™¤892æ¡, 27.3%) âœ… æ­£ç¡®
- æœ€ç»ˆåˆå¹¶: 3262æ¡ (1:1åŒ¹é…) â†’ 2370æ¡ âœ… æ­£ç¡®

================================================================================
è¾“å‡ºæ–‡ä»¶
================================================================================
è¾“å‡ºç›®å½•: results/step7_4_multi_system/

**1. ä¸»æ•°æ®æ–‡ä»¶**:
   - step7_4_all_systems_data.csv
     å†…å®¹: æ‰€æœ‰åŒ¹é…çš„èƒ½é‡-Lindemannæ•°æ®
     åˆ—: match_key, structure, system_type, system_id, temp, 
         avg_energy, energy_std, delta, phase, run_id
     ç»Ÿè®¡: 2370æ¡ (--msd-filter) æˆ– 2287æ¡ (--msd-filter --iqr-filter)

**2. åˆ†ææŠ¥å‘Š**:
   - step7_4_multi_system_comparison.md
     å†…å®¹: 55ä¸ªç»“æ„çš„çƒ­å®¹åˆ†ææ±‡æ€»
     åŒ…å«: å›ºæ€/é¢„ç†”/æ¶²æ€ä¸‰åŒºåŸŸçƒ­å®¹ã€RÂ²å€¼ã€æ¸©åº¦èŒƒå›´

**3. ç»¼åˆå¯¹æ¯”å›¾**:
   - step7_4_multi_system_comparison.png
     å†…å®¹: 6ä¸ªä½“ç³»ç±»å‹çš„çƒ­å®¹å¯¹æ¯” (4Ã—3å­å›¾å¸ƒå±€)
     å­å›¾: 
       - å›ºæ€Cv vs Snå«é‡
       - é¢„ç†”Cv vs Snå«é‡  
       - æ¶²æ€Cv vs Snå«é‡
       - ç†”åŒ–æ¸©åº¦ vs Snå«é‡

**4. å•ç»“æ„è¯¦ç»†åˆ†æå›¾** (55ä¸ªPNGæ–‡ä»¶):
   ç›®å½•: individual_structure_plots/
   å‘½å: {structure_name}_individual_runs_analysis.png
   ç¤ºä¾‹: 
     - Cv-1_individual_runs_analysis.png
     - Pt8Sn0_individual_runs_analysis.png
     - Pt6Sn3_individual_runs_analysis.png
     - O2Pt4Sn6_individual_runs_analysis.png
   
   å›¾è¡¨å¸ƒå±€ (2Ã—2):
     (a) ç›¸å¯¹èƒ½é‡ vs æ¸©åº¦
         - æ•£ç‚¹: å•æ¬¡è¿è¡Œæ•°æ®ç‚¹ (æŒ‰ç›¸æ€ç€è‰²)
         - çº¿: å›ºæ€/é¢„ç†”/æ¶²æ€ä¸‰åŒºåŸŸæ‹Ÿåˆ
         - æ ‡æ³¨: æ–œç‡(çƒ­å®¹)ã€RÂ²ã€æ¸©åº¦èŒƒå›´
     
     (b) çƒ­å®¹æŸ±çŠ¶å›¾
         - ä¸‰æŸ±: å›ºæ€ã€é¢„ç†”ã€æ¶²æ€çƒ­å®¹
         - è½½ä½“çº¿: Cv_supportå‚è€ƒå€¼
         - æ ‡æ³¨: æ•°å€¼ã€è¯¯å·®ã€RÂ²å€¼
     
     (c) æ¸©åº¦-ç›¸æ€åˆ†å¸ƒå›¾ (å †å æŸ±çŠ¶å›¾)
         - Yè½´: æ¸©åº¦ (200-1100K)
         - é¢œè‰²: å›ºæ€(è“)ã€é¢„ç†”(æ©™)ã€æ¶²æ€(çº¢)
         - æ˜¾ç¤º: æ¯ä¸ªæ¸©åº¦çš„ç›¸æ€åˆ†å¸ƒ
     
     (d) Lindemannæ•£ç‚¹å›¾
         - æ•£ç‚¹: LindemannæŒ‡æ•° vs æ¸©åº¦
         - é˜ˆå€¼çº¿: 0.1 (å›ºæ€ä¸Šé™)ã€0.15 (æ¶²æ€ä¸‹é™)
         - åŒºåŸŸ: å›ºæ€åŒº(è“)ã€é¢„ç†”åŒº(æ©™)ã€æ¶²æ€åŒº(çº¢)

================================================================================
åˆ†æç®—æ³•
================================================================================
**ä¸‰åŒºåŸŸçº¿æ€§æ‹Ÿåˆ** (å›ºæ€ â†’ é¢„ç†” â†’ æ¶²æ€):

1. ç›¸æ€åˆ†ç±» (åŸºäºLindemannæŒ‡æ•°):
   - å›ºæ€: Î´ < 0.1
   - é¢„ç†”: 0.1 â‰¤ Î´ < 0.15
   - æ¶²æ€: Î´ â‰¥ 0.15

2. æ¸©åº¦åŒºé—´è¯†åˆ« (æ¯ä¸ªç›¸æ€):
   - è‡ªåŠ¨æ£€æµ‹è¿ç»­æ¸©åº¦èŒƒå›´
   - æœ€å°æ•°æ®ç‚¹: 5ä¸ª (ä¿è¯ç»Ÿè®¡å¯é æ€§)

3. çº¿æ€§æ‹Ÿåˆ (æ¯ä¸ªåŒºåŸŸ):
   E_total(T) = slope Ã— T + intercept
   Cv_total = slope Ã— 1000 (eV/K â†’ meV/K)
   Cv_cluster = Cv_total - Cv_support

4. ç»Ÿè®¡è¯„ä¼°:
   - RÂ²å€¼: æ‹Ÿåˆä¼˜åº¦ (>0.995ä¸ºä¼˜ç§€)
   - på€¼: ç»Ÿè®¡æ˜¾è‘—æ€§ (<0.05ä¸ºæ˜¾è‘—)
   - æ ‡å‡†è¯¯å·®: å‚æ•°ä¸ç¡®å®šæ€§

================================================================================
ä½“ç³»åˆ†ç±»ç­–ç•¥
================================================================================
è‡ªåŠ¨è¯†åˆ«6å¤§ç±»55ä¸ªç»“æ„:

1. **Cvç³»åˆ—** (5ä¸ªç»“æ„):
   - åŒ¹é…æ¨¡å¼: ^Cv-\\d+
   - ç¤ºä¾‹: Cv-1, Cv-2, Cv-3, Cv-4, Cv-5
   - ç‰¹ç‚¹: å‚è€ƒä½“ç³»,å›ºå®šæˆåˆ†

2. **Pt6ç³»åˆ—** (1ä¸ªç»“æ„):
   - åŒ¹é…æ¨¡å¼: ^pt6$
   - ç¤ºä¾‹: Pt6 (çº¯Pt)

3. **Pt6SnXç³»åˆ—** (9ä¸ªç»“æ„):
   - åŒ¹é…æ¨¡å¼: ^pt6sn\\d+
   - ç¤ºä¾‹: Pt6Sn1, Pt6Sn2, ..., Pt6Sn9
   - ç‰¹ç‚¹: å›ºå®š6ä¸ªPt,å˜Snå«é‡

4. **Pt8SnXç³»åˆ—** (11ä¸ªç»“æ„):
   - åŒ¹é…æ¨¡å¼: ^pt8sn\\d+
   - ç¤ºä¾‹: Pt8Sn0, Pt8Sn1, ..., Pt8Sn10
   - ç‰¹ç‚¹: å›ºå®š8ä¸ªPt,å˜Snå«é‡

5. **PtxSnyç³»åˆ—** (4ä¸ªç»“æ„):
   - åŒ¹é…æ¨¡å¼: ^pt\\d+sn\\d+$ (æ— O)
   - ç¤ºä¾‹: Pt3Sn5, Pt4Sn4, Pt5Sn3, Pt7Sn1
   - ç‰¹ç‚¹: å˜Pt/Snæ¯”,æ— æ°§

6. **PtxSnyOzç³»åˆ—** (25ä¸ªç»“æ„):
   - åŒ¹é…æ¨¡å¼: åŒ…å«Oçš„ä¸‰å…ƒç³»ç»Ÿ
   - ç¤ºä¾‹: O2Pt4Sn6, Pt7Sn5O1, Sn10Pt7O4ç­‰
   - ç‰¹ç‚¹: Pt-Sn-Oä¸‰å…ƒåˆé‡‘

================================================================================
ä½¿ç”¨è¯´æ˜
================================================================================
**å‘½ä»¤è¡Œå‚æ•°**:
    --msd-filter         : å¼€å¯Step 1çš„MSDå¼‚å¸¸å€¼ç­›é€‰(è·¯å¾„ç­¾ååŒ¹é…)
    --iqr-filter         : å¼€å¯IQRç»Ÿè®¡ç­›é€‰(LindemannæŒ‡æ•° + èƒ½é‡å€¼)
    --iqr-factor FLOAT   : IQRå€æ•°é˜ˆå€¼(é»˜è®¤3.0,æ›´ä¸¥æ ¼åˆ™ç”¨æ›´å¤§å€¼)

**ç­›é€‰ç­–ç•¥**:
    æ–¹æ³•1 (å¯é€‰): Step 1 MSDå¼‚å¸¸å€¼ç­›é€‰
      - åŸºäºè·¯å¾„ç­¾ååŒ¹é… (4çº§ç­¾å: batch/parent/composition/run)
      - 892ä¸ªå¼‚å¸¸æ¨¡æ‹Ÿè·¯å¾„ç­¾å
      - å…¸å‹ç­›é™¤27-28%æ•°æ®
      - æ¥æº: large_D_outliers.csv
      - ä½¿ç”¨æ–¹æ³•: --msd-filter

    æ–¹æ³•2 (å¯é€‰): IQRç»Ÿè®¡ç­›é€‰ (Lindemann + Energy)
      - æŒ‰(ç»“æ„,æ¸©åº¦)åˆ†ç»„è¿›è¡Œç»Ÿè®¡åˆ†æ
      - **Lindemann IQR**: è¶…å‡º [Q1-k*IQR, Q3+k*IQR] èŒƒå›´çš„Î´å€¼
      - **Energy IQR**: è¶…å‡º [Q1-k*IQR, Q3+k*IQR] èŒƒå›´çš„èƒ½é‡å€¼
      - å¯è°ƒé˜ˆå€¼k (é»˜è®¤3.0*IQR, ä¸¥äºæ ‡å‡†1.5*IQR)
      - è¦æ±‚æ¯ç»„ â‰¥5 ä¸ªæ•°æ®ç‚¹æ‰è¿›è¡Œæ£€æµ‹
      - ä½¿ç”¨æ–¹æ³•: --iqr-filter

**è¿è¡Œç¤ºä¾‹**:
    # ç¤ºä¾‹1: é»˜è®¤è¿è¡Œ (æ— ç­›é€‰,ä½¿ç”¨å…¨éƒ¨åŸå§‹æ•°æ®)
    python step7_4_multi_system_heat_capacity.py

    # ç¤ºä¾‹2: ä»…æ–¹æ³•1ç­›é€‰
    python step7_4_multi_system_heat_capacity.py --msd-filter

    # ç¤ºä¾‹3: ä»…æ–¹æ³•2ç»Ÿè®¡ç­›é€‰ (Lindemann + Energy)
    python step7_4_multi_system_heat_capacity.py --iqr-filter

    # ç¤ºä¾‹4: åŒé‡ç­›é€‰ (æ–¹æ³•1 + æ–¹æ³•2)
    python step7_4_multi_system_heat_capacity.py --msd-filter --iqr-filter

    # ç¤ºä¾‹5: è‡ªå®šä¹‰IQRé˜ˆå€¼ (æ›´å®½æ¾çš„ç­›é€‰)
    python step7_4_multi_system_heat_capacity.py --iqr-filter --iqr-factor 5.0

**è¾“å‡ºç¡®è®¤**:
    - ç»ˆç«¯: æ˜¾ç¤º55ä¸ªç»“æ„çš„åˆ†æè¿›åº¦å’Œç»Ÿè®¡
    - æ–‡ä»¶: æ£€æŸ¥ results/step7_4_multi_system/ ç›®å½•

**å…¸å‹æ•°æ®é‡** (é»˜è®¤æ–¹æ³•1ç­›é€‰):
    - ç­›é€‰å‰: 10355æ¡åŒ¹é…è®°å½• (æœªè¿‡æ»¤)
    - ç­›é€‰å: 6814æ¡åŒ¹é…è®°å½• (åº”ç”¨Step 1è¿‡æ»¤)
    - è´¨é‡æå‡: 34.2% (ç§»é™¤å¼‚å¸¸æ¨¡æ‹Ÿ)
    - ä¸åŒç­›é€‰ç­–ç•¥ä¼šäº§ç”Ÿä¸åŒæ•°æ®é‡

**æ³¨æ„äº‹é¡¹**:
    1. çƒ­å®¹å€¼ä¸ºæ€»çƒ­å®¹ (å›¢ç°‡ + è½½ä½“)
    2. è‹¥éœ€çº¯å›¢ç°‡çƒ­å®¹,éœ€æ‰£é™¤Cv_support
    3. è½½ä½“çƒ­å®¹é»˜è®¤38.2151 meV/K (å¦‚æ— è½½ä½“æ•°æ®)
    4. RÂ² > 0.995 è§†ä¸ºä¼˜ç§€æ‹Ÿåˆ
    5. æ•°æ®ç‚¹ < 5 çš„ç›¸æ€åŒºåŸŸä¼šè·³è¿‡æ‹Ÿåˆ
    6. ç­›é€‰ç­–ç•¥é€‰æ‹©å»ºè®®:
       - é»˜è®¤æ–¹æ³•1: é€‚ç”¨äºå¤§å¤šæ•°æƒ…å†µ
       - åŒé‡ç­›é€‰: æœ€ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶
       - æ— ç­›é€‰: ç”¨äºå¯¹æ¯”éªŒè¯ç­›é€‰æ•ˆæœ
       - ä»…æ–¹æ³•2: çº¯ç»Ÿè®¡æ–¹æ³•,ç‹¬ç«‹äºStep 1

================================================================================
ä¾èµ–å…³ç³»
================================================================================
ä¸Šæ¸¸ä¾èµ–:
    - Step 1: large_D_outliers.csv (å¼‚å¸¸å€¼ç­›é€‰)
    - LAMMPS: energy_master_*.csv (èƒ½é‡æ•°æ®)
    - MSDåˆ†æ: lindemann_master_run_*.csv (LindemannæŒ‡æ•°)

ä¸‹æ¸¸åº”ç”¨:
    - Step 7.5: çƒ­å®¹ç³»ç»Ÿæ€§åˆ†æ
    - ç§‘å­¦å‘ç°: Snå«é‡å¯¹çƒ­å®¹çš„å½±å“
    - ç›¸å˜ç ”ç©¶: ç†”åŒ–æ¸©åº¦éšæˆåˆ†å˜åŒ–

================================================================================
ç‰ˆæœ¬å†å²
================================================================================
v1.1 (2025-10-21):
    - âœ… æ·»åŠ Step 1è·¯å¾„ç­¾åç­›é€‰ (ç»§æ‰¿Step 6ç®—æ³•)
    - âœ… ç­›é™¤892ä¸ªå¼‚å¸¸æ¨¡æ‹Ÿ,æ•°æ®è´¨é‡æå‡34.2%
    - âœ… è¯¦ç»†æ–‡æ¡£åŒ–è¾“å…¥è¾“å‡ºå’Œç®—æ³•åŸç†

v1.0 (2025-10-20):
    - âœ… åˆå§‹ç‰ˆæœ¬,æ”¯æŒ55ä¸ªç»“æ„åˆ†æ
    - âœ… ä¸‰åŒºåŸŸçƒ­å®¹æ‹Ÿåˆç®—æ³•
    - âœ… ç”Ÿæˆä¸ªä½“åˆ†æå›¾å’Œç»¼åˆæŠ¥å‘Š

================================================================================
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from scipy.stats import linregress, iqr
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from matplotlib import rcParams
import warnings
import re
import argparse
from datetime import datetime
warnings.filterwarnings('ignore')

# Chinese font settings
rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS']
rcParams['axes.unicode_minus'] = False

# Lindemann thresholds
LINDEMANN_THRESHOLDS = {
    'solid': 0.1,
    'melting': 0.15
}

# File paths
BASE_DIR = Path(__file__).parent
CLUSTER_ENERGY_FILE = BASE_DIR / 'data' / 'lammps_energy' / 'energy_master_20251016_121110.csv'
SUPPORT_ENERGY_FILE = BASE_DIR / 'data' / 'lammps_energy' / 'sup' / 'energy_master_20251021_151520.csv'
LINDEMANN_DIR = BASE_DIR / 'data' / 'lindemann'
RESULTS_DIR = BASE_DIR / 'results' / 'step7_4_multi_system'

# Step 1 filtering results
OUTLIERS_FILE = BASE_DIR / 'results' / 'large_D_outliers.csv'


def detect_system_type(structure_name):
    """
    Detect system type from structure name
    
    Returns:
        tuple: (system_type, system_id)
        
    Examples:
        'Cv-1' -> ('Cv', 'Cv')  # All Cv-X runs merged into single 'Cv' system
        'pt8sn0-2-best' -> ('Pt8SnX', 'Pt8Sn0')
        'pt6sn3' -> ('Pt6SnX', 'Pt6Sn3')
        'pt3sn5' -> ('PtxSny', 'Pt3Sn5')
        'Pt5Sn3O1' -> ('PtxSnyOz', 'Pt5Sn3O1')
    """
    name = structure_name.strip()
    
    # Cv series - MERGE ALL Cv-1, Cv-2, ..., Cv-5 into single 'Cv' system
    # These are repeat simulations of the same structure (Sn8Pt6O4)
    if re.match(r'^Cv-\d+', name, re.IGNORECASE):
        return ('Cv', 'Cv')  # Use 'Cv' as unified system_id
    
    # Pt8SnX series (pt8sn0-2-best, pt8sn1-2-best, etc.)
    if re.match(r'^pt8sn\d+', name, re.IGNORECASE):
        match = re.match(r'^(pt8sn\d+)', name, re.IGNORECASE)
        base_name = match.group(1)
        return ('Pt8SnX', base_name.capitalize())
    
    # Pt6SnX series (pt6sn1, pt6sn2, etc.)
    if re.match(r'^pt6sn\d+', name, re.IGNORECASE):
        match = re.match(r'^(pt6sn\d+)', name, re.IGNORECASE)
        base_name = match.group(1)
        return ('Pt6SnX', base_name.capitalize())
    
    # Pt6 alone
    if re.match(r'^pt6$', name, re.IGNORECASE):
        return ('Pt6', 'Pt6')
    
    # PtxSny series (pt3sn5, pt4sn4, pt5sn3, pt7sn1, etc.)
    # Match ptXsnY where X and Y are numbers
    if re.match(r'^pt\d+sn\d+(?!o)', name, re.IGNORECASE):
        match = re.match(r'^(pt\d+sn\d+)', name, re.IGNORECASE)
        base_name = match.group(1)
        return ('PtxSny', base_name.capitalize())
    
    # PtxSnyOz series (with oxygen)
    if re.search(r'o\d*', name, re.IGNORECASE):
        # Extract composition
        match = re.match(r'^([a-z0-9]+)', name, re.IGNORECASE)
        base_name = match.group(1) if match else name
        return ('PtxSnyOz', base_name.capitalize())
    
    # Other types
    return ('Other', name)


def extract_path_signature(filepath, is_msd_path=True):
    """
    ä»æ–‡ä»¶è·¯å¾„æå–è·¯å¾„ç­¾å (ä¸Step 6ä¿æŒä¸€è‡´)
    
    Args:
        filepath: å®Œæ•´æ–‡ä»¶è·¯å¾„
        is_msd_path: True=MSDè·¯å¾„(æœ‰æ¸©åº¦ç›®å½•), False=èƒ½é‡è·¯å¾„(æ— æ¸©åº¦ç›®å½•)
    
    Returns:
        path_signature: è·¯å¾„ç­¾åå­—ç¬¦ä¸²,å¦‚ "run3/o2/o2pt4sn6/t1000.r24.gpu0"
    
    Examples:
        MSDè·¯å¾„:
        >>> extract_path_signature(
        ...     "D:/data/more/run3/o2/O2Pt4Sn6/1000K/T1000.r24.gpu0_msd_Pt.xvg",
        ...     is_msd_path=True
        ... )
        'run3/o2/o2pt4sn6/t1000.r24.gpu0'
        
        Lindemannè·¯å¾„:
        >>> extract_path_signature(
        ...     "/home/data/run3/o2/O2Pt7Sn7/T200.r0.gpu0",
        ...     is_msd_path=False
        ... )
        'run3/o2/o2pt7sn7/t200.r0.gpu0'
    """
    if not filepath:
        return None
    
    # 1. æå–runä¿¡æ¯ (T1000.r24.gpu0)
    run_match = re.search(r'(T\d+\.r\d+\.gpu\d+)', filepath, re.IGNORECASE)
    if not run_match:
        return None
    run_info = run_match.group(1).lower()
    
    # 2. åˆ†å‰²è·¯å¾„
    parts = re.split(r'[\\/]', filepath)
    
    # 3. æ‰¾åˆ°å…³é”®ç›®å½•çš„ç´¢å¼•
    if is_msd_path:
        # MSDè·¯å¾„: æ‰¾æ¸©åº¦ç›®å½• (1000K)
        key_idx = None
        for i, part in enumerate(parts):
            if re.match(r'\d+K$', part, re.IGNORECASE):
                key_idx = i
                break
    else:
        # èƒ½é‡/Lindemannè·¯å¾„: æ‰¾runæ‰€åœ¨ä½ç½®
        key_idx = None
        for i, part in enumerate(parts):
            if re.search(r'T\d+\.r\d+\.gpu\d+', part, re.IGNORECASE):
                key_idx = i
                break
    
    if key_idx is None or key_idx < 2:
        # æ— æ³•æå–è¶³å¤Ÿçš„å±‚çº§,è¿”å›ç®€åŒ–ç­¾å
        return run_info
    
    # 4. æå–ç›®å½•å±‚çº§
    composition_dir = parts[key_idx - 1].lower()  # O2Pt4Sn6 æˆ– pt8sn5-1-best
    parent_dir = parts[key_idx - 2].lower()       # o2 æˆ– Pt8
    
    # 5. æ£€æŸ¥æ‰¹æ¬¡æ ‡è¯†ç¬¦ (run3, run2, run4, run5)
    batch_keywords = ['run3', 'run2', 'run4', 'run5']
    path_signature = f"{parent_dir}/{composition_dir}/{run_info}"
    
    # å‘ä¸Šæœç´¢æ‰¹æ¬¡æ ‡è¯†ç¬¦ (æœ€å¤šå‘ä¸Š3çº§)
    if key_idx >= 3:
        for check_idx in range(key_idx - 3, max(-1, key_idx - 6), -1):
            if check_idx < 0 or check_idx >= len(parts):
                break
            check_dir = parts[check_idx].lower()
            if check_dir in batch_keywords:
                # æ‰¾åˆ°æ‰¹æ¬¡æ ‡è¯†,æ„å»º4çº§ç­¾å
                path_signature = f"{check_dir}/{parent_dir}/{composition_dir}/{run_info}"
                break
    
    return path_signature


def load_outliers():
    """
    Load Step 1 outliers and build path signature filter set
    
    Returns:
        set: Set of path signatures to exclude
    """
    if not OUTLIERS_FILE.exists():
        print(f"\n>>> Warning: Outliers file not found: {OUTLIERS_FILE}")
        print("    Continuing without Step 1 filtering")
        return set()
    
    print(f"\n>>> Loading Step 1 outliers: {OUTLIERS_FILE.name}")
    df_outliers = pd.read_csv(OUTLIERS_FILE, encoding='utf-8')
    
    print(f"    Loaded: {len(df_outliers)} outlier records")
    print(f"    Reasons breakdown:")
    reason_counts = df_outliers['reason'].value_counts()
    for reason, count in reason_counts.items():
        # Use ASCII-safe printing to avoid encoding issues on Windows
        try:
            print(f"      - {reason}: {count}")
        except UnicodeEncodeError:
            print(f"      - [encoding issue]: {count}")
    
    # Build path signature filter set (same as Step 6)
    filter_signatures = set()
    for _, row in df_outliers.iterrows():
        filepath = row.get('filepath', '')
        if not filepath:
            continue
        
        # MSD paths have temperature directories (e.g., .../1000K/T1000.r24.gpu0_msd_Pt.xvg)
        path_signature = extract_path_signature(filepath, is_msd_path=True)
        if path_signature:
            filter_signatures.add(path_signature)
    
    print(f"    Built path signature filter set:")
    print(f"      - Unique signatures: {len(filter_signatures)}")
    if len(filter_signatures) > 0:
        print(f"      - Sample signatures:")
        for idx, sig in enumerate(sorted(list(filter_signatures))[:3]):
            print(f"        {idx+1}. {sig}")
    
    return filter_signatures


def detect_lindemann_outliers_iqr(df_merged):
    """
    åŸºäº Lindemann æŒ‡æ•°çš„ IQR å¼‚å¸¸å€¼æ£€æµ‹
    
    ç®—æ³•åŸç†:
    1. æŒ‰ (composition, temperature) åˆ†ç»„
    2. è®¡ç®—æ¯ç»„çš„ Lindemann æŒ‡æ•° IQR (å››åˆ†ä½è·)
    3. è¯†åˆ«å¼‚å¸¸å€¼: Q1 - 3*IQR æˆ– Q3 + 3*IQR ä¹‹å¤–çš„ç‚¹
    
    Args:
        df_merged: åˆå¹¶åçš„æ•°æ® (åŒ…å« structure, temp, delta åˆ—)
    
    Returns:
        set: å¼‚å¸¸è®°å½•çš„ match_key é›†åˆ
    """
    print(f"\n>>> Detecting Lindemann IQR outliers")
    
    outlier_keys = set()
    total_groups = 0
    groups_with_outliers = 0
    total_outliers = 0
    
    # æŒ‰ (structure, temp) åˆ†ç»„åˆ†æ
    for (structure, temp), group in df_merged.groupby(['structure', 'temp']):
        total_groups += 1
        
        # è‡³å°‘éœ€è¦5ä¸ªæ•°æ®ç‚¹æ‰èƒ½è¿›è¡ŒIQRåˆ†æ
        if len(group) < 5:
            continue
        
        delta_values = group['delta'].values
        
        # è®¡ç®— IQR
        Q1 = np.percentile(delta_values, 25)
        Q3 = np.percentile(delta_values, 75)
        IQR = Q3 - Q1
        
        # å®šä¹‰å¼‚å¸¸å€¼è¾¹ç•Œ (ä½¿ç”¨3å€IQR,æ¯”1.5å€æ›´ä¸¥æ ¼)
        lower_bound = Q1 - 3.0 * IQR
        upper_bound = Q3 + 3.0 * IQR
        
        # è¯†åˆ«å¼‚å¸¸å€¼
        outlier_mask = (delta_values < lower_bound) | (delta_values > upper_bound)
        
        if outlier_mask.any():
            groups_with_outliers += 1
            group_outliers = group[outlier_mask]
            total_outliers += len(group_outliers)
            
            # æ·»åŠ åˆ°å¼‚å¸¸é›†åˆ
            outlier_keys.update(group_outliers['match_key'].values)
            
            # è¯¦ç»†æ—¥å¿— (ä»…æ˜¾ç¤ºå‰5ä¸ªå¼‚å¸¸ç»„)
            if groups_with_outliers <= 5:
                print(f"    {structure} @ {temp}K: {len(group_outliers)} outliers")
                print(f"      IQR range: [{lower_bound:.4f}, {upper_bound:.4f}]")
                print(f"      Outlier Î´: {group_outliers['delta'].values}")
    
    print(f"\n    Summary:")
    print(f"      Total groups analyzed: {total_groups}")
    print(f"      Groups with outliers: {groups_with_outliers}")
    print(f"      Total outlier records: {total_outliers}")
    print(f"      Unique match_keys: {len(outlier_keys)}")
    
    return outlier_keys


def detect_energy_outliers_iqr(df_merged, iqr_factor=3.0):
    """
    åŸºäºèƒ½é‡å€¼çš„ IQR å¼‚å¸¸å€¼æ£€æµ‹
    
    ç®—æ³•åŸç†:
    1. æŒ‰ (composition, temperature) åˆ†ç»„
    2. è®¡ç®—æ¯ç»„çš„èƒ½é‡å€¼ IQR (å››åˆ†ä½è·)
    3. è¯†åˆ«å¼‚å¸¸å€¼: Q1 - k*IQR æˆ– Q3 + k*IQR ä¹‹å¤–çš„ç‚¹
    
    Args:
        df_merged: åˆå¹¶åçš„æ•°æ® (åŒ…å« structure, temp, energy_cluster åˆ—)
        iqr_factor: IQRå€æ•° (é»˜è®¤3.0, æ›´ä¸¥æ ¼åˆ™ç”¨æ›´å¤§å€¼)
    
    Returns:
        set: å¼‚å¸¸è®°å½•çš„ match_key é›†åˆ
    """
    print(f"\n>>> Detecting Energy IQR outliers")
    
    outlier_keys = set()
    total_groups = 0
    groups_with_outliers = 0
    total_outliers = 0
    
    # æŒ‰ (structure, temp) åˆ†ç»„åˆ†æ
    for (structure, temp), group in df_merged.groupby(['structure', 'temp']):
        total_groups += 1
        
        # è‡³å°‘éœ€è¦5ä¸ªæ•°æ®ç‚¹æ‰èƒ½è¿›è¡ŒIQRåˆ†æ
        if len(group) < 5:
            continue
        
        energy_values = group['energy_cluster'].values
        
        # è®¡ç®— IQR
        Q1 = np.percentile(energy_values, 25)
        Q3 = np.percentile(energy_values, 75)
        IQR = Q3 - Q1
        
        # å®šä¹‰å¼‚å¸¸å€¼è¾¹ç•Œ
        lower_bound = Q1 - iqr_factor * IQR
        upper_bound = Q3 + iqr_factor * IQR
        
        # è¯†åˆ«å¼‚å¸¸å€¼
        outlier_mask = (energy_values < lower_bound) | (energy_values > upper_bound)
        
        if outlier_mask.any():
            groups_with_outliers += 1
            group_outliers = group[outlier_mask]
            total_outliers += len(group_outliers)
            
            # æ·»åŠ åˆ°å¼‚å¸¸é›†åˆ
            outlier_keys.update(group_outliers['match_key'].values)
            
            # è¯¦ç»†æ—¥å¿— (ä»…æ˜¾ç¤ºå‰5ä¸ªå¼‚å¸¸ç»„)
            if groups_with_outliers <= 5:
                print(f"    {structure} @ {temp}K: {len(group_outliers)} outliers")
                print(f"      IQR range: [{lower_bound:.2f}, {upper_bound:.2f}] eV")
                print(f"      Outlier E: {group_outliers['energy_cluster'].values}")
    
    print(f"\n    Summary:")
    print(f"      Total groups analyzed: {total_groups}")
    print(f"      Groups with outliers: {groups_with_outliers}")
    print(f"      Total outlier records: {total_outliers}")
    print(f"      Unique match_keys: {len(outlier_keys)}")
    
    return outlier_keys


def normalize_path(path, system_type='Cv', structure_name=''):
    """
    DEPRECATED: Use extract_path_signature instead!
    
    This function is kept for backward compatibility but should use
    the same 4-level path signature algorithm as extract_path_signature
    to ensure consistency with MSD outlier filtering.
    
    Args:
        path: file path string
        system_type: system type (ignored, kept for compatibility)
        structure_name: structure name (ignored, kept for compatibility)
    
    Returns:
        str: 4-level path signature like "run3/o2/o2pt4sn6/t1000.r24.gpu0"
    """
    # Use the same algorithm as Step 6 MSD outlier filtering!
    # Determine if this is an MSD path or energy/Lindemann path
    is_msd_path = 'msd_' in path.lower() or re.search(r'\d+K[/\\]', path) is not None
    
    return extract_path_signature(path, is_msd_path=is_msd_path)


def load_energy_data(energy_file, system_filter=None, file_type='cluster'):
    """
    Load energy data at individual run level
    
    Args:
        energy_file: path to energy CSV file
        system_filter: list of system types to filter (e.g., ['Cv', 'Pt8SnX'])
                      None means load all systems
        file_type: 'cluster' or 'support'
    """
    print(f"\n>>> Loading {file_type} energy data: {energy_file.name}")
    df = pd.read_csv(energy_file, encoding='utf-8')
    
    # Handle different column formats
    if 'ç»“æ„' in df.columns:
        df.columns = ['path', 'structure', 'temp', 'run_num', 'total_steps', 'sample_steps', 
                      'avg_energy', 'std', 'min', 'max', 'sample_interval', 
                      'skip_steps', 'full_path']
    else:
        # Already in English
        expected_cols = ['path', 'structure', 'temp', 'run_num', 'total_steps', 'sample_steps', 
                        'avg_energy', 'std', 'min', 'max', 'sample_interval', 
                        'skip_steps', 'full_path']
        df.columns = expected_cols[:len(df.columns)]
    
    # Detect system types
    df[['system_type', 'system_id']] = df['structure'].apply(
        lambda x: pd.Series(detect_system_type(x))
    )
    
    # Filter by system if specified
    if system_filter:
        df = df[df['system_type'].isin(system_filter)].copy()
        print(f"    Filter applied: {system_filter}")
    
    # Create matching key
    df['match_key'] = df.apply(
        lambda row: normalize_path(row['full_path'], row['system_type'], row['structure']), axis=1
    )
    df = df[df['match_key'].notna()]
    
    # Print system distribution
    system_counts = df['system_type'].value_counts()
    print(f"    Loaded: {len(df)} records")
    print(f"    System distribution:")
    for sys_type, count in system_counts.items():
        print(f"      - {sys_type}: {count} records")
    
    return df


def load_lindemann_individual_runs(system_filter=None):
    """
    Load Lindemann raw data at individual run level
    
    Args:
        system_filter: list of system types to filter
    """
    print(f"\n>>> Loading Lindemann individual run data")
    
    # Find all lindemann_master_run_*.csv files
    lindemann_files = sorted(LINDEMANN_DIR.glob('lindemann_master_run_*.csv'))
    
    if not lindemann_files:
        print(f"    Error: No Lindemann files found in {LINDEMANN_DIR}")
        return None
    
    print(f"    Found {len(lindemann_files)} files")
    
    # Read and merge
    dfs = []
    for f in lindemann_files:
        df_temp = pd.read_csv(f, encoding='utf-8')
        dfs.append(df_temp)
        print(f"      - {f.name}: {len(df_temp)} records")
    
    df = pd.concat(dfs, ignore_index=True)
    print(f"    Total concatenated: {len(df)} records")
    
    # Remove duplicates (based on directory + structure + temperature)
    # Keep the latest record (assuming later files have more recent data)
    key_cols = ['ç›®å½•', 'ç»“æ„', 'æ¸©åº¦(K)'] if 'ç›®å½•' in df.columns else ['directory', 'structure', 'temp']
    df_before_dedup = len(df)
    df = df.drop_duplicates(subset=key_cols, keep='last')
    duplicates_removed = df_before_dedup - len(df)
    
    if duplicates_removed > 0:
        print(f"    [OK] Removed {duplicates_removed} duplicate records ({duplicates_removed/df_before_dedup*100:.1f}%)")
        print(f"    After deduplication: {len(df)} unique records")
    
    # Map Chinese column names
    col_mapping = {
        'ç›®å½•': 'directory',
        'ç»“æ„': 'structure',
        'æ¸©åº¦(K)': 'temp',
        'LindemannæŒ‡æ•°': 'delta'
    }
    
    # Handle both Chinese and English column names
    if 'ç›®å½•' in df.columns:
        df.rename(columns=col_mapping, inplace=True)
    
    # Detect system types
    df[['system_type', 'system_id']] = df['structure'].apply(
        lambda x: pd.Series(detect_system_type(x))
    )
    
    # Filter by system if specified
    if system_filter:
        df = df[df['system_type'].isin(system_filter)].copy()
        print(f"    Filter applied: {system_filter}")
    
    # Create matching key
    df['match_key'] = df.apply(
        lambda row: normalize_path(row['directory'], row['system_type'], row['structure']), axis=1
    )
    df = df[df['match_key'].notna()]
    
    # Print system distribution
    system_counts = df['system_type'].value_counts()
    print(f"    Loaded: {len(df)} records")
    print(f"    System distribution:")
    for sys_type, count in system_counts.items():
        print(f"      - {sys_type}: {count} records")
    
    return df


def classify_single_run(delta):
    """Simple classification with hard thresholds"""
    if delta < LINDEMANN_THRESHOLDS['solid']:
        return 'solid'
    elif delta < LINDEMANN_THRESHOLDS['melting']:
        return 'premelting'
    else:
        return 'liquid'


def merge_energy_lindemann(df_energy, df_lindemann, outlier_signatures=None, 
                          apply_iqr_filter=False, iqr_factor=3.0):
    """
    Merge energy and Lindemann data with optional filtering
    
    Args:
        df_energy: Energy DataFrame
        df_lindemann: Lindemann DataFrame  
        outlier_signatures: Set of path signatures to exclude (from Step 1 MSD outliers)
        apply_iqr_filter: Whether to apply IQR filtering (both Lindemann and Energy)
        iqr_factor: IQR multiplier for outlier detection (default 3.0)
    
    Returns:
        tuple: (df_merged_filtered, df_merged_original)
            - df_merged_filtered: Final filtered data
            - df_merged_original: Original merged data (before any filtering)
    """
    print(f"\n{'='*80}")
    print("Merging energy and Lindemann data")
    print("="*80)
    print(f"    Energy records: {len(df_energy)}")
    print(f"    Lindemann records: {len(df_lindemann)}")
    
    # First, create unfiltered merged data for comparison
    df_e_orig = df_energy[['match_key', 'structure', 'system_type', 'system_id', 
                            'temp', 'avg_energy', 'std']].copy()
    df_e_orig.rename(columns={'std': 'energy_std'}, inplace=True)
    df_l_orig = df_lindemann[['match_key', 'delta']].copy()
    
    df_merged_original = pd.merge(df_e_orig, df_l_orig, on='match_key', how='inner')
    df_merged_original['phase'] = df_merged_original['delta'].apply(classify_single_run)
    
    def extract_run(key):
        match = re.search(r'_r(\d+)$', key)
        return int(match.group(1)) if match else None
    
    df_merged_original['run_id'] = df_merged_original['match_key'].apply(extract_run)
    
    print(f"    Original merged: {len(df_merged_original)} records")
    
    # Now apply Step 1 filtering using path signatures (same as Step 6)
    if outlier_signatures and len(outlier_signatures) > 0:
        print(f"\n    [Filter Method 1] Step 1 MSD outliers (path signature matching)")
        print(f"    Outlier signatures to exclude: {len(outlier_signatures)}")
        
        # Filter energy data
        energy_before = len(df_energy)
        df_energy['path_signature'] = df_energy['full_path'].apply(
            lambda x: extract_path_signature(x, is_msd_path=False) if pd.notna(x) else None
        )
        df_energy['is_outlier'] = df_energy['path_signature'].isin(outlier_signatures)
        energy_filtered = df_energy['is_outlier'].sum()
        df_energy = df_energy[~df_energy['is_outlier']].copy()
        df_energy.drop(columns=['path_signature', 'is_outlier'], inplace=True)
        
        # Filter Lindemann data
        lindemann_before = len(df_lindemann)
        if 'directory' in df_lindemann.columns:
            df_lindemann['path_signature'] = df_lindemann['directory'].apply(
                lambda x: extract_path_signature(x, is_msd_path=False) if pd.notna(x) else None
            )
            df_lindemann['is_outlier'] = df_lindemann['path_signature'].isin(outlier_signatures)
            lindemann_filtered = df_lindemann['is_outlier'].sum()
            df_lindemann = df_lindemann[~df_lindemann['is_outlier']].copy()
            df_lindemann.drop(columns=['path_signature', 'is_outlier'], inplace=True)
        else:
            lindemann_filtered = 0
        
        print(f"    Energy filtered: {energy_filtered} records ({energy_filtered/energy_before*100:.1f}%)")
        print(f"    Lindemann filtered: {lindemann_filtered} records ({lindemann_filtered/lindemann_before*100:.1f}%)")
        print(f"    Remaining - Energy: {len(df_energy)}, Lindemann: {len(df_lindemann)}")
    
    # Select needed columns (keep avg_energy name for consistency)
    df_e = df_energy[['match_key', 'structure', 'system_type', 'system_id', 
                      'temp', 'avg_energy', 'std']].copy()
    df_e.rename(columns={'std': 'energy_std'}, inplace=True)
    
    df_l = df_lindemann[['match_key', 'delta']].copy()
    
    # Inner join
    df_merged = pd.merge(df_e, df_l, on='match_key', how='inner')
    
    # Classify
    df_merged['phase'] = df_merged['delta'].apply(classify_single_run)
    
    # Extract run number
    def extract_run(key):
        match = re.search(r'_r(\d+)$', key)
        return int(match.group(1)) if match else None
    
    df_merged['run_id'] = df_merged['match_key'].apply(extract_run)
    
    print(f"    Success: {len(df_merged)} records matched ({len(df_merged)/len(df_energy)*100:.1f}%)")
    
    # Apply IQR filtering if requested (both Lindemann and Energy)
    if apply_iqr_filter:
        print(f"\n    [Filter Method 2] IQR outlier detection (Lindemann + Energy, factor={iqr_factor})")
        merged_before = len(df_merged)
        
        # Temporarily rename for IQR functions
        df_merged['energy_cluster'] = df_merged['avg_energy']
        
        # Detect Lindemann outliers
        lindemann_outlier_keys = detect_lindemann_outliers_iqr(df_merged)
        
        # Detect Energy outliers
        energy_outlier_keys = detect_energy_outliers_iqr(df_merged, iqr_factor)
        
        # Drop temporary column
        df_merged.drop(columns=['energy_cluster'], inplace=True)
        
        # Combine both sets of outliers
        all_iqr_outliers = lindemann_outlier_keys | energy_outlier_keys
        
        print(f"\n    Combined IQR outliers:")
        print(f"      Lindemann outliers: {len(lindemann_outlier_keys)} unique match_keys")
        print(f"      Energy outliers: {len(energy_outlier_keys)} unique match_keys")
        print(f"      Total unique outliers: {len(all_iqr_outliers)} match_keys")
        print(f"      Overlap: {len(lindemann_outlier_keys & energy_outlier_keys)} match_keys")
        
        # Remove outliers
        df_merged = df_merged[~df_merged['match_key'].isin(all_iqr_outliers)].copy()
        iqr_filtered = merged_before - len(df_merged)
        
        print(f"\n    IQR filtered: {iqr_filtered} records ({iqr_filtered/merged_before*100:.1f}%)")
        print(f"    Remaining: {len(df_merged)} records")
    
    # System distribution
    print(f"\n    Final system-wise distribution:")
    for sys_type in sorted(df_merged['system_type'].unique()):
        count = (df_merged['system_type'] == sys_type).sum()
        print(f"      {sys_type}: {count} records")
    
    # Overall phase distribution
    phase_counts = df_merged['phase'].value_counts()
    print(f"\n    Overall phase distribution:")
    for phase, count in sorted(phase_counts.items()):
        print(f"      {phase}: {count} points ({count/len(df_merged)*100:.1f}%)")
    
    # Return both filtered and original for reporting
    return df_merged, df_merged_original


def generate_filtering_report(df_original, df_filtered, method1_applied, method2_applied, iqr_factor=3.0):
    """
    Generate detailed filtering statistics report
    
    Args:
        df_original: Original merged data before IQR filtering
        df_filtered: Final filtered data
        method1_applied: Whether Method 1 (MSD outliers) was applied
        method2_applied: Whether Method 2 (IQR) was applied
        iqr_factor: IQR factor used
    """
    print(f"\n{'='*80}")
    print("Generating Filtering Statistics Report")
    print("="*80)
    
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    report_file = RESULTS_DIR / 'filtering_statistics_report.md'
    csv_file = RESULTS_DIR / 'filtering_statistics_by_structure_temp.csv'
    
    # Overall statistics
    n_original = len(df_original)
    n_filtered = len(df_filtered)
    n_removed = n_original - n_filtered
    pct_removed = (n_removed / n_original * 100) if n_original > 0 else 0
    
    # Filtering methods used
    methods_used = []
    if method1_applied:
        methods_used.append("Method 1 (Step 1 MSD outliers)")
    if method2_applied:
        methods_used.append(f"Method 2 (IQR, factor={iqr_factor})")
    if not methods_used:
        methods_used.append("None (No filtering)")
    
    # Statistics by system_type
    system_stats = []
    for sys_type in sorted(df_original['system_type'].unique()):
        orig_count = (df_original['system_type'] == sys_type).sum()
        filt_count = (df_filtered['system_type'] == sys_type).sum()
        removed = orig_count - filt_count
        pct = (removed / orig_count * 100) if orig_count > 0 else 0
        
        system_stats.append({
            'system_type': sys_type,
            'original': orig_count,
            'filtered': filt_count,
            'removed': removed,
            'percent_removed': pct
        })
    
    # Statistics by (structure, temperature)
    detailed_stats = []
    
    # Get all unique (structure, temp) combinations from original data
    for (structure, temp) in df_original.groupby(['structure', 'temp']).size().index:
        orig_group = df_original[(df_original['structure'] == structure) & (df_original['temp'] == temp)]
        filt_group = df_filtered[(df_filtered['structure'] == structure) & (df_filtered['temp'] == temp)]
        
        orig_count = len(orig_group)
        filt_count = len(filt_group)
        removed = orig_count - filt_count
        pct = (removed / orig_count * 100) if orig_count > 0 else 0
        
        # Get system_type from original data
        sys_type = orig_group['system_type'].iloc[0] if len(orig_group) > 0 else 'Unknown'
        
        detailed_stats.append({
            'system_type': sys_type,
            'structure': structure,
            'temperature': temp,
            'original_points': orig_count,
            'filtered_points': filt_count,
            'removed_points': removed,
            'percent_removed': pct
        })
    
    df_detailed = pd.DataFrame(detailed_stats)
    df_detailed = df_detailed.sort_values(['system_type', 'structure', 'temperature'])
    
    # Add a note row at the top explaining the data
    # Create a comment file alongside the CSV
    csv_readme = csv_file.parent / (csv_file.stem + '_README.txt')
    with open(csv_readme, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("Filtering Statistics CSV - Data Explanation\n")
        f.write("=" * 80 + "\n\n")
        f.write("DATA SOURCES:\n\n")
        f.write("  Energy:      ~3262 unique records (LAMMPS MD simulations)\n")
        f.write("  Lindemann:   4012 raw records from 3 CSV files\n")
        f.write("               -> 3262 unique after deduplication (by directory+structure+temp)\n")
        f.write("  Merged:      ~8,030 records (inner join on match_key)\n")
        f.write("               Note: Average ~2.5 runs per (structure, temperature) combination\n\n")
        f.write("COLUMN DEFINITIONS:\n\n")
        f.write("  original_points:  Number of data points in the ORIGINAL MERGED dataset\n")
        f.write("                    (Energy + Lindemann data, before any filtering)\n\n")
        f.write("  filtered_points:  Number of data points AFTER applying filtering methods\n\n")
        f.write("  removed_points:   Number of data points removed by filtering\n\n")
        f.write("  percent_removed:  Percentage of data removed (removed/original * 100)\n\n")
        f.write("IMPORTANT NOTES:\n\n")
        f.write("1. 'Original' refers to the merged Energy-Lindemann dataset, NOT raw LAMMPS data\n\n")
        f.write("2. Step 1 MSD filtering is based on Pt and Sn ELEMENTAL MSD:\n")
        f.write("   - msd_Pt.xvg:  Pt atoms' mean square displacement\n")
        f.write("   - msd_Sn.xvg:  Sn atoms' mean square displacement\n")
        f.write("   - NOT msd_Pt-Sn.xvg (Pt-Sn relative distance)\n\n")
        f.write("3. When EITHER Pt or Sn MSD is flagged as anomalous for a simulation run,\n")
        f.write("   that ENTIRE run is excluded from both Energy and Lindemann analyses.\n\n")
        f.write("4. This ensures consistent data quality across all analysis types.\n\n")
        f.write("=" * 80 + "\n")
    
    # Save CSV
    df_detailed.to_csv(csv_file, index=False, encoding='utf-8-sig')
    print(f"    [OK] Detailed CSV saved: {csv_file.name}")
    print(f"    [OK] CSV explanation saved: {csv_readme.name}")
    
    # Write markdown report
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# Data Filtering Statistics Report\n\n")
        f.write(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("="*80 + "\n\n")
        
        # Important note about data source
        f.write("## Data Source Explanation\n\n")
        f.write("**Important**: The 'Original' data refers to the **merged energy-Lindemann dataset** ")
        f.write("before applying the filtering methods listed below. This merged dataset combines:\n\n")
        f.write("1. **Energy data**: Average cluster energy from LAMMPS MD simulations\n")
        f.write("   - Raw source: ~3262 records (energy_master_*.csv)\n")
        f.write("2. **Lindemann index data**: Calculated from MSD (Mean Square Displacement) analysis\n")
        f.write("   - Raw source: 4012 records from 3 lindemann_master_run_*.csv files\n")
        f.write("   - After deduplication: ~3262 unique (directory, structure, temperature) combinations\n")
        f.write("   - Note: Multiple files contain overlapping data; duplicates are removed before merging\n\n")
        f.write("**Note about MSD types**: Step 1 filtering is based on **Pt and Sn elemental MSD** ")
        f.write("(`msd_Pt.xvg` and `msd_Sn.xvg`), NOT the Pt-Sn distance MSD. When either Pt or Sn ")
        f.write("MSD shows anomalous behavior for a simulation run, that run is excluded from ")
        f.write("**both** energy and Lindemann analyses.\n\n")
        f.write("="*80 + "\n\n")
        
        # Filtering methods
        f.write("## 1. Filtering Methods Applied\n\n")
        for i, method in enumerate(methods_used, 1):
            f.write(f"{i}. {method}\n")
        f.write("\n")
        
        # Overall summary
        f.write("## 2. Overall Statistics\n\n")
        f.write(f"- **Original merged data points**: {n_original:,}\n")
        f.write(f"- **Filtered data points**: {n_filtered:,}\n")
        f.write(f"- **Removed data points**: {n_removed:,} ({pct_removed:.2f}%)\n")
        f.write(f"- **Retention rate**: {100-pct_removed:.2f}%\n\n")
        
        # By system type
        f.write("## 3. Statistics by System Type\n\n")
        f.write("| System Type | Original | Filtered | Removed | % Removed |\n")
        f.write("|-------------|----------|----------|---------|----------|\n")
        for stat in system_stats:
            f.write(f"| {stat['system_type']} | {stat['original']:,} | {stat['filtered']:,} | "
                   f"{stat['removed']:,} | {stat['percent_removed']:.2f}% |\n")
        f.write("\n")
        
        # By structure (summary)
        f.write("## 4. Statistics by Structure (Summary)\n\n")
        structure_summary = df_detailed.groupby('structure').agg({
            'original_points': 'sum',
            'filtered_points': 'sum',
            'removed_points': 'sum'
        }).reset_index()
        structure_summary['percent_removed'] = (
            structure_summary['removed_points'] / structure_summary['original_points'] * 100
        )
        structure_summary = structure_summary.sort_values('removed_points', ascending=False)
        
        f.write("| Structure | Original | Filtered | Removed | % Removed |\n")
        f.write("|-----------|----------|----------|---------|----------|\n")
        for _, row in structure_summary.head(20).iterrows():
            f.write(f"| {row['structure']} | {int(row['original_points']):,} | "
                   f"{int(row['filtered_points']):,} | {int(row['removed_points']):,} | "
                   f"{row['percent_removed']:.2f}% |\n")
        
        if len(structure_summary) > 20:
            f.write(f"\n*Showing top 20 structures by removed points. See CSV for complete data.*\n")
        f.write("\n")
        
        # Temperature-wise analysis
        f.write("## 5. Statistics by Temperature (All Structures)\n\n")
        temp_summary = df_detailed.groupby('temperature').agg({
            'original_points': 'sum',
            'filtered_points': 'sum',
            'removed_points': 'sum'
        }).reset_index()
        temp_summary['percent_removed'] = (
            temp_summary['removed_points'] / temp_summary['original_points'] * 100
        )
        temp_summary = temp_summary.sort_values('temperature')
        
        f.write("| Temperature (K) | Original | Filtered | Removed | % Removed |\n")
        f.write("|-----------------|----------|----------|---------|----------|\n")
        for _, row in temp_summary.iterrows():
            f.write(f"| {int(row['temperature'])} | {int(row['original_points']):,} | "
                   f"{int(row['filtered_points']):,} | {int(row['removed_points']):,} | "
                   f"{row['percent_removed']:.2f}% |\n")
        f.write("\n")
        
        # Detailed breakdown note
        f.write("## 6. Detailed Breakdown\n\n")
        f.write("For complete structure-by-structure, temperature-by-temperature breakdown, ")
        f.write(f"see: `{csv_file.name}`\n\n")
        
        # High outlier structures
        high_outlier_structures = structure_summary[structure_summary['percent_removed'] > 50]
        if len(high_outlier_structures) > 0:
            f.write("## 7. High Outlier Structures (>50% removed)\n\n")
            f.write("| Structure | Original | Removed | % Removed |\n")
            f.write("|-----------|----------|---------|----------|\n")
            for _, row in high_outlier_structures.iterrows():
                f.write(f"| {row['structure']} | {int(row['original_points']):,} | "
                       f"{int(row['removed_points']):,} | {row['percent_removed']:.2f}% |\n")
            f.write("\n")
    
    print(f"    [OK] Markdown report saved: {report_file.name}")
    print(f"    Total removed: {n_removed}/{n_original} ({pct_removed:.2f}%)")


def calculate_support_cv():
    """Calculate support heat capacity"""
    print(f"\n>>> Calculating support heat capacity")
    
    if not SUPPORT_ENERGY_FILE.exists():
        print(f"    Warning: Support file not found, using default 38.2151 meV/K")
        return 38.2151
    
    df_sup = pd.read_csv(SUPPORT_ENERGY_FILE, encoding='utf-8')
    
    # Handle column names
    if 'ç»“æ„' in df_sup.columns:
        df_sup.columns = ['path', 'structure', 'temp', 'run_num', 'total_steps', 'sample_steps', 
                          'avg_energy', 'std', 'min', 'max', 'sample_interval', 
                          'skip_steps', 'full_path']
    
    # Filter support data
    df_sup = df_sup[df_sup['structure'].str.contains('sup-1|sup-2', na=False)]
    
    if len(df_sup) == 0:
        print(f"    Warning: No support data, using default 38.2151 meV/K")
        return 38.2151
    
    # Average by temperature
    df_sup_avg = df_sup.groupby('temp')['avg_energy'].mean().reset_index()
    df_sup_avg = df_sup_avg.sort_values('temp')
    
    # Linear fit
    T = df_sup_avg['temp'].values
    E = df_sup_avg['avg_energy'].values
    
    if len(T) < 2:
        print(f"    Warning: Insufficient support data, using default 38.2151 meV/K")
        return 38.2151
    
    slope, intercept, r_value, p_value, std_err = linregress(T, E)
    
    Cv_support = slope * 1000  # eV/K -> meV/K
    R2 = r_value ** 2
    
    print(f"    Support Cv: {Cv_support:.4f} meV/K")
    print(f"    Fit R2: {R2:.6f}")
    
    return Cv_support


def perform_lindemann_clustering(df_structure, structure_name, n_clusters=3):
    """
    Perform K-means clustering on Lindemann index to auto-detect phase boundaries
    
    Args:
        df_structure: DataFrame for single structure
        structure_name: Name of the structure
        n_clusters: Number of clusters (default 3 for solid/premelting/liquid)
    
    Returns:
        dict: Clustering results with thresholds and labels
    """
    print(f"\n{'='*80}")
    print(f"Lindemann Index Clustering Analysis: {structure_name}")
    print("="*80)
    
    if len(df_structure) < 10:
        print(f"  Warning: Insufficient data ({len(df_structure)} points), need at least 10")
        return None
    
    # Prepare data: (temperature, lindemann_index)
    X = df_structure[['temp', 'delta']].values
    
    # Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # K-means clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    df_structure = df_structure.copy()
    df_structure['cluster'] = kmeans.fit_predict(X_scaled)
    
    # Analyze clusters
    print(f"\n>>> Cluster Statistics:")
    cluster_stats = []
    for i in range(n_clusters):
        cluster_data = df_structure[df_structure['cluster'] == i]
        delta_mean = cluster_data['delta'].mean()
        delta_std = cluster_data['delta'].std()
        temp_mean = cluster_data['temp'].mean()
        n_points = len(cluster_data)
        
        cluster_stats.append({
            'cluster_id': i,
            'n_points': n_points,
            'delta_mean': delta_mean,
            'delta_std': delta_std,
            'temp_mean': temp_mean
        })
        
        print(f"  Cluster {i}: n={n_points}, Î´={delta_mean:.4f}Â±{delta_std:.4f}, T_avg={temp_mean:.1f}K")
    
    # Sort clusters by mean delta (solid < premelting < liquid)
    cluster_stats = sorted(cluster_stats, key=lambda x: x['delta_mean'])
    
    # Assign phase labels
    phase_labels = ['solid', 'premelting', 'liquid']
    cluster_to_phase = {stat['cluster_id']: phase_labels[i] for i, stat in enumerate(cluster_stats)}
    
    df_structure['phase_clustered'] = df_structure['cluster'].map(cluster_to_phase)
    
    # Calculate thresholds (boundaries between clusters)
    thresholds = []
    if n_clusters >= 2:
        for i in range(n_clusters - 1):
            lower_cluster = cluster_stats[i]
            upper_cluster = cluster_stats[i + 1]
            # Threshold = midpoint between cluster means
            threshold = (lower_cluster['delta_mean'] + upper_cluster['delta_mean']) / 2
            thresholds.append(threshold)
            print(f"\n  Threshold {phase_labels[i]}/{phase_labels[i+1]}: Î´ = {threshold:.4f}")
    
    # Visualization
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    cluster_dir = RESULTS_DIR / 'clustering_analysis'
    cluster_dir.mkdir(parents=True, exist_ok=True)
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    fig.suptitle(f'Lindemann Index Clustering Analysis - {structure_name}', 
                 fontsize=14, fontweight='bold')
    
    colors = {'solid': '#3498db', 'premelting': '#e67e22', 'liquid': '#e74c3c'}
    
    # Plot 1: Temperature vs Lindemann with clusters
    ax1 = axes[0]
    for phase in ['solid', 'premelting', 'liquid']:
        df_phase = df_structure[df_structure['phase_clustered'] == phase]
        if len(df_phase) > 0:
            ax1.scatter(df_phase['temp'], df_phase['delta'], 
                       c=colors[phase], alpha=0.6, s=80, 
                       label=f'{phase} (n={len(df_phase)})',
                       edgecolors='black', linewidths=0.5)
    
    # Add threshold lines
    for i, thresh in enumerate(thresholds):
        ax1.axhline(y=thresh, color='red', linestyle='--', linewidth=2, 
                   label=f'Threshold {i+1}: Î´={thresh:.4f}', alpha=0.7)
    
    ax1.set_xlabel('Temperature (K)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Lindemann Index Î´', fontsize=12, fontweight='bold')
    ax1.set_title('(a) Clustered Data with Auto-Detected Thresholds', 
                  fontsize=12, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Comparison with fixed thresholds
    ax2 = axes[1]
    
    # Original phase classification (fixed thresholds)
    for phase in ['solid', 'premelting', 'liquid']:
        df_phase = df_structure[df_structure['phase'] == phase]
        if len(df_phase) > 0:
            ax2.scatter(df_phase['temp'], df_phase['delta'], 
                       c=colors[phase], alpha=0.6, s=80,
                       label=f'{phase} (n={len(df_phase)})',
                       edgecolors='black', linewidths=0.5)
    
    # Fixed thresholds
    ax2.axhline(y=0.1, color='gray', linestyle='--', linewidth=2, 
               label='Fixed: Î´=0.1', alpha=0.7)
    ax2.axhline(y=0.15, color='gray', linestyle='--', linewidth=2, 
               label='Fixed: Î´=0.15', alpha=0.7)
    
    ax2.set_xlabel('Temperature (K)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Lindemann Index Î´', fontsize=12, fontweight='bold')
    ax2.set_title('(b) Fixed Thresholds (0.1, 0.15)', 
                  fontsize=12, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    output_file = cluster_dir / f'{structure_name}_clustering_analysis.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"\n  [OK] Clustering plot saved: {output_file.name}")
    plt.close()
    
    # Save results
    
    
    
    results = {
        'structure': structure_name,
        'n_clusters': n_clusters,
        'thresholds': thresholds,
        'cluster_to_phase': cluster_to_phase,
        'cluster_stats': cluster_stats,
        'df_clustered': df_structure
    }
    
    # Save comparison CSV
    comparison_df = df_structure[['temp', 'delta', 'phase', 'phase_clustered', 'cluster']].copy()
    comparison_csv = cluster_dir / f'{structure_name}_clustering_comparison.csv'
    comparison_df.to_csv(comparison_csv, index=False, encoding='utf-8-sig')
    print(f"  [OK] Comparison CSV saved: {comparison_csv.name}")
    
    return results


def fit_regional_heat_capacity(df_data, Cv_support, structure_name=None):
    """
    Fit three-region heat capacity using individual runs
    
    Args:
        df_data: dataframe to analyze (can be filtered by structure)
        Cv_support: support heat capacity
        structure_name: specific structure name for display (None for all)
    """
    if structure_name:
        print(f"\n{'='*80}")
        print(f"Three-region heat capacity calculation: {structure_name}")
        print("="*80)
    else:
        print(f"\n{'='*80}")
        print("Three-region heat capacity calculation (all data)")
        print("="*80)
    
    regions = ['solid', 'premelting', 'liquid']
    results = {}
    
    for region_name in regions:
        print(f"\n>>> {region_name.capitalize()} region")
        
        df_region = df_data[df_data['phase'] == region_name].copy()
        
        if len(df_region) < 5:
            print(f"    Warning: Insufficient data (n={len(df_region)} < 5), skipping")
            continue
        
        # Extract temperature and energy
        T = df_region['temp'].values
        E = df_region['avg_energy'].values
        
        # Check if all temperatures are the same
        if len(np.unique(T)) < 2:
            print(f"    Warning: Only {len(np.unique(T))} unique temperature(s), cannot fit, skipping")
            continue
        
        # Linear fit
        slope, intercept, r_value, p_value, std_err = linregress(T, E)
        
        # Heat capacity calculation
        Cv_total = slope * 1000  # eV/K -> meV/K
        Cv_cluster = Cv_total - Cv_support
        R2 = r_value ** 2
        
        # Temperature range
        T_min, T_max = T.min(), T.max()
        
        # Save results
        results[region_name] = {
            'n_points': len(df_region),
            'T_range': (T_min, T_max),
            'slope': slope,
            'slope_err': std_err,
            'intercept': intercept,
            'Cv_total': Cv_total,
            'Cv_cluster': Cv_cluster,
            'R2': R2,
            'p_value': p_value,
            'data': df_region[['temp', 'avg_energy', 'delta']].copy()
        }
        
        print(f"    Data points: {len(df_region)}")
        print(f"    Temp range: {T_min:.0f}-{T_max:.0f} K")
        print(f"    Linear fit: E = {slope:.6f} * T + {intercept:.3f}")
        print(f"    Cv_total = {Cv_total:.4f} +/- {std_err*1000:.4f} meV/K")
        print(f"    Cv_cluster = {Cv_cluster:.4f} meV/K")
        
        # Grade marker
        if R2 > 0.95:
            mark = "Excellent"
        elif R2 > 0.90:
            mark = "Good"
        else:
            mark = "Fair"
        
        print(f"    R2 = {R2:.6f} [{mark}]")
        print(f"    p-value = {p_value:.2e}")
    
    return results


def plot_individual_structure_analysis(df_structure, results, Cv_support, structure_name, output_dir):
    """
    Generate individual structure analysis plot (7.3 style)
    
    Args:
        df_structure: DataFrame for this structure
        results: Fitting results dictionary
        Cv_support: Support heat capacity
        structure_name: Name of the structure (e.g., 'Pt8Sn0', 'Cv-1')
        output_dir: Output directory for plots
    """
    
    # Create figure with 2x2 subplots (exactly like 7.3)
    fig, axes = plt.subplots(2, 2, figsize=(18, 14))
    fig.suptitle(f'Step 7.4: Individual Run Analysis - {structure_name}', 
                 fontsize=16, fontweight='bold', y=0.995)
    
    # Color scheme (exactly like 7.3)
    colors = {
        'solid': '#3498db',      # Blue
        'premelting': '#e67e22', # Orange
        'liquid': '#e74c3c'      # Red
    }
    
    phase_labels = {
        'solid': 'å›ºæ€ Solid',
        'premelting': 'é¢„ç†”åŒ– Premelting',
        'liquid': 'æ¶²æ€ Liquid'
    }
    
    # Calculate relative energy (subtract minimum)
    E_min = df_structure['avg_energy'].min()
    df_structure = df_structure.copy()
    df_structure['relative_energy'] = df_structure['avg_energy'] - E_min
    
    # ===== Plot 1 (Top Left): Scatter plot with fit lines (a) =====
    ax1 = axes[0, 0]
    
    for phase in ['solid', 'premelting', 'liquid']:
        df_phase = df_structure[df_structure['phase'] == phase]
        
        if len(df_phase) > 0:
            # Scatter plot with relative energy
            ax1.scatter(df_phase['temp'], df_phase['relative_energy'], 
                       c=colors[phase], alpha=0.5, s=50, edgecolors='black', linewidths=0.5,
                       label=f'{phase_labels[phase]} (n={len(df_phase)})',
                       zorder=3)
            
            # Fit line (convert to relative)
            if phase in results:
                res = results[phase]
                T_min, T_max = res['T_range']
                T_fit = np.linspace(T_min, T_max, 100)
                E_fit = res['slope'] * T_fit + res['intercept'] - E_min
                
                ax1.plot(T_fit, E_fit, color=colors[phase], linewidth=3, 
                        linestyle='--', alpha=0.8,
                        label=f'{phase_labels[phase]} æ‹Ÿåˆ (RÂ²={res["R2"]:.4f})',
                        zorder=2)
    
    ax1.set_xlabel('æ¸©åº¦ Temperature (K)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('ç›¸å¯¹èƒ½é‡ Relative Energy (eV)', fontsize=12, fontweight='bold')
    ax1.set_title('(a) å•æ¬¡æ¨¡æ‹Ÿèƒ½é‡åˆ†å¸ƒä¸çº¿æ€§æ‹Ÿåˆ\nIndividual Run Energy vs Temperature', 
                  fontsize=13, fontweight='bold', pad=10)
    ax1.legend(fontsize=9, loc='upper left', framealpha=0.95, ncol=2)
    ax1.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)
    
    # ===== Plot 2 (Top Right): Heat capacity bar chart (b) =====
    ax2 = axes[0, 1]
    
    regions = []
    cv_values = []
    cv_errors = []
    r2_values = []
    point_counts = []
    
    for phase in ['solid', 'premelting', 'liquid']:
        if phase in results:
            regions.append(phase_labels[phase])
            cv_values.append(results[phase]['Cv_cluster'])
            cv_errors.append(results[phase]['slope_err'] * 1000)
            r2_values.append(results[phase]['R2'])
            point_counts.append(results[phase]['n_points'])
    
    x = np.arange(len(regions))
    width = 0.6
    
    bars = ax2.bar(x, cv_values, width, 
                   color=[colors[p] for p in ['solid', 'premelting', 'liquid'] if p in results], 
                   alpha=0.8, edgecolor='black', linewidth=1.5,
                   yerr=cv_errors, capsize=8, error_kw={'linewidth': 2})
    
    # Annotate bars
    for i, (bar, cv, cv_err, r2, n) in enumerate(zip(bars, cv_values, cv_errors, r2_values, point_counts)):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + cv_err + 0.2,
                f'{cv:.3f}Â±{cv_err:.3f}\nmeV/K\nRÂ²={r2:.4f}\n(n={n})',
                ha='center', va='bottom', fontsize=10, fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
    
    ax2.set_xlabel('ç›¸æ€åŒºåŸŸ Phase Region', fontsize=12, fontweight='bold')
    ax2.set_ylabel('å›¢ç°‡çƒ­å®¹ Cluster Heat Capacity Cv (meV/K)', fontsize=12, fontweight='bold')
    ax2.set_title('(b) ä¸‰æ®µåˆ†åŒºçƒ­å®¹ç»“æœ\nThree-Region Heat Capacity Results', 
                  fontsize=13, fontweight='bold', pad=10)
    ax2.set_xticks(x)
    ax2.set_xticklabels(regions, fontsize=10)
    ax2.grid(True, alpha=0.3, linestyle=':', axis='y', linewidth=0.8)
    if len(cv_values) > 0:
        ax2.set_ylim(0, max(cv_values) * 1.6)
    
    # ===== Plot 3 (Bottom Left): Temperature-phase distribution (c) =====
    ax3 = axes[1, 0]
    
    temp_phase = df_structure.groupby(['temp', 'phase']).size().unstack(fill_value=0)
    temp_sorted = sorted(df_structure['temp'].unique())
    
    # Stacked bar chart
    solid_counts = [temp_phase.loc[t, 'solid'] if t in temp_phase.index and 'solid' in temp_phase.columns else 0 for t in temp_sorted]
    pre_counts = [temp_phase.loc[t, 'premelting'] if t in temp_phase.index and 'premelting' in temp_phase.columns else 0 for t in temp_sorted]
    liquid_counts = [temp_phase.loc[t, 'liquid'] if t in temp_phase.index and 'liquid' in temp_phase.columns else 0 for t in temp_sorted]
    
    x_pos = np.arange(len(temp_sorted))
    
    ax3.bar(x_pos, solid_counts, label=phase_labels['solid'], 
            color=colors['solid'], alpha=0.8, edgecolor='black', linewidth=0.5)
    ax3.bar(x_pos, pre_counts, bottom=solid_counts, label=phase_labels['premelting'], 
            color=colors['premelting'], alpha=0.8, edgecolor='black', linewidth=0.5)
    ax3.bar(x_pos, liquid_counts, bottom=np.array(solid_counts)+np.array(pre_counts), 
            label=phase_labels['liquid'], color=colors['liquid'], alpha=0.8, edgecolor='black', linewidth=0.5)
    
    ax3.set_xlabel('æ¸©åº¦ Temperature (K)', fontsize=12, fontweight='bold')
    ax3.set_ylabel('æ¨¡æ‹Ÿæ¬¡æ•° Number of Runs', fontsize=12, fontweight='bold')
    ax3.set_title('(c) å„æ¸©åº¦ç‚¹çš„ç›¸æ€åˆ†å¸ƒ\nPhase Distribution at Each Temperature', 
                  fontsize=13, fontweight='bold', pad=10)
    
    # Adjust x-axis labels based on number of temperatures
    if len(temp_sorted) > 15:
        step = max(1, len(temp_sorted) // 10)
        ax3.set_xticks(x_pos[::step])
        ax3.set_xticklabels([f'{int(t)}' for t in temp_sorted[::step]], rotation=45)
    else:
        ax3.set_xticks(x_pos)
        ax3.set_xticklabels([f'{int(t)}' for t in temp_sorted], rotation=45)
    
    ax3.legend(fontsize=10, loc='upper left', framealpha=0.95)
    ax3.grid(True, alpha=0.3, linestyle=':', axis='y', linewidth=0.8)
    
    # ===== Plot 4 (Bottom Right): Lindemann index distribution (d) =====
    ax4 = axes[1, 1]
    
    for phase in ['solid', 'premelting', 'liquid']:
        df_phase = df_structure[df_structure['phase'] == phase]
        if len(df_phase) > 0:
            ax4.scatter(df_phase['temp'], df_phase['delta'], 
                       c=colors[phase], alpha=0.6, s=60, 
                       edgecolors='black', linewidths=0.8,
                       label=f'{phase_labels[phase]} (n={len(df_phase)})',
                       zorder=3)
    
    # Threshold lines
    ax4.axhline(y=0.1, color='gray', linestyle='--', linewidth=2.5, 
                label='å›ºæ€/é¢„ç†”åŒ–é˜ˆå€¼ Î´=0.1', alpha=0.7, zorder=1)
    ax4.axhline(y=0.15, color='red', linestyle='--', linewidth=2.5, 
                label='é¢„ç†”åŒ–/æ¶²æ€é˜ˆå€¼ Î´=0.15', alpha=0.7, zorder=1)
    
    # Shade regions
    if len(df_structure) > 0:
        delta_max = max(df_structure['delta']) * 1.1
        ax4.axhspan(0, 0.1, alpha=0.1, color=colors['solid'], zorder=0)
        ax4.axhspan(0.1, 0.15, alpha=0.1, color=colors['premelting'], zorder=0)
        ax4.axhspan(0.15, delta_max, alpha=0.1, color=colors['liquid'], zorder=0)
        ax4.set_ylim(0, delta_max)
    
    ax4.set_xlabel('æ¸©åº¦ Temperature (K)', fontsize=12, fontweight='bold')
    ax4.set_ylabel('Lindemann æŒ‡æ•° Î´', fontsize=12, fontweight='bold')
    ax4.set_title('(d) å•æ¬¡æ¨¡æ‹Ÿ Lindemann æŒ‡æ•°åˆ†å¸ƒ\nIndividual Run Lindemann Index Distribution', 
                  fontsize=13, fontweight='bold', pad=10)
    ax4.legend(fontsize=9, loc='upper left', framealpha=0.95, ncol=2)
    ax4.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)
    
    plt.tight_layout(rect=[0, 0, 1, 0.99])
    
    # Save figure
    output_file = output_dir / f'{structure_name}_individual_runs_analysis.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"      > Plot saved: {structure_name}_individual_runs_analysis.png")
    
    plt.close()


def generate_system_comparison_report(df_merged, Cv_support):
    """Generate comparison report for all systems - BY INDIVIDUAL STRUCTURE"""
    print(f"\n{'='*80}")
    print("Generating individual structure comparison report")
    print("="*80)
    
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    report_file = RESULTS_DIR / 'step7_4_multi_system_comparison.md'
    
    # Create subdirectory for individual structure plots
    individual_plots_dir = RESULTS_DIR / 'individual_structure_plots'
    individual_plots_dir.mkdir(parents=True, exist_ok=True)
    
    # Analyze each INDIVIDUAL STRUCTURE (system_id), not system_type
    structures = sorted(df_merged['system_id'].unique())
    system_results = {}
    
    print(f"\n  Total structures to analyze: {len(structures)}")
    
    for system_id in structures:
        print(f"\n  Analyzing {system_id}...")
        # Filter by system_id instead of system_type
        df_structure = df_merged[df_merged['system_id'] == system_id].copy()
        
        # Only analyze if enough data points
        if len(df_structure) < 10:
            print(f"    Warning: Only {len(df_structure)} points, skipping")
            continue
        
        results = fit_regional_heat_capacity(df_structure, Cv_support, system_id)
        system_results[system_id] = results
        
        # Generate individual visualization for this structure
        if results:
            plot_individual_structure_analysis(df_structure, results, Cv_support, system_id, individual_plots_dir)
    
    # Write report
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# Step 7.4: å¤šä½“ç³»çƒ­å®¹åˆ†æå¯¹æ¯”æŠ¥å‘Š\n\n")
        f.write("Multi-System Heat Capacity Analysis Comparison Report\n\n")
        f.write("=" * 80 + "\n\n")
        
        f.write(f"**æŠ¥å‘Šç”Ÿæˆæ—¶é—´ Report Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # Overview
        f.write("## 1. åˆ†ææ¦‚è§ˆ Analysis Overview\n\n")
        f.write(f"- **æ€»æ•°æ®ç‚¹ Total Points**: {len(df_merged)}\n")
        f.write(f"- **ç»“æ„æ•°é‡ Number of Structures**: {len(structures)}\n")
        f.write(f"- **å·²åˆ†æç»“æ„ Analyzed Structures**: {len(system_results)}\n")
        f.write(f"- **Support çƒ­å®¹ Support Cv**: {Cv_support:.4f} meV/K\n\n")
        
        f.write("### 1.1 ç»“æ„åˆ—è¡¨ Structure List\n\n")
        
        # Group by system_type for display
        system_type_groups = df_merged.groupby('system_type')['system_id'].apply(lambda x: sorted(x.unique())).to_dict()
        
        for sys_type, ids in sorted(system_type_groups.items()):
            count = sum(df_merged['system_id'] == sid for sid in ids).sum()
            f.write(f"- **{sys_type}**: {len(ids)} ä¸ªç»“æ„ ({', '.join(ids[:5])}{'...' if len(ids) > 5 else ''})\n")
        
        # System-wise results
        f.write("\n## 2. å„ç»“æ„çƒ­å®¹ç»“æœ Structure-wise Heat Capacity Results\n\n")
        
        for system_id in sorted(system_results.keys()):
            results = system_results[system_id]
            
            if not results:
                f.write(f"### {system_id}\n\n")
                f.write("âš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•å®Œæˆåˆ†æ Insufficient data for analysis\n\n")
                continue
            
            f.write(f"### {system_id}\n\n")
            
            # Summary table
            f.write("| åŒºåŸŸ Region | æ¸©åº¦èŒƒå›´ Temp Range | æ•°æ®ç‚¹ Points | Cv_cluster (meV/K) | RÂ² |\n")
            f.write("|-------------|---------------------|--------------|-------------------|----||\n")
            
            phase_map = {'solid': 'å›ºæ€ Solid', 'premelting': 'é¢„ç†”åŒ– Premelting', 'liquid': 'æ¶²æ€ Liquid'}
            
            for phase in ['solid', 'premelting', 'liquid']:
                if phase in results:
                    res = results[phase]
                    T_range = f"{res['T_range'][0]:.0f}-{res['T_range'][1]:.0f} K"
                    cv_str = f"{res['Cv_cluster']:.4f} Â± {res['slope_err']*1000:.4f}"
                    f.write(f"| {phase_map[phase]} | {T_range} | {res['n_points']} | {cv_str} | {res['R2']:.6f} |\n")
            
            f.write("\n")
        
        # Comparison tables - group by system_type
        f.write("## 3. ä½“ç³»å¯¹æ¯” System Comparison\n\n")
        
        for sys_type in sorted(system_type_groups.keys()):
            structure_ids = system_type_groups[sys_type]
            
            f.write(f"### {sys_type} ç³»åˆ—\n\n")
            
            # Solid comparison
            f.write("#### å›ºæ€çƒ­å®¹ Solid Heat Capacity\n\n")
            f.write("| ç»“æ„ Structure | Cv_cluster (meV/K) | RÂ² | æ•°æ®ç‚¹ Points |\n")
            f.write("|----------------|-------------------|----|---------------|\n")
            
            for system_id in structure_ids:
                if system_id in system_results and 'solid' in system_results[system_id]:
                    res = system_results[system_id]['solid']
                    cv_str = f"{res['Cv_cluster']:.4f} Â± {res['slope_err']*1000:.4f}"
                    f.write(f"| {system_id} | {cv_str} | {res['R2']:.6f} | {res['n_points']} |\n")
            
            f.write("\n")
            
            # Liquid comparison
            f.write("#### æ¶²æ€çƒ­å®¹ Liquid Heat Capacity\n\n")
            f.write("| ç»“æ„ Structure | Cv_cluster (meV/K) | RÂ² | æ•°æ®ç‚¹ Points |\n")
            f.write("|----------------|-------------------|----|---------------|\n")
            
            for system_id in structure_ids:
                if system_id in system_results and 'liquid' in system_results[system_id]:
                    res = system_results[system_id]['liquid']
                    cv_str = f"{res['Cv_cluster']:.4f} Â± {res['slope_err']*1000:.4f}"
                    f.write(f"| {system_id} | {cv_str} | {res['R2']:.6f} | {res['n_points']} |\n")
            
            f.write("\n")
        
        # Conclusions - find best structures overall
        f.write("\n## 4. ç»“è®º Conclusions\n\n")
        
        # Find best R2 for each region across ALL structures
        for region in ['solid', 'premelting', 'liquid']:
            best_r2 = 0
            best_structure = None
            for system_id in system_results:
                results = system_results[system_id]
                if region in results and results[region]['R2'] > best_r2:
                    best_r2 = results[region]['R2']
                    best_structure = system_id
            
            if best_structure:
                f.write(f"- **{region.capitalize()} åŒºåŸŸæœ€ä½³æ‹Ÿåˆ Best fit**: {best_structure} (RÂ² = {best_r2:.6f})\n")
        
        f.write(f"\n---\n")
        f.write(f"**è„šæœ¬ç‰ˆæœ¬ Script Version**: step7_4_multi_system_heat_capacity.py v1.0\n")
    
    print(f"    [OK] Report saved: {report_file}")
    
    # Save merged data
    # IMPORTANT: Replace 'structure' with 'system_id' for consistency
    # This ensures Cv-1/Cv-2/.../Cv-5 all show as 'Cv' in saved CSV
    df_merged_export = df_merged.copy()
    df_merged_export['structure'] = df_merged_export['system_id']
    
    csv_file = RESULTS_DIR / 'step7_4_all_systems_data.csv'
    df_merged_export.to_csv(csv_file, index=False, encoding='utf-8-sig')
    print(f"    [OK] Data saved: {csv_file}")
    
    return system_results


def plot_multi_system_comparison(df_merged, system_results, Cv_support):
    """Generate multi-system comparison visualization - BY INDIVIDUAL STRUCTURE"""
    print(f"\n>>> Generating multi-structure comparison plots")
    
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    
    # Get list of structures (system_id)
    structures = sorted(df_merged['system_id'].unique())
    
    # Group by system_type for coloring
    system_type_map = df_merged[['system_id', 'system_type']].drop_duplicates().set_index('system_id')['system_type'].to_dict()
    
    # Create color map by system_type
    system_types = sorted(df_merged['system_type'].unique())
    cmap = plt.cm.get_cmap('tab10')
    type_colors = {sys_type: cmap(i) for i, sys_type in enumerate(system_types)}
    
    # Map structure to color via its type
    structure_colors = {sid: type_colors[system_type_map[sid]] for sid in structures}
    
    # Create figure
    fig, axes = plt.subplots(2, 2, figsize=(18, 14))
    fig.suptitle('Step 7.4: Multi-Structure Heat Capacity Comparison (Individual Analysis)', 
                 fontsize=16, fontweight='bold', y=0.995)
    
    # Plot 1: All structures scatter (color by system_type)
    ax1 = axes[0, 0]
    
    # Plot by system_type for legend
    for sys_type in system_types:
        df_type = df_merged[df_merged['system_type'] == sys_type]
        ax1.scatter(df_type['temp'], df_type['avg_energy'], 
                   c=[type_colors[sys_type]], alpha=0.5, s=30,
                   label=f'{sys_type}', edgecolors='black', linewidths=0.3)
    
    ax1.set_xlabel('æ¸©åº¦ Temperature (K)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('èƒ½é‡ Energy (eV)', fontsize=12, fontweight='bold')
    ax1.set_title('(a) æ‰€æœ‰ç»“æ„èƒ½é‡åˆ†å¸ƒ (æŒ‰ä½“ç³»ç±»å‹ç€è‰²)\nAll Structures Energy Distribution (Colored by System Type)', 
                  fontsize=13, fontweight='bold', pad=10)
    ax1.legend(fontsize=9, loc='upper left', ncol=2)
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Solid Cv comparison (top structures with RÂ²>0.9)
    ax2 = axes[0, 1]
    solid_data = []
    
    for system_id in structures:
        if system_id in system_results and 'solid' in system_results[system_id]:
            res = system_results[system_id]['solid']
            if res['R2'] > 0.9:  # Only show high-quality results
                solid_data.append({
                    'id': system_id,
                    'cv': res['Cv_cluster'],
                    'err': res['slope_err'] * 1000,
                    'r2': res['R2'],
                    'color': structure_colors[system_id]
                })
    
    if solid_data:
        # Sort by RÂ² descending
        solid_data.sort(key=lambda x: x['r2'], reverse=True)
        # Take top 15
        solid_data = solid_data[:15]
        
        x = np.arange(len(solid_data))
        bars = ax2.bar(x, [d['cv'] for d in solid_data], 
                      color=[d['color'] for d in solid_data],
                      alpha=0.8, edgecolor='black', linewidth=1.0,
                      yerr=[d['err'] for d in solid_data], capsize=4)
        
        ax2.set_xlabel('ç»“æ„ Structure', fontsize=12, fontweight='bold')
        ax2.set_ylabel('å›ºæ€çƒ­å®¹ Solid Cv (meV/K)', fontsize=12, fontweight='bold')
        ax2.set_title(f'(b) å›ºæ€çƒ­å®¹å¯¹æ¯” (RÂ²>0.9, Top 15)\nSolid Heat Capacity Comparison', 
                      fontsize=13, fontweight='bold', pad=10)
        ax2.set_xticks(x)
        ax2.set_xticklabels([d['id'] for d in solid_data], rotation=45, ha='right', fontsize=8)
        ax2.grid(True, alpha=0.3, axis='y')
    else:
        ax2.text(0.5, 0.5, 'No high-quality solid data\n(RÂ² > 0.9)', 
                ha='center', va='center', fontsize=14, transform=ax2.transAxes)
    
    # Plot 3: Liquid Cv comparison (top structures with RÂ²>0.9)
    ax3 = axes[1, 0]
    liquid_data = []
    
    for system_id in structures:
        if system_id in system_results and 'liquid' in system_results[system_id]:
            res = system_results[system_id]['liquid']
            if res['R2'] > 0.9:
                liquid_data.append({
                    'id': system_id,
                    'cv': res['Cv_cluster'],
                    'err': res['slope_err'] * 1000,
                    'r2': res['R2'],
                    'color': structure_colors[system_id]
                })
    
    if liquid_data:
        liquid_data.sort(key=lambda x: x['r2'], reverse=True)
        liquid_data = liquid_data[:15]
        
        x = np.arange(len(liquid_data))
        bars = ax3.bar(x, [d['cv'] for d in liquid_data], 
                      color=[d['color'] for d in liquid_data],
                      alpha=0.8, edgecolor='black', linewidth=1.0,
                      yerr=[d['err'] for d in liquid_data], capsize=4)
        
        ax3.set_xlabel('ç»“æ„ Structure', fontsize=12, fontweight='bold')
        ax3.set_ylabel('æ¶²æ€çƒ­å®¹ Liquid Cv (meV/K)', fontsize=12, fontweight='bold')
        ax3.set_title(f'(c) æ¶²æ€çƒ­å®¹å¯¹æ¯” (RÂ²>0.9, Top 15)\nLiquid Heat Capacity Comparison', 
                      fontsize=13, fontweight='bold', pad=10)
        ax3.set_xticks(x)
        ax3.set_xticklabels([d['id'] for d in liquid_data], rotation=45, ha='right', fontsize=8)
        ax3.grid(True, alpha=0.3, axis='y')
    else:
        ax3.text(0.5, 0.5, 'No high-quality liquid data\n(RÂ² > 0.9)', 
                ha='center', va='center', fontsize=14, transform=ax3.transAxes)
    
    # Plot 4: RÂ² distribution by system_type
    ax4 = axes[1, 1]
    
    # Collect RÂ² values by system_type
    r2_by_type = {sys_type: {'solid': [], 'liquid': []} for sys_type in system_types}
    
    for system_id in structures:
        sys_type = system_type_map[system_id]
        if system_id in system_results:
            if 'solid' in system_results[system_id]:
                r2_by_type[sys_type]['solid'].append(system_results[system_id]['solid']['R2'])
            if 'liquid' in system_results[system_id]:
                r2_by_type[sys_type]['liquid'].append(system_results[system_id]['liquid']['R2'])
    
    # Box plot
    solid_r2_data = []
    liquid_r2_data = []
    labels = []
    
    for sys_type in system_types:
        if r2_by_type[sys_type]['solid']:
            solid_r2_data.append(r2_by_type[sys_type]['solid'])
            liquid_r2_data.append(r2_by_type[sys_type]['liquid'])
            labels.append(sys_type)
    
    if solid_r2_data:
        positions_solid = np.arange(len(labels)) * 2
        positions_liquid = positions_solid + 0.8
        
        bp1 = ax4.boxplot(solid_r2_data, positions=positions_solid, widths=0.6,
                          patch_artist=True, labels=labels, showfliers=False)
        bp2 = ax4.boxplot(liquid_r2_data, positions=positions_liquid, widths=0.6,
                          patch_artist=True, labels=[''] * len(labels), showfliers=False)
        
        for patch in bp1['boxes']:
            patch.set_facecolor('skyblue')
        for patch in bp2['boxes']:
            patch.set_facecolor('salmon')
        
        ax4.axhline(y=0.9, color='red', linestyle='--', linewidth=2, alpha=0.7, label='RÂ²=0.9 threshold')
        ax4.set_xlabel('ä½“ç³»ç±»å‹ System Type', fontsize=12, fontweight='bold')
        ax4.set_ylabel('RÂ² å€¼', fontsize=12, fontweight='bold')
        ax4.set_title('(d) RÂ² åˆ†å¸ƒ (å›ºæ€ vs æ¶²æ€)\nRÂ² Distribution (Solid vs Liquid)', 
                      fontsize=13, fontweight='bold', pad=10)
        ax4.set_xticks(positions_solid + 0.4)
        ax4.set_xticklabels(labels, rotation=45, ha='right')
        ax4.legend(['RÂ²=0.9', 'å›ºæ€ Solid', 'æ¶²æ€ Liquid'], fontsize=9)
        ax4.grid(True, alpha=0.3, axis='y')
        ax4.set_ylim(-0.1, 1.1)
    
    plt.tight_layout(rect=[0, 0, 1, 0.99])
    
    # Save
    output_file = RESULTS_DIR / 'step7_4_multi_system_comparison.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"    [OK] Figure saved: {output_file}")
    
    plt.close()


def main():
    """Main function with command-line argument support"""
    
    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description='Step 7.4: Multi-System Heat Capacity Analysis with Optional Filtering',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Default: No filtering (use all data)
  python step7_4_multi_system_heat_capacity.py

  # Apply Step 1 MSD outlier filtering only
  python step7_4_multi_system_heat_capacity.py --msd-filter

  # Apply IQR filtering only (Lindemann + Energy)
  python step7_4_multi_system_heat_capacity.py --iqr-filter

  # Apply both filtering methods
  python step7_4_multi_system_heat_capacity.py --msd-filter --iqr-filter

  # Adjust IQR threshold (default 3.0)
  python step7_4_multi_system_heat_capacity.py --iqr-filter --iqr-factor 2.5
        """
    )
    
    parser.add_argument(
        '--msd-filter',
        action='store_true',
        help='Enable Step 1 MSD outlier filtering (path signature matching)'
    )
    
    parser.add_argument(
        '--iqr-filter',
        action='store_true',
        help='Enable IQR-based outlier detection (both Lindemann and Energy)'
    )
    
    parser.add_argument(
        '--iqr-factor',
        type=float,
        default=3.0,
        help='IQR multiplier for outlier detection (default: 3.0, stricter than 1.5)'
    )
    
    parser.add_argument(
        '--cluster-analysis',
        type=str,
        default=None,
        help='Structure name for clustering analysis (e.g., Pt6Sn8). Uses K-means to auto-detect phase boundaries.'
    )
    
    args = parser.parse_args()
    
    print(f"\n{'='*80}")
    print("Step 7.4: Multi-Structure Individual Run Heat Capacity Analysis")
    print("="*80)
    print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Display filtering configuration
    print(f"[Filtering Configuration]")
    if args.msd_filter:
        print(f"  [ON]  Filter Method 1: Step 1 MSD outliers (path signature matching)")
    else:
        print(f"  [OFF] Filter Method 1: Disabled")
    
    if args.iqr_filter:
        print(f"  [ON]  Filter Method 2: IQR outliers (Lindemann + Energy, factor={args.iqr_factor})")
    else:
        print(f"  [OFF] Filter Method 2: Disabled")
    
    if not args.msd_filter and not args.iqr_filter:
        print(f"  [!!!] WARNING: No filtering applied - using all raw data!")
    
    print()
    
    # User can specify which systems to analyze
    # Set to None to analyze all systems
    # Examples:
    #   system_filter = ['Cv']  # Only Cv series
    #   system_filter = ['Cv', 'Pt8SnX', 'Pt6SnX']  # Multiple series
    #   system_filter = None  # All systems
    # 
    # NOTE: This filters by system_type (Cv, Pt8SnX, etc.)
    #       But analysis is done PER STRUCTURE (Cv-1, Pt8Sn3, etc.)
    
    system_filter = None  # Change this to filter specific system types
    
    print(f"System type filter: {system_filter if system_filter else 'All system types'}")
    print(f"Analysis level: Individual structures (system_id)")
    print(f"Note: Each structure (e.g., Cv-1, Pt8Sn3) analyzed separately\n")
    
    # 1. Load data
    df_energy = load_energy_data(CLUSTER_ENERGY_FILE, system_filter, 'cluster')
    df_lindemann = load_lindemann_individual_runs(system_filter)
    
    if df_energy is None or df_lindemann is None:
        print("\nError: Data loading failed")
        return
    
    if len(df_energy) == 0 or len(df_lindemann) == 0:
        print("\nError: No data after filtering")
        return
    
    # 1.5. Load Step 1 outliers for data quality filtering (if enabled)
    outliers = set()
    if args.msd_filter:
        print("\n" + "="*80)
        print("Loading Step 1 Outlier Filtering Results")
        print("="*80)
        outliers = load_outliers()
    
    # 2. Merge data with optional filtering
    print("\n" + "="*80)
    print("Merging Energy and Lindemann Data")
    print("="*80)
    df_merged, df_merged_original = merge_energy_lindemann(
        df_energy, 
        df_lindemann, 
        outlier_signatures=outliers if args.msd_filter else None,
        apply_iqr_filter=args.iqr_filter,
        iqr_factor=args.iqr_factor
    )
    
    if len(df_merged) == 0:
        print("\nError: Data matching failed")
        return
    
    # 2.5. Generate filtering statistics report
    if args.msd_filter or args.iqr_filter:
        generate_filtering_report(
            df_merged_original,  # Original unfiltered data
            df_merged,           # Final filtered data
            method1_applied=args.msd_filter,
            method2_applied=args.iqr_filter,
            iqr_factor=args.iqr_factor
        )
    
    # 3. Calculate support Cv
    Cv_support = calculate_support_cv()
    
    # 3.5. Optional: Clustering analysis for specific structure
    if args.cluster_analysis:
        print(f"\n{'='*80}")
        print(f"Performing Clustering Analysis for: {args.cluster_analysis}")
        print("="*80)
        
        # Filter data for the specified structure
        df_cluster_target = df_merged[df_merged['structure'] == args.cluster_analysis].copy()
        
        if len(df_cluster_target) == 0:
            print(f"  [ERROR] Structure '{args.cluster_analysis}' not found in data!")
            print(f"  Available structures: {sorted(df_merged['structure'].unique())[:10]}...")
        else:
            print(f"  Found {len(df_cluster_target)} data points for {args.cluster_analysis}")
            clustering_results = perform_lindemann_clustering(df_cluster_target, args.cluster_analysis, n_clusters=3)
            
            if clustering_results:
                # Optionally: Re-analyze heat capacity using clustered phases
                print(f"\n  Analyzing heat capacity with clustered phase boundaries...")
                df_cluster_target['phase_original'] = df_cluster_target['phase']
                df_cluster_target['phase'] = clustering_results['df_clustered']['phase_clustered']
                
                cluster_results = fit_regional_heat_capacity(df_cluster_target, Cv_support, args.cluster_analysis)
                
                if cluster_results:
                    print(f"\n  >>> Comparison: Fixed vs Clustered Thresholds")
                    print(f"  Fixed thresholds: Î´=0.10, 0.15")
                    print(f"  Clustered thresholds: {', '.join([f'Î´={t:.4f}' for t in clustering_results['thresholds']])}")
                    
                    # Compare Cv values
                    for phase in ['solid', 'premelting', 'liquid']:
                        if phase in cluster_results:
                            print(f"  {phase}: Cv_cluster = {cluster_results[phase]['Cv_cluster']:.4f} meV/K "
                                  f"(R2={cluster_results[phase]['R2']:.4f}, n={cluster_results[phase]['n_points']})")
    
    # 4. Multi-system analysis
    system_results = generate_system_comparison_report(df_merged, Cv_support)
    
    # 5. Visualization
    plot_multi_system_comparison(df_merged, system_results, Cv_support)
    
    # 6. Summary
    print(f"\n{'='*80}")
    print("Step 7.4 Multi-System Analysis Complete")
    print("="*80)
    
    print(f"\n[Analysis Summary]")
    print(f"  Total data points: {len(df_merged)}")
    print(f"  Number of system types: {len(df_merged['system_type'].unique())}")
    print(f"  Number of structures: {len(df_merged['system_id'].unique())}")
    print(f"  Structures analyzed: {len(system_results)}")
    
    print(f"\n[Structure Distribution]")
    for sys_type in sorted(df_merged['system_type'].unique()):
        structures = df_merged[df_merged['system_type'] == sys_type]['system_id'].unique()
        print(f"  {sys_type}: {len(structures)} structures")
    
    print(f"\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)


if __name__ == '__main__':
    main()
